import os
import re
import sqlite3
import logging
import time
import json
import traceback
from typing import Optional, List, Dict, Any
from datetime import datetime, timedelta
from contextlib import contextmanager
from enum import Enum
import asyncio
import threading
import stripe

# Import SEO utilities
try:
    from seo_utils import SEOEventProcessor, generate_event_json_ld, generate_seo_metadata, slugify
except ImportError:
    print("‚ö†Ô∏è SEO utilities not available - install seo_utils.py for full SEO features")
    def slugify(text): return text.lower().replace(' ', '-')
    SEOEventProcessor = None

import uvicorn
from dotenv import load_dotenv

from fastapi import FastAPI, HTTPException, Depends, status, BackgroundTasks, Request, Header

# MissionOps import
try:
    from missionops_endpoints import missionops_router
except ImportError:
    missionops_router = None
from fastapi.security import OAuth2PasswordBearer, OAuth2PasswordRequestForm
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, Response, JSONResponse

from pydantic import BaseModel, EmailStr, validator
from passlib.context import CryptContext
import jwt

# Scheduler imports
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.interval import IntervalTrigger
import httpx

# Load environment variables
load_dotenv()

# Security configuration
SECRET_KEY = os.getenv("SECRET_KEY", "fallback-secret-key-for-development-only")
ALGORITHM = os.getenv("ALGORITHM", "HS256")
ACCESS_TOKEN_EXPIRE_MINUTES = int(os.getenv("ACCESS_TOKEN_EXPIRE_MINUTES", 30))

# Environment configuration 
IS_PRODUCTION = os.getenv("RENDER", False) or os.getenv("RAILWAY_ENVIRONMENT", False)
DB_URL = os.getenv("DATABASE_URL", None)

# Stripe configuration
stripe.api_key = os.getenv("STRIPE_SECRET_KEY")
STRIPE_PUBLISHABLE_KEY = os.getenv("STRIPE_PUBLISHABLE_KEY")
STRIPE_WEBHOOK_SECRET = os.getenv("STRIPE_WEBHOOK_SECRET")
STRIPE_PRICE_ID = os.getenv("STRIPE_PRICE_ID")  # Monthly subscription price ID

# Password hashing setup
pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")
oauth2_scheme = OAuth2PasswordBearer(tokenUrl="token")

# Logging setup
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Create FastAPI app
app = FastAPI(title="EventFinder API")

# Register MissionOps router
if missionops_router:
    app.include_router(missionops_router)
    print("‚úÖ MissionOps endpoints registered")

# Dynamic CORS origins based on environment
def get_cors_origins():
    base_origins = [
        "http://localhost:5173",  # Local Vite dev server
        "http://127.0.0.1:5173",
        "http://localhost:5174",  # Admin dashboard local
        "http://127.0.0.1:5174",
    ]
    
    if IS_PRODUCTION:
        # In production, add common Render.com patterns including the actual deployed URL
        production_origins = [
            "https://todoevents.onrender.com",  # Actual frontend URL
            "https://eventfinder-api.onrender.com",
            "https://eventfinder-web.onrender.com", 
            "https://eventfinder-admin.onrender.com",
            "https://todoevents-1.onrender.com",
            "https://todoevents-1-frontend.onrender.com", 
            "https://todoevents-1-web.onrender.com",
            "https://todoevents-api.onrender.com",
            "https://todoevents-frontend.onrender.com",
            "https://todoevents-web.onrender.com",
        ]
        return base_origins + production_origins
    else:
        return base_origins

# Custom CORS middleware for flexible Render.com handling
@app.middleware("http")
async def cors_handler(request, call_next):
    # Log the request for debugging (reduced logging for performance)
    if request.url.path == "/api/report-event":
        logger.info(f"üö® CORS MIDDLEWARE: {request.method} {request.url.path}")
        logger.info(f"üö® Headers: {dict(request.headers)}")
    origin = request.headers.get("origin")
    
    # Handle preflight requests
    if request.method == "OPTIONS":
        # Determine allowed origin
        allowed_origin = "*"  # Default fallback
        
        if origin:
            # Allow localhost and 127.0.0.1 for development
            if ("localhost" in origin or "127.0.0.1" in origin):
                allowed_origin = origin
            # Allow Render.com domains for production
            elif ".onrender.com" in origin:
                allowed_origin = origin
            # Allow the actual domain
            elif "todo-events.com" in origin:
                allowed_origin = origin
            else:
                allowed_origin = origin
        
        # Return proper preflight response
        return Response(
            status_code=200,
            headers={
                "Access-Control-Allow-Origin": allowed_origin,
                "Access-Control-Allow-Methods": "GET, POST, PUT, DELETE, OPTIONS",
                "Access-Control-Allow-Headers": "Content-Type, Authorization, X-Requested-With, Cache-Control, Pragma",
                "Access-Control-Allow-Credentials": "true",
                "Access-Control-Max-Age": "86400",
            }
        )
    
    try:
        # Call the next middleware or endpoint handler
        response = await call_next(request)
        
        # Add CORS headers to successful responses
        if origin:
            # Determine allowed origin for response
            allowed_origin = "*"
            if ("localhost" in origin or "127.0.0.1" in origin or 
                ".onrender.com" in origin or "todo-events.com" in origin):
                allowed_origin = origin
            
            response.headers["Access-Control-Allow-Origin"] = allowed_origin
            response.headers["Access-Control-Allow-Credentials"] = "true"
            response.headers["Access-Control-Allow-Methods"] = "GET, POST, PUT, DELETE, OPTIONS"
            response.headers["Access-Control-Allow-Headers"] = "Content-Type, Authorization, X-Requested-With, Cache-Control, Pragma"
        
        return response
        
    except Exception as e:
        # Handle exceptions and ensure CORS headers are added to error responses
        logger.error(f"Error during request processing: {str(e)}")
        
        # Create response with error details
        status_code = 500
        if isinstance(e, HTTPException):
            status_code = e.status_code
            
        error_response = JSONResponse(
            status_code=status_code,
            content={"detail": str(e)},
        )
        
        # Add CORS headers to error response
        if origin:
            allowed_origin = "*"
            if ("localhost" in origin or "127.0.0.1" in origin or 
                ".onrender.com" in origin or "todo-events.com" in origin):
                allowed_origin = origin
                
            error_response.headers["Access-Control-Allow-Origin"] = allowed_origin
            error_response.headers["Access-Control-Allow-Credentials"] = "true"
            
        return error_response

# CORS middleware with dynamic origins (as fallback)
# Commenting out the built-in CORS middleware since we have custom CORS handling
app.add_middleware(
    CORSMiddleware,
    allow_origins=get_cors_origins(),
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allow_headers=["*"],
    max_age=86400,  # Cache preflight requests for 24 hours
)

# Database file for SQLite (development only)
DB_FILE = os.path.join(os.path.dirname(__file__), "events.db")

# Enums
class UserRole(str, Enum):
    ADMIN = "admin"
    USER = "user"
    PREMIUM = "premium"
    ENTERPRISE = "enterprise"

# Database context manager with retry logic
@contextmanager
def get_db_transaction():
    """
    Get database connection with transaction control - properly handles autocommit for PostgreSQL
    
    CRITICAL FIX: PostgreSQL connections default to autocommit=True which prevents manual 
    transaction control (BEGIN/COMMIT/ROLLBACK). This function temporarily disables autocommit
    to allow explicit transaction management, then restores the original setting.
    
    This fixes the "set_session cannot be used inside a transaction" error.
    """
    if IS_PRODUCTION and DB_URL:
        # In production with PostgreSQL
        import psycopg2
        from psycopg2.extras import RealDictCursor
        
        conn = None
        original_autocommit = None
        
        try:
            conn = psycopg2.connect(
                DB_URL,
                cursor_factory=RealDictCursor,
                connect_timeout=8,
                keepalives=1,
                keepalives_idle=30,
                keepalives_interval=5,
                keepalives_count=3,
                application_name='todoevents'
            )
            
            # Store original autocommit setting and disable it for transaction control
            original_autocommit = conn.autocommit
            conn.autocommit = False  # CRITICAL: Disable autocommit for manual transactions
            
            yield conn
            
        finally:
            if conn:
                # Restore original autocommit setting
                if original_autocommit is not None:
                    try:
                        conn.autocommit = original_autocommit
                    except:
                        pass
                conn.close()
    else:
        # Local development with SQLite
        conn = sqlite3.connect(DB_FILE, timeout=10)
        conn.row_factory = sqlite3.Row
        conn.execute('PRAGMA journal_mode=WAL')
        conn.execute('PRAGMA synchronous=NORMAL')
        conn.execute('PRAGMA cache_size=10000')
        try:
            yield conn
        finally:
            conn.close()

@contextmanager
def get_db():
    if IS_PRODUCTION and DB_URL:
        # In production with PostgreSQL
        import psycopg2
        from psycopg2.extras import RealDictCursor
        
        # Reduced retry count for faster failover
        retry_count = 2  # Reduced from 3
        conn = None
        
        for attempt in range(retry_count):
            try:
                # Optimized connection parameters for faster connections
                conn = psycopg2.connect(
                    DB_URL,
                    cursor_factory=RealDictCursor,
                    connect_timeout=8,  # Reduced from 10
                    keepalives=1,
                    keepalives_idle=30,
                    keepalives_interval=5,
                    keepalives_count=3,
                    # Add connection pooling parameters
                    application_name='todoevents'
                )
                
                # Set autocommit for read operations to reduce lock time
                conn.autocommit = True
                break  # Connection successful, exit retry loop
                
            except Exception as e:
                if conn:
                    try:
                        conn.close()
                        conn = None
                    except:
                        pass
                
                error_msg = str(e)
                
                if attempt < retry_count - 1:
                    # Faster backoff for production
                    wait_time = 1  # Reduced wait time
                    time.sleep(wait_time)
                else:
                    logger.critical(f"Failed to connect to database after {retry_count} attempts: {error_msg}")
                    # Instead of falling back to SQLite, raise a proper HTTP exception
                    raise HTTPException(
                        status_code=503,
                        detail="Database connection failed. Please try again later."
                    )
        
        # If we made it here, conn should be valid
        try:
            yield conn
        finally:
            if conn:
                conn.close()
    else:
        # Local development with SQLite - optimized
        conn = sqlite3.connect(DB_FILE, timeout=10)  # Add timeout
        conn.row_factory = sqlite3.Row
        # Enable WAL mode for better concurrent access
        conn.execute('PRAGMA journal_mode=WAL')
        conn.execute('PRAGMA synchronous=NORMAL')  # Faster than FULL
        conn.execute('PRAGMA cache_size=10000')    # Increase cache
        try:
            yield conn
        finally:
            conn.close()

# Helper function to get placeholder style based on environment
def get_placeholder():
    if IS_PRODUCTION and DB_URL:
        return "%s"  # PostgreSQL uses %s
    else:
        return "?"   # SQLite uses ?

# Helper function to get count from cursor result (handles both SQLite and PostgreSQL)
def get_count_from_result(cursor_result):
    """Extract count value from cursor result, handling both SQLite tuples and PostgreSQL RealDictRow"""
    if cursor_result is None:
        return 0
    
    # Check if this is a dict-like object (PostgreSQL RealDictRow)
    if isinstance(cursor_result, dict) or hasattr(cursor_result, 'get'):
        # PostgreSQL RealDictCursor returns dict-like objects
        # Try various count column names
        for key in ['count', 'COUNT(*)', 'count(*)', 'cnt']:
            if hasattr(cursor_result, 'get'):
                value = cursor_result.get(key)
            else:
                value = cursor_result.get(key, None)
            if value is not None:
                return value
        # If no count key found, return the first value
        if hasattr(cursor_result, 'values'):
            values = list(cursor_result.values())
            return values[0] if values else 0
        return 0
    else:
        # SQLite returns tuples
        return cursor_result[0] if cursor_result and len(cursor_result) > 0 else 0

def format_cursor_row(row, column_names):
    """Format a single cursor row for both SQLite and PostgreSQL compatibility"""
    if isinstance(row, dict) or hasattr(row, 'get'):
        # PostgreSQL RealDictCursor returns dict-like objects
        return {col: row.get(col) for col in column_names}
    else:
        # SQLite returns tuples
        return {col: row[i] if i < len(row) else None for i, col in enumerate(column_names)}

# Missing validation function that's being called
def validate_recurring_event(event_data, user_role):
    """
    Validate recurring event data and permissions
    This function was being called but was missing, causing NameError
    """
    # For now, just return the event data unchanged
    # Can be enhanced later with actual validation logic
    return event_data



# Database initialization
# Force production database migration for interest/view tracking - v2.1
def init_db():
    try:
        with get_db() as conn:
            c = conn.cursor()
            
            if IS_PRODUCTION and DB_URL:
                # PostgreSQL table creation
                
                # Create users table
                c.execute('''
                    CREATE TABLE IF NOT EXISTS users (
                        id SERIAL PRIMARY KEY,
                        email TEXT UNIQUE NOT NULL,
                        hashed_password TEXT NOT NULL,
                        role TEXT NOT NULL,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                ''')
                
                # Create events table with user relationship
                c.execute('''
                    CREATE TABLE IF NOT EXISTS events (
                        id SERIAL PRIMARY KEY,
                        title TEXT NOT NULL,
                        description TEXT NOT NULL,
                        date TEXT NOT NULL,
                        start_time TEXT NOT NULL,
                        end_time TEXT,
                        end_date TEXT,
                        category TEXT NOT NULL,
                        secondary_category TEXT,
                        address TEXT NOT NULL,
                        lat REAL NOT NULL,
                        lng REAL NOT NULL,
                        recurring BOOLEAN NOT NULL DEFAULT FALSE,
                        frequency TEXT,
                        created_by INTEGER,
                        interest_count INTEGER DEFAULT 0,
                        view_count INTEGER DEFAULT 0,
                        fee_required TEXT,
                        event_url TEXT,
                        host_name TEXT,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        verified BOOLEAN DEFAULT FALSE,
                        FOREIGN KEY(created_by) REFERENCES users(id)
                    )
                ''')
                
                # Add migration for existing events to have start_time and end_time
                try:
                    # Use PostgreSQL-compatible column checking method with fallback
                    def column_exists(table_name, column_name):
                        try:
                            # Try information_schema first
                            c.execute("""
                                SELECT column_name FROM information_schema.columns 
                                WHERE table_name = %s AND column_name = %s
                            """, (table_name, column_name))
                            return c.fetchone() is not None
                        except Exception:
                            # Fallback: try to add column and catch error if it exists
                            try:
                                c.execute(f'ALTER TABLE {table_name} ADD COLUMN {column_name}_test_temp TEXT')
                                c.execute(f'ALTER TABLE {table_name} DROP COLUMN {column_name}_test_temp')
                                # If we got here, column doesn't exist
                                return False
                            except Exception:
                                # If adding temp column failed, assume original column exists
                                return True
                    
                    # Check if we need to migrate old 'time' column to 'start_time'
                    if column_exists('events', 'time'):
                        # Migrate old 'time' column to 'start_time'
                        c.execute('ALTER TABLE events RENAME COLUMN time TO start_time')
                        logger.info("‚úÖ Migrated 'time' column to 'start_time'")
                        conn.commit()
                        
                    # Add start_time column if it doesn't exist
                    if not column_exists('events', 'start_time'):
                        c.execute('ALTER TABLE events ADD COLUMN start_time TEXT DEFAULT \'12:00\'')
                        logger.info("‚úÖ Added 'start_time' column")
                        conn.commit()
                        
                    # Add end_time column if it doesn't exist
                    if not column_exists('events', 'end_time'):
                        c.execute('ALTER TABLE events ADD COLUMN end_time TEXT')
                        logger.info("‚úÖ Added 'end_time' column")
                        conn.commit()
                        
                    # Add end_date column if it doesn't exist
                    if not column_exists('events', 'end_date'):
                        c.execute('ALTER TABLE events ADD COLUMN end_date TEXT')
                        logger.info("‚úÖ Added 'end_date' column")
                        conn.commit()
                    
                    # Add interest_count column if it doesn't exist
                    if not column_exists('events', 'interest_count'):
                        c.execute('ALTER TABLE events ADD COLUMN interest_count INTEGER DEFAULT 0')
                        logger.info("‚úÖ Added 'interest_count' column")
                        conn.commit()
                    
                    # Add view_count column if it doesn't exist
                    if not column_exists('events', 'view_count'):
                        c.execute('ALTER TABLE events ADD COLUMN view_count INTEGER DEFAULT 0')
                        logger.info("‚úÖ Added 'view_count' column")
                        conn.commit()
                    
                    # Add new UX enhancement columns
                    if not column_exists('events', 'fee_required'):
                        c.execute('ALTER TABLE events ADD COLUMN fee_required TEXT')
                        logger.info("‚úÖ Added 'fee_required' column")
                        conn.commit()
                    
                    if not column_exists('events', 'event_url'):
                        c.execute('ALTER TABLE events ADD COLUMN event_url TEXT')
                        logger.info("‚úÖ Added 'event_url' column")
                        conn.commit()
                    
                    if not column_exists('events', 'host_name'):
                        c.execute('ALTER TABLE events ADD COLUMN host_name TEXT')
                        logger.info("‚úÖ Added 'host_name' column")
                        conn.commit()
                    
                    # Add secondary_category column if it doesn't exist
                    if not column_exists('events', 'secondary_category'):
                        c.execute('ALTER TABLE events ADD COLUMN secondary_category TEXT')
                        logger.info("‚úÖ Added 'secondary_category' column")
                        conn.commit()
                        
                    # Add verified column if it doesn't exist
                    if not column_exists('events', 'verified'):
                        c.execute('ALTER TABLE events ADD COLUMN verified BOOLEAN DEFAULT FALSE')
                        logger.info("‚úÖ Added 'verified' column")
                        conn.commit()
                        
                except Exception as migration_error:
                    logger.error(f"‚ùå Schema fix error: {migration_error}")
                    # Don't fail the entire initialization, just log the error
                    try:
                        conn.rollback()
                    except:
                        pass
                
                # Create activity_logs table
                c.execute('''
                    CREATE TABLE IF NOT EXISTS activity_logs (
                        id SERIAL PRIMARY KEY,
                        user_id INTEGER,
                        action TEXT NOT NULL,
                        details TEXT,
                        timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        FOREIGN KEY(user_id) REFERENCES users(id)
                    )
                ''')
                
                # Create interest tracking table
                c.execute('''CREATE TABLE IF NOT EXISTS event_interests (
                            id SERIAL PRIMARY KEY,
                            event_id INTEGER NOT NULL,
                            user_id INTEGER,
                            browser_fingerprint TEXT NOT NULL DEFAULT 'legacy',
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            UNIQUE(event_id, user_id, browser_fingerprint),
                            FOREIGN KEY(event_id) REFERENCES events(id) ON DELETE CASCADE,
                            FOREIGN KEY(user_id) REFERENCES users(id) ON DELETE SET NULL
                        )''')
                
                # Create view tracking table
                c.execute('''CREATE TABLE IF NOT EXISTS event_views (
                            id SERIAL PRIMARY KEY,
                            event_id INTEGER NOT NULL,
                            user_id INTEGER,
                            browser_fingerprint TEXT NOT NULL DEFAULT 'legacy',
                            viewed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            UNIQUE(event_id, user_id, browser_fingerprint),
                            FOREIGN KEY(event_id) REFERENCES events(id) ON DELETE CASCADE,
                            FOREIGN KEY(user_id) REFERENCES users(id) ON DELETE SET NULL
                        )''')
                
                # Create page visits tracking table
                c.execute('''CREATE TABLE IF NOT EXISTS page_visits (
                            id SERIAL PRIMARY KEY,
                            page_type TEXT NOT NULL,
                            page_path TEXT NOT NULL,
                            user_id INTEGER,
                            browser_fingerprint TEXT NOT NULL DEFAULT 'anonymous',
                            visited_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            FOREIGN KEY(user_id) REFERENCES users(id) ON DELETE SET NULL
                        )''')

                # Create privacy requests table for CCPA compliance
                c.execute('''CREATE TABLE IF NOT EXISTS privacy_requests (
                            id SERIAL PRIMARY KEY,
                            request_type TEXT NOT NULL CHECK (request_type IN ('access', 'delete', 'opt_out')),
                            email TEXT NOT NULL,
                            full_name TEXT,
                            verification_info TEXT,
                            details TEXT,
                            status TEXT DEFAULT 'pending' CHECK (status IN ('pending', 'in_progress', 'completed', 'denied')),
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            completed_at TIMESTAMP,
                            admin_notes TEXT
                        )''')

                # Create event reports table for reporting functionality
                c.execute('''CREATE TABLE IF NOT EXISTS event_reports (
                            id SERIAL PRIMARY KEY,
                            event_id INTEGER NOT NULL,
                            event_title TEXT,
                            event_address TEXT,
                            event_date TEXT,
                            reason TEXT NOT NULL,
                            category TEXT NOT NULL,
                            description TEXT NOT NULL,
                            reporter_email TEXT NOT NULL,
                            reporter_name TEXT,
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            status TEXT DEFAULT 'pending',
                            FOREIGN KEY(event_id) REFERENCES events(id) ON DELETE CASCADE
                        )''')
                
                # Ensure tables are committed
                conn.commit()
                logger.info("‚úÖ Interest, view tracking, privacy requests, and event reports tables created/verified")
            else:
                # SQLite table creation (existing code)
                c.execute('''CREATE TABLE IF NOT EXISTS users (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            email TEXT UNIQUE NOT NULL,
                            hashed_password TEXT NOT NULL,
                            role TEXT NOT NULL,
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                        )''')
                
                # Check if events table exists and needs migration
                c.execute('''PRAGMA table_info(events)''')
                columns = [column[1] for column in c.fetchall()]
                
                if 'events' not in [table[0] for table in c.execute("SELECT name FROM sqlite_master WHERE type='table'").fetchall()]:
                    # Create new table with updated schema
                    c.execute('''CREATE TABLE events (
                                id INTEGER PRIMARY KEY AUTOINCREMENT,
                                title TEXT NOT NULL,
                                description TEXT NOT NULL,
                                date TEXT NOT NULL,
                                start_time TEXT NOT NULL,
                                end_time TEXT,
                                end_date TEXT,
                                category TEXT NOT NULL,
                                secondary_category TEXT,
                                address TEXT NOT NULL,
                                lat REAL NOT NULL,
                                lng REAL NOT NULL,
                                recurring BOOLEAN NOT NULL DEFAULT 0,
                                frequency TEXT,
                                created_by INTEGER,
                                interest_count INTEGER DEFAULT 0,
                                view_count INTEGER DEFAULT 0,
                                fee_required TEXT,
                                event_url TEXT,
                                host_name TEXT,
                                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                                verified BOOLEAN DEFAULT FALSE,
                                FOREIGN KEY(created_by) REFERENCES users(id)
                            )''')
                else:
                    # Migrate existing table
                    if 'time' in columns and 'start_time' not in columns:
                        # Rename time to start_time
                        c.execute('''ALTER TABLE events RENAME COLUMN time TO start_time''')
                        logger.info("Migrated 'time' column to 'start_time'")
                    
                    if 'start_time' not in columns:
                        # Add start_time column with default value from time column if it exists
                        if 'time' in columns:
                            c.execute('''ALTER TABLE events ADD COLUMN start_time TEXT''')
                            c.execute('''UPDATE events SET start_time = time WHERE start_time IS NULL''')
                            logger.info("Added start_time column and migrated from time")
                        else:
                            c.execute('''ALTER TABLE events ADD COLUMN start_time TEXT DEFAULT "12:00"''')
                            logger.info("Added start_time column with default value")
                    
                    if 'end_time' not in columns:
                        c.execute('''ALTER TABLE events ADD COLUMN end_time TEXT''')
                        logger.info("Added end_time column")
                    
                    # Add interest_count and view_count columns if they don't exist
                    if 'interest_count' not in columns:
                        c.execute('''ALTER TABLE events ADD COLUMN interest_count INTEGER DEFAULT 0''')
                        logger.info("Added interest_count column")
                    
                    if 'view_count' not in columns:
                        c.execute('''ALTER TABLE events ADD COLUMN view_count INTEGER DEFAULT 0''')
                        logger.info("Added view_count column")
                    
                    # Add new UX enhancement columns for SQLite
                    if 'fee_required' not in columns:
                        c.execute('''ALTER TABLE events ADD COLUMN fee_required TEXT''')
                        logger.info("Added fee_required column")
                    
                    if 'event_url' not in columns:
                        c.execute('''ALTER TABLE events ADD COLUMN event_url TEXT''')
                        logger.info("Added event_url column")
                    
                    if 'host_name' not in columns:
                        c.execute('''ALTER TABLE events ADD COLUMN host_name TEXT''')
                        logger.info("Added host_name column")
                    
                    # Add secondary_category column if it doesn't exist  
                    if 'secondary_category' not in columns:
                        c.execute('''ALTER TABLE events ADD COLUMN secondary_category TEXT''')
                        logger.info("Added secondary_category column")
                    
                    # Add verified column if it doesn't exist
                    if 'verified' not in columns:
                        c.execute('''ALTER TABLE events ADD COLUMN verified BOOLEAN DEFAULT FALSE''')
                        logger.info("Added verified column")
                
                c.execute('''CREATE TABLE IF NOT EXISTS activity_logs (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            user_id INTEGER,
                            action TEXT NOT NULL,
                            details TEXT,
                            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                            FOREIGN KEY(user_id) REFERENCES users(id)
                        )''')
                
                # Create interest tracking table
                c.execute('''CREATE TABLE IF NOT EXISTS event_interests (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            event_id INTEGER NOT NULL,
                            user_id INTEGER,
                            browser_fingerprint TEXT NOT NULL DEFAULT 'legacy',
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            UNIQUE(event_id, user_id, browser_fingerprint),
                            FOREIGN KEY(event_id) REFERENCES events(id) ON DELETE CASCADE,
                            FOREIGN KEY(user_id) REFERENCES users(id) ON DELETE SET NULL
                        )''')
                
                # Create view tracking table
                c.execute('''CREATE TABLE IF NOT EXISTS event_views (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            event_id INTEGER NOT NULL,
                            user_id INTEGER,
                            browser_fingerprint TEXT NOT NULL DEFAULT 'legacy',
                            viewed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            UNIQUE(event_id, user_id, browser_fingerprint),
                            FOREIGN KEY(event_id) REFERENCES events(id) ON DELETE CASCADE,
                            FOREIGN KEY(user_id) REFERENCES users(id) ON DELETE SET NULL
                        )''')
                
                # Create page visits tracking table
                c.execute('''CREATE TABLE IF NOT EXISTS page_visits (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            page_type TEXT NOT NULL,
                            page_path TEXT NOT NULL,
                            user_id INTEGER,
                            browser_fingerprint TEXT NOT NULL DEFAULT 'anonymous',
                            visited_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            FOREIGN KEY(user_id) REFERENCES users(id) ON DELETE SET NULL
                        )''')

                # Create privacy requests table for CCPA compliance
                c.execute('''CREATE TABLE IF NOT EXISTS privacy_requests (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            request_type TEXT NOT NULL CHECK (request_type IN ('access', 'delete', 'opt_out')),
                            email TEXT NOT NULL,
                            full_name TEXT,
                            verification_info TEXT,
                            details TEXT,
                            status TEXT DEFAULT 'pending' CHECK (status IN ('pending', 'in_progress', 'completed', 'denied')),
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            completed_at TIMESTAMP,
                            admin_notes TEXT
                        )''')

                # Create event reports table for reporting functionality
                c.execute('''CREATE TABLE IF NOT EXISTS event_reports (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            event_id INTEGER NOT NULL,
                            event_title TEXT,
                            event_address TEXT,
                            event_date TEXT,
                            reason TEXT NOT NULL,
                            category TEXT NOT NULL,
                            description TEXT NOT NULL,
                            reporter_email TEXT NOT NULL,
                            reporter_name TEXT,
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            status TEXT DEFAULT 'pending',
                            FOREIGN KEY(event_id) REFERENCES events(id) ON DELETE CASCADE
                        )''')
                
                # SQLite does not have information_schema, so we skip the schema fixes
                # The tracking, privacy, and reporting tables are created correctly above for SQLite
            
            # Ensure password_resets table exists
            if IS_PRODUCTION and DB_URL:
                c.execute('''
                    CREATE TABLE IF NOT EXISTS password_resets (
                        id SERIAL PRIMARY KEY,
                        email TEXT NOT NULL,
                        reset_code TEXT NOT NULL,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        expires_at TIMESTAMP NOT NULL
                    )
                ''')
            else:
                c.execute('''
                    CREATE TABLE IF NOT EXISTS password_resets (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        email TEXT NOT NULL,
                        reset_code TEXT NOT NULL,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        expires_at TIMESTAMP NOT NULL
                    )
                ''')
            
            # Check for default admin user and create if needed
            # create_default_admin_user(conn)  # Moved to after security functions are defined
            
    except Exception as e:
        logger.error(f"Error initializing database: {str(e)}")
        # Don't raise the exception here - this allows the app to start even if DB init fails
        # We'll handle DB errors at the endpoint level
def create_default_admin_user(conn):
    """Create default admin user if none exists"""
    try:
        cursor = conn.cursor()
        
        # Check if any admin users exist
        cursor.execute("SELECT COUNT(*) FROM users WHERE role = 'admin'")
        admin_count = get_count_from_result(cursor.fetchone())
        
        if admin_count == 0:
            logger.info("No admin users found. Creating default admin user...")
            
            # Use a secure random password that must be changed
            import secrets
            import string
            
            # Generate a random 16-character password
            alphabet = string.ascii_letters + string.digits + "!@#$%^&*"
            admin_password = ''.join(secrets.choice(alphabet) for i in range(16))
            
            hashed_password = get_password_hash(admin_password)
            
            cursor.execute(
                "INSERT INTO users (email, hashed_password, role) VALUES (?, ?, ?)",
                ("admin@todo-events.com", hashed_password, "admin")
            )
            conn.commit()
            
            logger.info("‚úÖ Default admin user created:")
            logger.info(f"   üìß Email: admin@todo-events.com")
            logger.info(f"   üîë Password: {admin_password}")
            logger.info("‚ö†Ô∏è IMPORTANT: Change this password after first login!")
            
        else:
            logger.info(f"Found {admin_count} existing admin users. Default admin creation skipped.")
            
    except Exception as e:
        logger.error(f"Error creating default admin user: {str(e)}")
        return False
    
    return True

# Initialize database
try:
    init_db()
except Exception as e:
    logger.error(f"Database initialization failed: {str(e)}")

# =====================================================
# AUTOMATED AI SYNC SYSTEM
# =====================================================

class AutomatedTaskManager:
    """
    Manages automated tasks for AI search optimization:
    - Sitemap generation and updates
    - Event data refresh
    - AI tool synchronization
    - Performance monitoring
    """
    
    def __init__(self):
        self.scheduler = BackgroundScheduler()
        self.last_sitemap_update = None
        self.last_event_refresh = None
        self.task_status = {
            "sitemap_generation": {"status": "pending", "last_run": None, "next_run": None},
            "event_refresh": {"status": "pending", "last_run": None, "next_run": None},
            "ai_sync": {"status": "pending", "last_run": None, "next_run": None}
        }
        
    async def generate_sitemap_automatically(self):
        """Generate sitemap with current event data"""
        try:
            logger.info("üîÑ Starting automated sitemap generation...")
            
            # Get current events from database
            events = await self.get_current_events()
            
            # Generate sitemap content
            sitemap_content = await self.build_sitemap_content(events)
            
            # Save to file (in production, this could be uploaded to S3/CDN)
            await self.save_sitemap(sitemap_content)
            
            # Update task status
            self.task_status["sitemap_generation"]["status"] = "completed"
            self.task_status["sitemap_generation"]["last_run"] = datetime.utcnow().isoformat()
            self.last_sitemap_update = datetime.utcnow()
            
            logger.info("‚úÖ Automated sitemap generation completed successfully")
            
            # Notify search engines about the update
            await self.ping_search_engines()
            
        except Exception as e:
            logger.error(f"‚ùå Automated sitemap generation failed: {str(e)}")
            self.task_status["sitemap_generation"]["status"] = "failed"
    
    async def refresh_event_data(self):
        """Refresh and optimize event data for AI consumption"""
        try:
            logger.info("üîÑ Starting automated event data refresh...")
            
            # Clean up expired events
            await self.cleanup_expired_events()
            
            # Update event search index (if implemented)
            await self.update_search_index()
            
            # Cache popular queries for faster AI responses
            await self.cache_popular_queries()
            
            self.task_status["event_refresh"]["status"] = "completed"
            self.task_status["event_refresh"]["last_run"] = datetime.utcnow().isoformat()
            self.last_event_refresh = datetime.utcnow()
            
            logger.info("‚úÖ Automated event data refresh completed")
            
        except Exception as e:
            logger.error(f"‚ùå Automated event data refresh failed: {str(e)}")
            self.task_status["event_refresh"]["status"] = "failed"
    
    async def sync_with_ai_tools(self):
        """Optimize data specifically for AI tool consumption"""
        try:
            logger.info("ü§ñ Starting AI tools synchronization...")
            
            # Update AI-optimized cache
            await self.update_ai_cache()
            
            # Test AI endpoint responsiveness
            await self.test_ai_endpoint()
            
            # Update structured data if needed
            await self.validate_structured_data()
            
            self.task_status["ai_sync"]["status"] = "completed"
            self.task_status["ai_sync"]["last_run"] = datetime.utcnow().isoformat()
            
            logger.info("‚úÖ AI tools synchronization completed")
            
        except Exception as e:
            logger.error(f"‚ùå AI tools synchronization failed: {str(e)}")
            self.task_status["ai_sync"]["status"] = "failed"
    
    async def get_current_events(self):
        """Get current and upcoming events from database"""
        try:
            with get_db() as conn:
                c = conn.cursor()
                
                # Use proper date comparison for both PostgreSQL and SQLite
                if IS_PRODUCTION and DB_URL:
                    # PostgreSQL - cast date text to date for comparison
                    c.execute("""
                        SELECT id, title, description, date, start_time, end_time, end_date, category, 
                               address, lat, lng, created_at, slug, is_published
                        FROM events 
                        WHERE CAST(date AS DATE) >= (CURRENT_DATE - INTERVAL '30 days')::DATE 
                        AND (is_published = true OR is_published IS NULL)
                        ORDER BY CAST(date AS DATE), start_time
                    """)
                else:
                    # SQLite
                    c.execute("""
                        SELECT id, title, description, date, start_time, end_time, end_date, category, 
                               address, lat, lng, created_at, slug, is_published
                        FROM events 
                        WHERE date::date >= CURRENT_DATE AND (is_published = 1 OR is_published IS NULL)
                        ORDER BY date, start_time
                    """)
                return [dict(row) for row in c.fetchall()]
        except Exception as e:
            logger.error(f"Error fetching events for sitemap: {str(e)}")
            return []
    
    async def build_sitemap_content(self, events):
        """Build sitemap XML content with current events"""
        current_date = datetime.utcnow().strftime('%Y-%m-%d')
        domain = "https://todo-events.com"
        
        # Start with static sitemap structure
        sitemap = f'''<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9"
        xmlns:image="http://www.google.com/schemas/sitemap-image/1.1">

  <!-- Homepage - Primary landing page -->
  <url>
    <loc>{domain}/</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>daily</changefreq>
    <priority>1.0</priority>
    <image:image>
      <image:loc>{domain}/images/pin-logo.svg</image:loc>
      <image:caption>todo-events logo - Local event discovery platform</image:caption>
    </image:image>
  </url>

  <!-- Main navigation pages -->
  <url>
    <loc>{domain}/hosts</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>weekly</changefreq>
    <priority>0.8</priority>
  </url>
  <url>
    <loc>{domain}/creators</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>weekly</changefreq>
    <priority>0.8</priority>
  </url>'''
        
        # Add category pages - both query param and SEO-friendly formats
        categories = [
    'food-drink', 'music', 'arts', 'sports', 'automotive', 'airshows', 'vehicle-sports', 
    'community', 'religious', 'education', 'veteran', 'cookout', 'networking',
    'fair-festival', 'diving', 'shopping', 'health', 'outdoors', 'photography', 'family', 
    'gaming', 'real-estate', 'adventure', 'seasonal', 'other'
]
import os
import re
import sqlite3
import logging
import time
import json
import traceback
from typing import Optional, List, Dict, Any
from datetime import datetime, timedelta
from contextlib import contextmanager
from enum import Enum
import asyncio
import threading

# Import SEO utilities
try:
    from seo_utils import SEOEventProcessor, generate_event_json_ld, generate_seo_metadata, slugify
except ImportError:
    print("‚ö†Ô∏è SEO utilities not available - install seo_utils.py for full SEO features")
    def slugify(text): return text.lower().replace(' ', '-')
    SEOEventProcessor = None

import uvicorn
from dotenv import load_dotenv

from fastapi import FastAPI, HTTPException, Depends, status, BackgroundTasks, Request, Header

# MissionOps import
try:
    from missionops_endpoints import missionops_router
except ImportError:
    missionops_router = None
from fastapi.security import OAuth2PasswordBearer, OAuth2PasswordRequestForm
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, Response, JSONResponse

from pydantic import BaseModel, EmailStr, validator
from passlib.context import CryptContext
import jwt

# Scheduler imports
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.interval import IntervalTrigger
import httpx

# Load environment variables
load_dotenv()

# Security configuration
SECRET_KEY = os.getenv("SECRET_KEY", "fallback-secret-key-for-development-only")
ALGORITHM = os.getenv("ALGORITHM", "HS256")
ACCESS_TOKEN_EXPIRE_MINUTES = int(os.getenv("ACCESS_TOKEN_EXPIRE_MINUTES", 30))

# Environment configuration 
IS_PRODUCTION = os.getenv("RENDER", False) or os.getenv("RAILWAY_ENVIRONMENT", False)
DB_URL = os.getenv("DATABASE_URL", None)

# Stripe configuration
stripe.api_key = os.getenv("STRIPE_SECRET_KEY")
STRIPE_PUBLISHABLE_KEY = os.getenv("STRIPE_PUBLISHABLE_KEY")
STRIPE_WEBHOOK_SECRET = os.getenv("STRIPE_WEBHOOK_SECRET")
STRIPE_PRICE_ID = os.getenv("STRIPE_PRICE_ID")  # Monthly subscription price ID

# Password hashing setup
pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")
oauth2_scheme = OAuth2PasswordBearer(tokenUrl="token")

# Logging setup
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Create FastAPI app
app = FastAPI(title="EventFinder API")

# Register MissionOps router
if missionops_router:
    app.include_router(missionops_router)
    print("‚úÖ MissionOps endpoints registered")

# Dynamic CORS origins based on environment
def get_cors_origins():
    base_origins = [
        "http://localhost:5173",  # Local Vite dev server
        "http://127.0.0.1:5173",
        "http://localhost:5174",  # Admin dashboard local
        "http://127.0.0.1:5174",
    ]
    
    if IS_PRODUCTION:
        # In production, add common Render.com patterns including the actual deployed URL
        production_origins = [
            "https://todoevents.onrender.com",  # Actual frontend URL
            "https://eventfinder-api.onrender.com",
            "https://eventfinder-web.onrender.com", 
            "https://eventfinder-admin.onrender.com",
            "https://todoevents-1.onrender.com",
            "https://todoevents-1-frontend.onrender.com", 
            "https://todoevents-1-web.onrender.com",
            "https://todoevents-api.onrender.com",
            "https://todoevents-frontend.onrender.com",
            "https://todoevents-web.onrender.com",
        ]
        return base_origins + production_origins
    else:
        return base_origins
# Custom CORS middleware for flexible Render.com handling
@app.middleware("http")
async def cors_handler(request, call_next):
    # Log the request for debugging (reduced logging for performance)
    if request.url.path == "/api/report-event":
        logger.info(f"üö® CORS MIDDLEWARE: {request.method} {request.url.path}")
        logger.info(f"üö® Headers: {dict(request.headers)}")
    origin = request.headers.get("origin")
    
    # Handle preflight requests
    if request.method == "OPTIONS":
        # Determine allowed origin
        allowed_origin = "*"  # Default fallback
        
        if origin:
            # Allow localhost and 127.0.0.1 for development
            if ("localhost" in origin or "127.0.0.1" in origin):
                allowed_origin = origin
            # Allow Render.com domains for production
            elif ".onrender.com" in origin:
                allowed_origin = origin
            # Allow the actual domain
            elif "todo-events.com" in origin:
                allowed_origin = origin
            else:
                allowed_origin = origin
        
        # Return proper preflight response
        return Response(
            status_code=200,
            headers={
                "Access-Control-Allow-Origin": allowed_origin,
                "Access-Control-Allow-Methods": "GET, POST, PUT, DELETE, OPTIONS",
                "Access-Control-Allow-Headers": "Content-Type, Authorization, X-Requested-With, Cache-Control, Pragma",
                "Access-Control-Allow-Credentials": "true",
                "Access-Control-Max-Age": "86400",
            }
        )
    
    try:
        # Call the next middleware or endpoint handler
        response = await call_next(request)
        
        # Add CORS headers to successful responses
        if origin:
            # Determine allowed origin for response
            allowed_origin = "*"
            if ("localhost" in origin or "127.0.0.1" in origin or 
                ".onrender.com" in origin or "todo-events.com" in origin):
                allowed_origin = origin
            
            response.headers["Access-Control-Allow-Origin"] = allowed_origin
            response.headers["Access-Control-Allow-Credentials"] = "true"
            response.headers["Access-Control-Allow-Methods"] = "GET, POST, PUT, DELETE, OPTIONS"
            response.headers["Access-Control-Allow-Headers"] = "Content-Type, Authorization, X-Requested-With, Cache-Control, Pragma"
        
        return response
        
    except Exception as e:
        # Handle exceptions and ensure CORS headers are added to error responses
        logger.error(f"Error during request processing: {str(e)}")
        
        # Create response with error details
        status_code = 500
        if isinstance(e, HTTPException):
            status_code = e.status_code
            
        error_response = JSONResponse(
            status_code=status_code,
            content={"detail": str(e)},
        )
        
        # Add CORS headers to error response
        if origin:
            allowed_origin = "*"
            if ("localhost" in origin or "127.0.0.1" in origin or 
                ".onrender.com" in origin or "todo-events.com" in origin):
                allowed_origin = origin
                
            error_response.headers["Access-Control-Allow-Origin"] = allowed_origin
            error_response.headers["Access-Control-Allow-Credentials"] = "true"
            
        return error_response

# CORS middleware with dynamic origins (as fallback)
# Commenting out the built-in CORS middleware since we have custom CORS handling
# app.add_middleware(
#     CORSMiddleware,
#     allow_origins=get_cors_origins(),
#     allow_credentials=True,
#     allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
#     allow_headers=["*"],
#     max_age=86400,  # Cache preflight requests for 24 hours
# )

# Database file for SQLite (development only)
DB_FILE = os.path.join(os.path.dirname(__file__), "events.db")

# Database initialization
# Force production database migration for interest/view tracking - v2.1
def init_db():
    try:
        with get_db() as conn:
            c = conn.cursor()
            
            if IS_PRODUCTION and DB_URL:
                # PostgreSQL table creation
                
                # Create users table
                c.execute('''
                    CREATE TABLE IF NOT EXISTS users (
                        id SERIAL PRIMARY KEY,
                        email TEXT UNIQUE NOT NULL,
                        hashed_password TEXT NOT NULL,
                        role TEXT NOT NULL,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                ''')
                
                # Create events table with user relationship
                c.execute('''
                    CREATE TABLE IF NOT EXISTS events (
                        id SERIAL PRIMARY KEY,
                        title TEXT NOT NULL,
                        description TEXT NOT NULL,
                        date TEXT NOT NULL,
                        start_time TEXT NOT NULL,
                        end_time TEXT,
                        end_date TEXT,
                        category TEXT NOT NULL,
                        secondary_category TEXT,
                        address TEXT NOT NULL,
                        lat REAL NOT NULL,
                        lng REAL NOT NULL,
                        recurring BOOLEAN NOT NULL DEFAULT FALSE,
                        frequency TEXT,
                        created_by INTEGER,
                        interest_count INTEGER DEFAULT 0,
                        view_count INTEGER DEFAULT 0,
                        fee_required TEXT,
                        event_url TEXT,
                        host_name TEXT,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        verified BOOLEAN DEFAULT FALSE,
                        FOREIGN KEY(created_by) REFERENCES users(id)
                    )
                ''')
                
                # Add migration for existing events to have start_time and end_time
                try:
                    # Use PostgreSQL-compatible column checking method with fallback
                    def column_exists(table_name, column_name):
                        try:
                            # Try information_schema first
                            c.execute("""
                                SELECT column_name FROM information_schema.columns 
                                WHERE table_name = %s AND column_name = %s
                            """, (table_name, column_name))
                            return c.fetchone() is not None
                        except Exception:
                            # Fallback: try to add column and catch error if it exists
                            try:
                                c.execute(f'ALTER TABLE {table_name} ADD COLUMN {column_name}_test_temp TEXT')
                                c.execute(f'ALTER TABLE {table_name} DROP COLUMN {column_name}_test_temp')
                                # If we got here, column doesn't exist
                                return False
                            except Exception:
                                # If adding temp column failed, assume original column exists
                                return True
                    
                    # Check if we need to migrate old 'time' column to 'start_time'
                    if column_exists('events', 'time'):
                        # Migrate old 'time' column to 'start_time'
                        c.execute('ALTER TABLE events RENAME COLUMN time TO start_time')
                        logger.info("‚úÖ Migrated 'time' column to 'start_time'")
                        conn.commit()
                        
                    # Add start_time column if it doesn't exist
                    if not column_exists('events', 'start_time'):
                        c.execute('ALTER TABLE events ADD COLUMN start_time TEXT DEFAULT \'12:00\'')
                        logger.info("‚úÖ Added 'start_time' column")
                        conn.commit()
                        
                    # Add end_time column if it doesn't exist
                    if not column_exists('events', 'end_time'):
                        c.execute('ALTER TABLE events ADD COLUMN end_time TEXT')
                        logger.info("‚úÖ Added 'end_time' column")
                        conn.commit()
                        
                    # Add end_date column if it doesn't exist
                    if not column_exists('events', 'end_date'):
                        c.execute('ALTER TABLE events ADD COLUMN end_date TEXT')
                        logger.info("‚úÖ Added 'end_date' column")
                        conn.commit()
                    
                    # Add interest_count column if it doesn't exist
                    if not column_exists('events', 'interest_count'):
                        c.execute('ALTER TABLE events ADD COLUMN interest_count INTEGER DEFAULT 0')
                        logger.info("‚úÖ Added 'interest_count' column")
                        conn.commit()
                    
                    # Add view_count column if it doesn't exist
                    if not column_exists('events', 'view_count'):
                        c.execute('ALTER TABLE events ADD COLUMN view_count INTEGER DEFAULT 0')
                        logger.info("‚úÖ Added 'view_count' column")
                        conn.commit()
                    
                    # Add new UX enhancement columns
                    if not column_exists('events', 'fee_required'):
                        c.execute('ALTER TABLE events ADD COLUMN fee_required TEXT')
                        logger.info("‚úÖ Added 'fee_required' column")
                        conn.commit()
                    
                    if not column_exists('events', 'event_url'):
                        c.execute('ALTER TABLE events ADD COLUMN event_url TEXT')
                        logger.info("‚úÖ Added 'event_url' column")
                        conn.commit()
                    
                    if not column_exists('events', 'host_name'):
                        c.execute('ALTER TABLE events ADD COLUMN host_name TEXT')
                        logger.info("‚úÖ Added 'host_name' column")
                        conn.commit()
                    
                    # Add secondary_category column if it doesn't exist
                    if not column_exists('events', 'secondary_category'):
                        c.execute('ALTER TABLE events ADD COLUMN secondary_category TEXT')
                        logger.info("‚úÖ Added 'secondary_category' column")
                        conn.commit()
                        
                    # Add verified column if it doesn't exist
                    if not column_exists('events', 'verified'):
                        c.execute('ALTER TABLE events ADD COLUMN verified BOOLEAN DEFAULT FALSE')
                        logger.info("‚úÖ Added 'verified' column")
                        conn.commit()
                        
                except Exception as migration_error:
                    logger.error(f"‚ùå Schema fix error: {migration_error}")
                    # Don't fail the entire initialization, just log the error
                    try:
                        conn.rollback()
                    except:
                        pass
                
                # Create activity_logs table
                c.execute('''
                    CREATE TABLE IF NOT EXISTS activity_logs (
                        id SERIAL PRIMARY KEY,
                        user_id INTEGER,
                        action TEXT NOT NULL,
                        details TEXT,
                        timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        FOREIGN KEY(user_id) REFERENCES users(id)
                    )
                ''')
                
                # Create interest tracking table
                c.execute('''CREATE TABLE IF NOT EXISTS event_interests (
                            id SERIAL PRIMARY KEY,
                            event_id INTEGER NOT NULL,
                            user_id INTEGER,
                            browser_fingerprint TEXT NOT NULL DEFAULT 'legacy',
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            UNIQUE(event_id, user_id, browser_fingerprint),
                            FOREIGN KEY(event_id) REFERENCES events(id) ON DELETE CASCADE,
                            FOREIGN KEY(user_id) REFERENCES users(id) ON DELETE SET NULL
                        )''')
                
                # Create view tracking table
                c.execute('''CREATE TABLE IF NOT EXISTS event_views (
                            id SERIAL PRIMARY KEY,
                            event_id INTEGER NOT NULL,
                            user_id INTEGER,
                            browser_fingerprint TEXT NOT NULL DEFAULT 'legacy',
                            viewed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            UNIQUE(event_id, user_id, browser_fingerprint),
                            FOREIGN KEY(event_id) REFERENCES events(id) ON DELETE CASCADE,
                            FOREIGN KEY(user_id) REFERENCES users(id) ON DELETE SET NULL
                        )''')
                
                # Create page visits tracking table
                c.execute('''CREATE TABLE IF NOT EXISTS page_visits (
                            id SERIAL PRIMARY KEY,
                            page_type TEXT NOT NULL,
                            page_path TEXT NOT NULL,
                            user_id INTEGER,
                            browser_fingerprint TEXT NOT NULL DEFAULT 'anonymous',
                            visited_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            FOREIGN KEY(user_id) REFERENCES users(id) ON DELETE SET NULL
                        )''')

                # Create privacy requests table for CCPA compliance
                c.execute('''CREATE TABLE IF NOT EXISTS privacy_requests (
                            id SERIAL PRIMARY KEY,
                            request_type TEXT NOT NULL CHECK (request_type IN ('access', 'delete', 'opt_out')),
                            email TEXT NOT NULL,
                            full_name TEXT,
                            verification_info TEXT,
                            details TEXT,
                            status TEXT DEFAULT 'pending' CHECK (status IN ('pending', 'in_progress', 'completed', 'denied')),
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            completed_at TIMESTAMP,
                            admin_notes TEXT
                        )''')

                # Create event reports table for reporting functionality
                c.execute('''CREATE TABLE IF NOT EXISTS event_reports (
                            id SERIAL PRIMARY KEY,
                            event_id INTEGER NOT NULL,
                            event_title TEXT,
                            event_address TEXT,
                            event_date TEXT,
                            reason TEXT NOT NULL,
                            category TEXT NOT NULL,
                            description TEXT NOT NULL,
                            reporter_email TEXT NOT NULL,
                            reporter_name TEXT,
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            status TEXT DEFAULT 'pending',
                            FOREIGN KEY(event_id) REFERENCES events(id) ON DELETE CASCADE
                        )''')
                
                # Ensure tables are committed
                conn.commit()
                logger.info("‚úÖ Interest, view tracking, privacy requests, and event reports tables created/verified")
            else:
                # SQLite table creation (existing code)
                c.execute('''CREATE TABLE IF NOT EXISTS users (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            email TEXT UNIQUE NOT NULL,
                            hashed_password TEXT NOT NULL,
                            role TEXT NOT NULL,
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                        )''')
                
                # Check if events table exists and needs migration
                c.execute('''PRAGMA table_info(events)''')
                columns = [column[1] for column in c.fetchall()]
                
                if 'events' not in [table[0] for table in c.execute("SELECT name FROM sqlite_master WHERE type='table'").fetchall()]:
                    # Create new table with updated schema
                    c.execute('''CREATE TABLE events (
                                id INTEGER PRIMARY KEY AUTOINCREMENT,
                                title TEXT NOT NULL,
                                description TEXT NOT NULL,
                                date TEXT NOT NULL,
                                start_time TEXT NOT NULL,
                                end_time TEXT,
                                end_date TEXT,
                                category TEXT NOT NULL,
                                secondary_category TEXT,
                                address TEXT NOT NULL,
                                lat REAL NOT NULL,
                                lng REAL NOT NULL,
                                recurring BOOLEAN NOT NULL DEFAULT 0,
                                frequency TEXT,
                                created_by INTEGER,
                                interest_count INTEGER DEFAULT 0,
                                view_count INTEGER DEFAULT 0,
                                fee_required TEXT,
                                event_url TEXT,
                                host_name TEXT,
                                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                                verified BOOLEAN DEFAULT FALSE,
                                FOREIGN KEY(created_by) REFERENCES users(id)
                            )''')
                else:
                    # Migrate existing table
                    if 'time' in columns and 'start_time' not in columns:
                        # Rename time to start_time
                        c.execute('''ALTER TABLE events RENAME COLUMN time TO start_time''')
                        logger.info("Migrated 'time' column to 'start_time'")
                    
                    if 'start_time' not in columns:
                        # Add start_time column with default value from time column if it exists
                        if 'time' in columns:
                            c.execute('''ALTER TABLE events ADD COLUMN start_time TEXT''')
                            c.execute('''UPDATE events SET start_time = time WHERE start_time IS NULL''')
                            logger.info("Added start_time column and migrated from time")
                        else:
                            c.execute('''ALTER TABLE events ADD COLUMN start_time TEXT DEFAULT "12:00"''')
                            logger.info("Added start_time column with default value")
                    
                    if 'end_time' not in columns:
                        c.execute('''ALTER TABLE events ADD COLUMN end_time TEXT''')
                        logger.info("Added end_time column")
                    
                    # Add interest_count and view_count columns if they don't exist
                    if 'interest_count' not in columns:
                        c.execute('''ALTER TABLE events ADD COLUMN interest_count INTEGER DEFAULT 0''')
                        logger.info("Added interest_count column")
                    
                    if 'view_count' not in columns:
                        c.execute('''ALTER TABLE events ADD COLUMN view_count INTEGER DEFAULT 0''')
                        logger.info("Added view_count column")
                    
                    # Add new UX enhancement columns for SQLite
                    if 'fee_required' not in columns:
                        c.execute('''ALTER TABLE events ADD COLUMN fee_required TEXT''')
                        logger.info("Added fee_required column")
                    
                    if 'event_url' not in columns:
                        c.execute('''ALTER TABLE events ADD COLUMN event_url TEXT''')
                        logger.info("Added event_url column")
                    
                    if 'host_name' not in columns:
                        c.execute('''ALTER TABLE events ADD COLUMN host_name TEXT''')
                        logger.info("Added host_name column")
                    
                    # Add secondary_category column if it doesn't exist  
                    if 'secondary_category' not in columns:
                        c.execute('''ALTER TABLE events ADD COLUMN secondary_category TEXT''')
                        logger.info("Added secondary_category column")
                    
                    # Add verified column if it doesn't exist
                    if 'verified' not in columns:
                        c.execute('''ALTER TABLE events ADD COLUMN verified BOOLEAN DEFAULT FALSE''')
                        logger.info("Added verified column")
                
                c.execute('''CREATE TABLE IF NOT EXISTS activity_logs (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            user_id INTEGER,
                            action TEXT NOT NULL,
                            details TEXT,
                            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                            FOREIGN KEY(user_id) REFERENCES users(id)
                        )''')
                
                # Create interest tracking table
                c.execute('''CREATE TABLE IF NOT EXISTS event_interests (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            event_id INTEGER NOT NULL,
                            user_id INTEGER,
                            browser_fingerprint TEXT NOT NULL DEFAULT 'legacy',
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            UNIQUE(event_id, user_id, browser_fingerprint),
                            FOREIGN KEY(event_id) REFERENCES events(id) ON DELETE CASCADE,
                            FOREIGN KEY(user_id) REFERENCES users(id) ON DELETE SET NULL
                        )''')
                
                # Create view tracking table
                c.execute('''CREATE TABLE IF NOT EXISTS event_views (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            event_id INTEGER NOT NULL,
                            user_id INTEGER,
                            browser_fingerprint TEXT NOT NULL DEFAULT 'legacy',
                            viewed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            UNIQUE(event_id, user_id, browser_fingerprint),
                            FOREIGN KEY(event_id) REFERENCES events(id) ON DELETE CASCADE,
                            FOREIGN KEY(user_id) REFERENCES users(id) ON DELETE SET NULL
                        )''')
                
                # Create page visits tracking table
                c.execute('''CREATE TABLE IF NOT EXISTS page_visits (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            page_type TEXT NOT NULL,
                            page_path TEXT NOT NULL,
                            user_id INTEGER,
                            browser_fingerprint TEXT NOT NULL DEFAULT 'anonymous',
                            visited_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            FOREIGN KEY(user_id) REFERENCES users(id) ON DELETE SET NULL
                        )''')

                # Create privacy requests table for CCPA compliance
                c.execute('''CREATE TABLE IF NOT EXISTS privacy_requests (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            request_type TEXT NOT NULL CHECK (request_type IN ('access', 'delete', 'opt_out')),
                            email TEXT NOT NULL,
                            full_name TEXT,
                            verification_info TEXT,
                            details TEXT,
                            status TEXT DEFAULT 'pending' CHECK (status IN ('pending', 'in_progress', 'completed', 'denied')),
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            completed_at TIMESTAMP,
                            admin_notes TEXT
                        )''')

                # Create event reports table for reporting functionality
                c.execute('''CREATE TABLE IF NOT EXISTS event_reports (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            event_id INTEGER NOT NULL,
                            event_title TEXT,
                            event_address TEXT,
                            event_date TEXT,
                            reason TEXT NOT NULL,
                            category TEXT NOT NULL,
                            description TEXT NOT NULL,
                            reporter_email TEXT NOT NULL,
                            reporter_name TEXT,
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            status TEXT DEFAULT 'pending',
                            FOREIGN KEY(event_id) REFERENCES events(id) ON DELETE CASCADE
                        )''')
                
                # SQLite does not have information_schema, so we skip the schema fixes
                # The tracking, privacy, and reporting tables are created correctly above for SQLite
            
            # Ensure password_resets table exists
            if IS_PRODUCTION and DB_URL:
                c.execute('''
                    CREATE TABLE IF NOT EXISTS password_resets (
                        id SERIAL PRIMARY KEY,
                        email TEXT NOT NULL,
                        reset_code TEXT NOT NULL,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        expires_at TIMESTAMP NOT NULL
                    )
                ''')
            else:
                c.execute('''
                    CREATE TABLE IF NOT EXISTS password_resets (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        email TEXT NOT NULL,
                        reset_code TEXT NOT NULL,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        expires_at TIMESTAMP NOT NULL
                    )
                ''')
            
            # Check for default admin user and create if needed
            create_default_admin_user(conn)
            
    except Exception as e:
        logger.error(f"Error initializing database: {str(e)}")
        # Don't raise the exception here - this allows the app to start even if DB init fails
        # We'll handle DB errors at the endpoint level

def create_default_admin_user(conn):
    """Create default admin user if none exists"""
    try:
        cursor = conn.cursor()
        
        # Check if any admin users exist
        cursor.execute("SELECT COUNT(*) FROM users WHERE role = 'admin'")
        admin_count = get_count_from_result(cursor.fetchone())
        
        if admin_count == 0:
            logger.info("No admin users found. Creating default admin user...")
            
            # Use a secure random password that must be changed
            import secrets
            import string
            
            # Generate a random 16-character password
            alphabet = string.ascii_letters + string.digits + "!@#$%^&*"
            admin_password = ''.join(secrets.choice(alphabet) for i in range(16))
            
            hashed_password = get_password_hash(admin_password)
            
            cursor.execute(
                "INSERT INTO users (email, hashed_password, role) VALUES (?, ?, ?)",
                ("admin@todo-events.com", hashed_password, "admin")
            )
            conn.commit()
            
            logger.info("‚úÖ Default admin user created:")
            logger.info(f"   üìß Email: admin@todo-events.com")
            logger.info(f"   üîë Password: {admin_password}")
            logger.info("‚ö†Ô∏è IMPORTANT: Change this password after first login!")
            
        else:
            logger.info(f"Found {admin_count} existing admin users. Default admin creation skipped.")
            
    except Exception as e:
        logger.error(f"Error creating default admin user: {str(e)}")
        return False
    
    return True

# Initialize database
try:
    init_db()
except Exception as e:
    logger.error(f"Database initialization failed: {str(e)}")
class AutomatedTaskManager:
    """
    Manages automated tasks for AI search optimization:
    - Sitemap generation and updates
    - Event data refresh
    - AI tool synchronization
    - Performance monitoring
    """
    
    def __init__(self):
        self.scheduler = BackgroundScheduler()
        self.last_sitemap_update = None
        self.last_event_refresh = None
        self.task_status = {
            "sitemap_generation": {"status": "pending", "last_run": None, "next_run": None},
            "event_refresh": {"status": "pending", "last_run": None, "next_run": None},
            "ai_sync": {"status": "pending", "last_run": None, "next_run": None}
        }
        
    async def generate_sitemap_automatically(self):
        """Generate sitemap with current event data"""
        try:
            logger.info("üîÑ Starting automated sitemap generation...")
            
            # Get current events from database
            events = await self.get_current_events()
            
            # Generate sitemap content
            sitemap_content = await self.build_sitemap_content(events)
            
            # Save to file (in production, this could be uploaded to S3/CDN)
            await self.save_sitemap(sitemap_content)
            
            # Update task status
            self.task_status["sitemap_generation"]["status"] = "completed"
            self.task_status["sitemap_generation"]["last_run"] = datetime.utcnow().isoformat()
            self.last_sitemap_update = datetime.utcnow()
            
            logger.info("‚úÖ Automated sitemap generation completed successfully")
            
            # Notify search engines about the update
            await self.ping_search_engines()
            
        except Exception as e:
            logger.error(f"‚ùå Automated sitemap generation failed: {str(e)}")
            self.task_status["sitemap_generation"]["status"] = "failed"
    
    async def refresh_event_data(self):
        """Refresh and optimize event data for AI consumption"""
        try:
            logger.info("üîÑ Starting automated event data refresh...")
            
            # Clean up expired events
            await self.cleanup_expired_events()
            
            # Update event search index (if implemented)
            await self.update_search_index()
            
            # Cache popular queries for faster AI responses
            await self.cache_popular_queries()
            
            self.task_status["event_refresh"]["status"] = "completed"
            self.task_status["event_refresh"]["last_run"] = datetime.utcnow().isoformat()
            self.last_event_refresh = datetime.utcnow()
            
            logger.info("‚úÖ Automated event data refresh completed")
            
        except Exception as e:
            logger.error(f"‚ùå Automated event data refresh failed: {str(e)}")
            self.task_status["event_refresh"]["status"] = "failed"
    
    async def sync_with_ai_tools(self):
        """Optimize data specifically for AI tool consumption"""
        try:
            logger.info("ü§ñ Starting AI tools synchronization...")
            
            # Update AI-optimized cache
            await self.update_ai_cache()
            
            # Test AI endpoint responsiveness
            await self.test_ai_endpoint()
            
            # Update structured data if needed
            await self.validate_structured_data()
            
            self.task_status["ai_sync"]["status"] = "completed"
            self.task_status["ai_sync"]["last_run"] = datetime.utcnow().isoformat()
            
            logger.info("‚úÖ AI tools synchronization completed")
            
        except Exception as e:
            logger.error(f"‚ùå AI tools synchronization failed: {str(e)}")
            self.task_status["ai_sync"]["status"] = "failed"
    
    async def get_current_events(self):
        """Get current and upcoming events from database"""
        try:
            with get_db() as conn:
                c = conn.cursor()
                
                # Use proper date comparison for both PostgreSQL and SQLite
                if IS_PRODUCTION and DB_URL:
                    # PostgreSQL - cast date text to date for comparison
                    c.execute("""
                        SELECT id, title, description, date, start_time, end_time, end_date, category, 
                               address, lat, lng, created_at, slug, is_published
                        FROM events 
                        WHERE CAST(date AS DATE) >= (CURRENT_DATE - INTERVAL '30 days')::DATE 
                        AND (is_published = true OR is_published IS NULL)
                        ORDER BY CAST(date AS DATE), start_time
                    """)
                else:
                    # SQLite
                    c.execute("""
                        SELECT id, title, description, date, start_time, end_time, end_date, category, 
                               address, lat, lng, created_at, slug, is_published
                        FROM events 
                        WHERE date::date >= CURRENT_DATE AND (is_published = 1 OR is_published IS NULL)
                        ORDER BY date, start_time
                    """)
                return [dict(row) for row in c.fetchall()]
        except Exception as e:
            logger.error(f"Error fetching events for sitemap: {str(e)}")
            return []
    
    async def build_sitemap_content(self, events):
        """Build sitemap XML content with current events"""
        current_date = datetime.utcnow().strftime('%Y-%m-%d')
        domain = "https://todo-events.com"
        
        # Start with static sitemap structure
        sitemap = f'''<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9"
        xmlns:image="http://www.google.com/schemas/sitemap-image/1.1">

  <!-- Homepage - Primary landing page -->
  <url>
    <loc>{domain}/</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>daily</changefreq>
    <priority>1.0</priority>
    <image:image>
      <image:loc>{domain}/images/pin-logo.svg</image:loc>
      <image:caption>todo-events logo - Local event discovery platform</image:caption>
    </image:image>
  </url>

  <!-- Main navigation pages -->
  <url>
    <loc>{domain}/hosts</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>weekly</changefreq>
    <priority>0.8</priority>
  </url>
  <url>
    <loc>{domain}/creators</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>weekly</changefreq>
    <priority>0.8</priority>
  </url>'''
        
        # Add category pages - both query param and SEO-friendly formats
        categories = [
    'food-drink', 'music', 'arts', 'sports', 'automotive', 'airshows', 'vehicle-sports', 
    'community', 'religious', 'education', 'veteran', 'cookout', 'networking',
    'fair-festival', 'diving', 'shopping', 'health', 'outdoors', 'photography', 'family', 
    'gaming', 'real-estate', 'adventure', 'seasonal', 'other'
]
        sitemap += f'''

  <!-- Category pages -->'''
        
        for category in categories:
            sitemap += f'''
  <url>
    <loc>{domain}/events/{category}</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>daily</changefreq>
    <priority>0.8</priority>
  </url>'''
        
        # Add individual event pages with multiple URL formats
        sitemap += f'''

  <!-- Individual Event Pages -->'''
        
        for event in events[:100]:  # Limit to avoid huge sitemaps
            # Get dynamic lastmod date from event
            event_lastmod = current_date
            if event.get('updated_at'):
                try:
                    parsed_date = datetime.fromisoformat(str(event['updated_at']).replace('Z', '+00:00'))
                    event_lastmod = parsed_date.strftime('%Y-%m-%d')
                except:
                    pass
            elif event.get('created_at'):
                try:
                    parsed_date = datetime.fromisoformat(str(event['created_at']).replace('Z', '+00:00'))
                    event_lastmod = parsed_date.strftime('%Y-%m-%d')
                except:
                    pass
            
            # Check if event has a slug for SEO-friendly URL
            if event.get('slug'):
                event_slug = event['slug']
                
                # Generate canonical URL based on location data
                if event.get('city') and event.get('state'):
                    # Location-based URL: /us/state/city/events/slug
                    state_lower = event.get('state', '').lower()

                    city_slug = slugify(event.get('city', ''))
                    canonical_url = f"{domain}/us/{state_lower}/{city_slug}/events/{event_slug}"
                    priority = "0.9"  # Higher priority for geo-specific URLs
                else:
                    # Fallback URL: /events/slug
                    canonical_url = f"{domain}/events/{event_slug}"
                    priority = "0.85"
                
                sitemap += f'''
  <url>
    <loc>{canonical_url}</loc>
    <lastmod>{event_lastmod}</lastmod>
    <changefreq>monthly</changefreq>
    <priority>{priority}</priority>
  </url>'''
                
                # Add short URL for compatibility
                sitemap += f'''
  <url>
    <loc>{domain}/e/{event_slug}</loc>
    <lastmod>{event_lastmod}</lastmod>
    <changefreq>monthly</changefreq>
    <priority>0.7</priority>
  </url>'''
                
                # Add date-indexed URL if we have event date
                try:
                    event_date_obj = datetime.fromisoformat(str(event.get('date', current_date)))
                    year = event_date_obj.strftime('%Y')
                    month = event_date_obj.strftime('%m')
                    day = event_date_obj.strftime('%d')
                    
                    sitemap += f'''
  <url>
    <loc>{domain}/events/{year}/{month}/{day}/{event_slug}</loc>
    <lastmod>{event_lastmod}</lastmod>
    <changefreq>monthly</changefreq>
    <priority>0.75</priority>
  </url>'''
                except:
                    pass  # Skip date-indexed URL if date parsing fails
        
        # Add time-based event discovery pages
        sitemap += f'''

  <!-- Time-based Event Discovery -->
  <url>
    <loc>{domain}/events-today</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>hourly</changefreq>
    <priority>0.9</priority>
  </url>
  <url>
    <loc>{domain}/events-tonight</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>hourly</changefreq>
    <priority>0.8</priority>
  </url>
  <url>
    <loc>{domain}/events-this-weekend</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>daily</changefreq>
    <priority>0.9</priority>
  </url>
  <url>
    <loc>{domain}/events-tomorrow</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>daily</changefreq>
    <priority>0.8</priority>
  </url>
  <url>
    <loc>{domain}/events-this-week</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>daily</changefreq>
    <priority>0.8</priority>
  </url>'''

        # Add location-based discovery pages for major cities
        major_cities = [
            'new-york', 'los-angeles', 'chicago', 'houston', 'phoenix', 'philadelphia',
            'san-antonio', 'san-diego', 'dallas', 'san-jose', 'austin', 'jacksonville',
            'fort-worth', 'columbus', 'charlotte', 'san-francisco', 'indianapolis',
            'seattle', 'denver', 'washington', 'boston', 'nashville', 'detroit',
            'portland', 'memphis', 'las-vegas', 'miami', 'atlanta', 'milwaukee'
        ]

        sitemap += f'''

  <!-- "This Weekend in [City]" Pages -->'''
        
        for city in major_cities[:25]:  # Limit to top 25 cities
            sitemap += f'''
  <url>
    <loc>{domain}/this-weekend-in-{city}</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>daily</changefreq>
    <priority>0.75</priority>
  </url>'''

        sitemap += f'''

  <!-- "Free Events in [City]" Pages -->'''
        
        for city in major_cities[:25]:  # Limit to top 25 cities
            sitemap += f'''
  <url>
    <loc>{domain}/free-events-in-{city}</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>daily</changefreq>
    <priority>0.7</priority>
  </url>'''

        sitemap += f'''

  <!-- "Today in [City]" Pages -->'''
        
        for city in major_cities[:15]:  # Limit to top 15 cities for today
            sitemap += f'''
  <url>
    <loc>{domain}/today-in-{city}</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>hourly</changefreq>
    <priority>0.8</priority>
  </url>'''

        sitemap += f'''

  <!-- "Tonight in [City]" Pages -->'''
        
        for city in major_cities[:15]:  # Limit to top 15 cities for tonight
            sitemap += f'''
  <url>
    <loc>{domain}/tonight-in-{city}</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>hourly</changefreq>
    <priority>0.75</priority>
  </url>'''

        # Add general discovery and AI-optimized pages
        sitemap += f'''

  <!-- General Discovery Pages -->
  <url>
    <loc>{domain}/local-events-near-me</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>daily</changefreq>
    <priority>0.9</priority>
  </url>
  <url>
    <loc>{domain}/near-me</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>daily</changefreq>
    <priority>0.8</priority>
  </url>
  <url>
    <loc>{domain}/free-events-near-me</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>daily</changefreq>
    <priority>0.8</priority>
  </url>
  <url>
    <loc>{domain}/live-music-near-me</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>daily</changefreq>
    <priority>0.7</priority>
  </url>
  <url>
    <loc>{domain}/food-festivals-near-me</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>weekly</changefreq>
    <priority>0.7</priority>
  </url>
  <url>
    <loc>{domain}/art-events-near-me</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>weekly</changefreq>
    <priority>0.7</priority>
  </url>
  <url>
    <loc>{domain}/outdoor-events</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>daily</changefreq>
    <priority>0.7</priority>
  </url>
  <url>
    <loc>{domain}/family-friendly-events</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>daily</changefreq>
    <priority>0.7</priority>
  </url>
  <url>
    <loc>{domain}/discover</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>weekly</changefreq>
    <priority>0.6</priority>
  </url>

  <!-- Static Pages -->
  <url>
    <loc>{domain}/about</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>monthly</changefreq>
    <priority>0.6</priority>
  </url>
  <url>
    <loc>{domain}/how-it-works</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>monthly</changefreq>
    <priority>0.6</priority>
  </url>
  <url>
    <loc>{domain}/create-event</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>weekly</changefreq>
    <priority>0.7</priority>
  </url>
  <url>
    <loc>{domain}/contact</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>monthly</changefreq>
    <priority>0.5</priority>
  </url>
  <url>
    <loc>{domain}/privacy</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>yearly</changefreq>
    <priority>0.3</priority>
  </url>
  <url>
    <loc>{domain}/terms</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>yearly</changefreq>
    <priority>0.3</priority>
  </url>
</urlset>
<!-- Note: This sitemap was automatically generated on {current_date} -->
<!-- Contains {len(events)} individual events and comprehensive SEO URLs -->
'''
        
        return sitemap
    
    async def save_sitemap(self, content):
        """Save sitemap content to file"""
        # In production, you might want to save to a shared storage like S3
        # For now, we'll create an endpoint to serve the dynamic sitemap
        self.current_sitemap = content
        logger.info(f"Sitemap updated with {content.count('<url>')} URLs")
    
    async def ping_search_engines(self):
        """Notify search engines about sitemap updates"""
        urls = [
            "https://www.google.com/ping?sitemap=https://todo-events.com/sitemap.xml",
            "https://www.bing.com/ping?sitemap=https://todo-events.com/sitemap.xml"
        ]
        
        async with httpx.AsyncClient() as client:
            for url in urls:
                try:
                    response = await client.get(url, timeout=10)
                    logger.info(f"‚úÖ Pinged search engine: {url} - Status: {response.status_code}")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Failed to ping search engine: {url} - Error: {str(e)}")
    
    async def cleanup_expired_events(self):
        """Enhanced cleanup of expired events with 32-day archive policy"""
        try:
            logger.info("üßπ Starting enhanced automated event cleanup...")
            
            with get_db() as conn:
                cursor = conn.cursor()
                placeholder = get_placeholder()
                
                # Get current datetime and 32-day archive cutoff with full timestamp precision
                current_datetime = datetime.utcnow()
                current_date = current_datetime.date()
                archive_cutoff_datetime = current_datetime - timedelta(days=32)  # 32-day archive
                archive_cutoff = archive_cutoff_datetime.date()
                
                logger.info(f"Current datetime: {current_datetime.isoformat()}")
                logger.info(f"Current date: {current_date}")
                logger.info(f"Archive cutoff datetime: {archive_cutoff_datetime.isoformat()}")
                logger.info(f"Archive cutoff date: {archive_cutoff} (events before this will be deleted)")
                
                # First, get expired events for logging
                if placeholder == "?":
                    cursor.execute("""
                        SELECT id, title, date, created_at 
                        FROM events 
                        WHERE date < ? 
                        ORDER BY date DESC
                    """, (str(archive_cutoff),))
                else:
                    cursor.execute("""
                        SELECT id, title, date, created_at 
                        FROM events 
                        WHERE date < %s 
                        ORDER BY date DESC
                    """, (str(archive_cutoff),))
                
                expired_events = cursor.fetchall()
                
                if not expired_events:
                    logger.info("‚úÖ No expired events found beyond 32-day archive period")
                    return
                
                logger.info(f"üìä Found {len(expired_events)} events beyond 32-day archive:")
                for event in expired_events[:5]:  # Log first 5
                    if isinstance(event, dict):
                        event_data = event
                    else:
                        event_data = {
                            'id': event[0], 'title': event[1], 
                            'date': event[2], 'created_at': event[3]
                        }
                    logger.info(f"  - ID {event_data['id']}: '{event_data['title']}' (Date: {event_data['date']})")
                
                if len(expired_events) > 5:
                    logger.info(f"  ... and {len(expired_events) - 5} more")
                
                # Delete expired events and related data
                expired_ids = [event[0] if not isinstance(event, dict) else event['id'] for event in expired_events]
                
                # Delete in transaction for data integrity
                cursor.execute("BEGIN")
                
                try:
                    # Delete related data first (to avoid foreign key issues)
                    for event_id in expired_ids:
                        # Delete event interests
                        if placeholder == "?":
                            cursor.execute("DELETE FROM event_interests WHERE event_id = ?", (event_id,))
                            cursor.execute("DELETE FROM event_views WHERE event_id = ?", (event_id,))
                        else:
                            cursor.execute("DELETE FROM event_interests WHERE event_id = %s", (event_id,))
                            cursor.execute("DELETE FROM event_views WHERE event_id = %s", (event_id,))
                    
                    # Delete the events themselves
                    if placeholder == "?":
                        placeholders = ",".join(["?" for _ in expired_ids])
                        cursor.execute(f"DELETE FROM events WHERE id IN ({placeholders})", expired_ids)
                    else:
                        placeholders = ",".join(["%s" for _ in expired_ids])
                        cursor.execute(f"DELETE FROM events WHERE id IN ({placeholders})", expired_ids)
                    
                    # Commit the transaction
                    cursor.execute("COMMIT")
                    
                    # Clear caches
                    event_cache.clear()
                    
                    logger.info(f"‚úÖ Successfully cleaned up {len(expired_events)} expired events")
                    logger.info(f"üßπ Cleared event cache after cleanup")
                    
                    # Log archive policy  
                    placeholder = get_placeholder()
                    cursor.execute(f"""
                        SELECT COUNT(*) 
                        FROM events 
                        WHERE date >= {placeholder} AND date < {placeholder}
                    """, (str(archive_cutoff), str(current_date)))
                    
                    archived_count = get_count_from_result(cursor.fetchone())
                    logger.info(f"üìÅ {archived_count} events in 32-day archive (from {archive_cutoff} to {current_date})")
                    
                except Exception as cleanup_error:
                    cursor.execute("ROLLBACK")
                    logger.error(f"‚ùå Event cleanup failed, transaction rolled back: {cleanup_error}")
                    raise
                
        except Exception as e:
            logger.error(f"‚ùå Event cleanup automation error: {e}")

    async def update_search_index(self):
        """Update search index after cleanup"""
        try:
            logger.info("üîç Updating search index after event cleanup...")
            # Trigger sitemap regeneration to exclude deleted events
            await self.generate_sitemap_automatically()
            logger.info("‚úÖ Search index updated successfully")
        except Exception as e:
            logger.error(f"‚ùå Search index update error: {e}")
    
    async def cache_popular_queries(self):
        """Cache responses for popular AI queries"""
        popular_locations = [
            (40.7128, -74.0060),  # New York
            (34.0522, -118.2437), # Los Angeles
            (41.8781, -87.6298),  # Chicago
        ]
        
        # Pre-cache popular query responses
        for lat, lng in popular_locations:
            try:
                # This would call our AI endpoint internally to warm the cache
                pass
            except Exception as e:
                logger.warning(f"Failed to cache location {lat}, {lng}: {str(e)}")
    
    async def update_ai_cache(self):
        """Update cached data for AI consumption"""
        logger.info("ü§ñ AI cache updated")
    
    async def test_ai_endpoint(self):
        """Test AI endpoint responsiveness"""
        try:
            # Test our own AI endpoint
            async with httpx.AsyncClient() as client:
                response = await client.get(
                    f"http://localhost:{os.getenv('PORT', 8000)}/api/v1/local-events?limit=1",
                    timeout=5
                )
                if response.status_code == 200:
                    logger.info("‚úÖ AI endpoint test passed")
                else:
                    logger.warning(f"‚ö†Ô∏è AI endpoint test failed: {response.status_code}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è AI endpoint test error: {str(e)}")
    
    async def validate_structured_data(self):
        """Validate structured data markup"""
        logger.info("üìã Structured data validation completed")
    
    def start_scheduler(self):
        """Start the background scheduler"""
        try:
            # Schedule tasks every 6 hours (aligned with AI tool update patterns)
            
            # Create wrapper functions for async tasks
            def run_sitemap_generation():
                """Synchronous wrapper for sitemap generation"""
                try:
                    # Create new event loop if none exists
                    try:
                        loop = asyncio.get_event_loop()
                    except RuntimeError:
                        loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(loop)
                    
                    # Run the async function
                    loop.run_until_complete(self.generate_sitemap_automatically())
                except Exception as e:
                    logger.error(f"Error in sitemap generation wrapper: {str(e)}")

            def run_event_refresh():
                """Synchronous wrapper for event data refresh"""
                try:
                    # Create new event loop if none exists
                    try:
                        loop = asyncio.get_event_loop()
                    except RuntimeError:
                        loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(loop)
                    
                    # Run the async function
                    loop.run_until_complete(self.refresh_event_data())
                except Exception as e:
                    logger.error(f"Error in event refresh wrapper: {str(e)}")

            def run_ai_sync():
                """Synchronous wrapper for AI sync"""
                try:
                    # Create new event loop if none exists
                    try:
                        loop = asyncio.get_event_loop()
                    except RuntimeError:
                        loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(loop)
                    
                    # Run the async function
                    loop.run_until_complete(self.sync_with_ai_tools())
                except Exception as e:
                    logger.error(f"Error in AI sync wrapper: {str(e)}")
            
            # Sitemap generation - every 6 hours
            self.scheduler.add_job(
                func=run_sitemap_generation,
                trigger=IntervalTrigger(hours=6),
                id='sitemap_generation',
                name='Automated Sitemap Generation',
                replace_existing=True
            )
            
            # Event data refresh - every 6 hours (offset by 2 hours)
            self.scheduler.add_job(
                func=run_event_refresh,
                trigger=IntervalTrigger(hours=6, start_date=datetime.utcnow() + timedelta(hours=2)),
                id='event_refresh',
                name='Event Data Refresh',
                replace_existing=True
            )
            
            # AI sync - every 6 hours (offset by 4 hours)
            self.scheduler.add_job(
                func=run_ai_sync,
                trigger=IntervalTrigger(hours=6, start_date=datetime.utcnow() + timedelta(hours=4)),
                id='ai_sync',
                name='AI Tools Synchronization',
                replace_existing=True
            )
            
            # Event cleanup with 32-day archive - every 6 hours (offset by 3 hours)
            def run_event_cleanup():
                try:
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    loop.run_until_complete(self.cleanup_expired_events())
                    loop.close()
                except Exception as e:
                    logger.error(f"Scheduled event cleanup failed: {e}")
            
            self.scheduler.add_job(
                func=run_event_cleanup,
                trigger=IntervalTrigger(hours=6, start_date=datetime.utcnow() + timedelta(hours=3)),
                id='event_cleanup',
                name='Event Cleanup (32-day Archive)',
                replace_existing=True
            )
            
            # Health check - every hour
            self.scheduler.add_job(
                func=self.health_check,
                trigger=IntervalTrigger(hours=1),
                id='health_check',
                name='System Health Check',
                replace_existing=True
            )
            
            self.scheduler.start()
            logger.info("üöÄ Automated task scheduler started successfully")
            logger.info("üìÖ Next sitemap generation: 6 hours")
            logger.info("üìÖ Next event refresh: 8 hours") 
            logger.info("üìÖ Next AI sync: 10 hours")
            logger.info("üìÖ Next event cleanup: 9 hours (32-day archive policy)")
            
        except Exception as e:
            logger.error(f"Failed to start scheduler: {str(e)}")
    
    def health_check(self):
        """Periodic health check for automated tasks"""
        current_time = datetime.utcnow()
        
        # Check if tasks are running on schedule
        for task_name, status in self.task_status.items():
            if status["last_run"]:
                last_run = datetime.fromisoformat(status["last_run"])
                hours_since_last_run = (current_time - last_run).total_seconds() / 3600
                
                if hours_since_last_run > 8:  # Alert if task hasn't run in 8+ hours
                    logger.warning(f"‚ö†Ô∏è Task '{task_name}' hasn't run in {hours_since_last_run:.1f} hours")
        
        logger.info("üíì Automated task health check completed")
    
    def stop_scheduler(self):
        """Stop the background scheduler"""
        if self.scheduler.running:
            self.scheduler.shutdown()
            logger.info("üõë Automated task scheduler stopped")

# Initialize the automated task manager
task_manager = AutomatedTaskManager()

# Start scheduler when in production
if IS_PRODUCTION:
    task_manager.start_scheduler()
    logger.info("ü§ñ AI sync automation enabled for production environment")
else:
    logger.info("üîß AI sync automation disabled in development mode")

# Create default admin user now that all functions are available
try:
    with get_db() as conn:
        create_default_admin_user(conn)
        logger.info("‚úÖ Default admin user initialization completed")
except Exception as e:
    logger.error(f"‚ùå Error creating default admin user during startup: {str(e)}")
class PasswordValidator:
    """
    Comprehensive password validation system with detailed feedback
    """
    @staticmethod
    def validate_password(password: str) -> dict:
        """
        Validate password and return detailed feedback
        
        Returns a dictionary with:
        - is_valid: Boolean indicating if password meets all criteria
        - feedback: List of specific validation messages
        - strength: Strength rating (weak/medium/strong)
        """
        # Initialize validation results
        validation_result = {
            "is_valid": True,
            "feedback": [],
            "strength": "weak"
        }
        
        # Check length - reduced requirement
        if len(password) < 6:
            validation_result["is_valid"] = False
            validation_result["feedback"].append(
                "Password must be at least 6 characters long"
            )
        
        # Count character types present
        character_types = 0
        character_type_messages = []
        
        # Check for uppercase letters
        if re.search(r'[A-Z]', password):
            character_types += 1
        else:
            character_type_messages.append("uppercase letter")
        
        # Check for lowercase letters
        if re.search(r'[a-z]', password):
            character_types += 1
        else:
            character_type_messages.append("lowercase letter")
        
        # Check for numbers
        if re.search(r'\d', password):
            character_types += 1
        else:
            character_type_messages.append("number")
        
        # Check for special characters
        if re.search(r'[!@#$%^&*(),.?":{}|<>]', password):
            character_types += 1
        else:
            character_type_messages.append("special character (!@#$%^&*(),.?\":{}|<>)")
        
        # Require at least 3 out of 4 character types (more flexible)
        if character_types < 3:
            validation_result["is_valid"] = False
            missing_types = 4 - character_types
            validation_result["feedback"].append(
                f"Password must contain at least 3 different character types. "
                f"Missing {missing_types} types from: {', '.join(character_type_messages)}"
            )
        
        # Check for common weak passwords
        common_weak_passwords = [
            "password", "123456", "qwerty", "admin", 
            "letmein", "welcome", "monkey", "abc123"
        ]
        if password.lower() in common_weak_passwords:
            validation_result["is_valid"] = False
            validation_result["feedback"].append(
                "Password is too common and easily guessable"
            )
        
        # Determine strength based on length and character variety
        strength_criteria = 0
        if len(password) >= 8:
            strength_criteria += 1
        if len(password) >= 12:
            strength_criteria += 1
        strength_criteria += character_types
        
        if strength_criteria <= 2:
            validation_result["strength"] = "weak"
        elif strength_criteria <= 4:
            validation_result["strength"] = "medium"
        else:
            validation_result["strength"] = "strong"
        
        return validation_result
# Pydantic Models
class EventBase(BaseModel):
    title: str
    description: str
    short_description: Optional[str] = None
    date: str
    start_time: str
    end_time: Optional[str] = None
    end_date: Optional[str] = None
    category: str
    secondary_category: Optional[str] = None
    address: str
    city: Optional[str] = None
    state: Optional[str] = None
    country: Optional[str] = "USA"
    lat: float
    lng: float
    recurring: bool = False
    frequency: Optional[str] = None
    # Pricing fields
    fee_required: Optional[str] = None  # Details about tickets/fees
    price: Optional[float] = 0.0        # Normalized price
    currency: Optional[str] = "USD"     # Currency code
    # Organization fields
    event_url: Optional[str] = None     # External event URL
    host_name: Optional[str] = None     # Organization/host name
    organizer_url: Optional[str] = None # Organizer website
    # Premium features
    verified: Optional[bool] = False    # Event verification status
    # SEO fields
    slug: Optional[str] = None          # URL-friendly slug
    is_published: Optional[bool] = True # Publication status

    @validator('date')
    def validate_date(cls, v):
        try:
            # Try parsing the date to ensure it's in a valid format
            datetime.strptime(v, '%Y-%m-%d')
            return v
        except ValueError:
            raise ValueError('Date must be in YYYY-MM-DD format')

    @validator('start_time')
    def validate_start_time(cls, v):
        try:
            # Try parsing the start_time to ensure it's in a valid format
            datetime.strptime(v, '%H:%M')
            return v
        except ValueError:
            raise ValueError('Start time must be in HH:MM 24-hour format')

    @validator('end_time')
    def validate_end_time(cls, v):
        if v is None or v == "":
            return v
        try:
            # Try parsing the end_time to ensure it's in a valid format
            datetime.strptime(v, '%H:%M')
            return v
        except ValueError:
            raise ValueError('End time must be in HH:MM 24-hour format')

    @validator('end_date')
    def validate_end_date(cls, v):
        if v is None or v == "":
            return v
        try:
            # Try parsing the date to ensure it's in a valid format
            datetime.strptime(v, '%Y-%m-%d')
            return v
        except ValueError:
            raise ValueError('End date must be in YYYY-MM-DD format')

    @validator('fee_required')
    def validate_fee_required(cls, v):
        if v is None:
            return ""  # Convert None to empty string
        if v == "":
            return v
        # Limit fee description to reasonable length
        if len(v) > 500:
            raise ValueError('Fee description must be 500 characters or less')
        return v.strip()

    @validator('event_url')
    def validate_event_url(cls, v):
        if v is None:
            return ""  # Convert None to empty string
        
        # Strip whitespace and check if empty
        v_stripped = v.strip() if isinstance(v, str) else ""
        if v_stripped == "":
            return ""  # Return empty string for None, empty, or whitespace-only values
            
        # Basic URL validation
        if not v_stripped.startswith(('http://', 'https://')):
            v_stripped = 'https://' + v_stripped  # Add https if missing
        
        # Simple URL pattern check
        import re
        url_pattern = re.compile(
            r'^https?://'  # http:// or https://
            r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\.)+[A-Z]{2,6}\.?|'  # domain...
            r'localhost|'  # localhost...
            r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})'  # ...or ip
            r'(?::\d+)?'  # optional port
            r'(?:/?|[/?]\S+)$', re.IGNORECASE)
        
        if not url_pattern.match(v_stripped):
            raise ValueError('Please enter a valid URL')
        
        # Limit URL length
        if len(v_stripped) > 2000:
            raise ValueError('URL must be 2000 characters or less')
        
        return v_stripped

    @validator('host_name')
    def validate_host_name(cls, v):
        if v is None:
            return ""  # Convert None to empty string
        if v == "":
            return v
        # Limit host name to reasonable length
        if len(v) > 200:
            raise ValueError('Host name must be 200 characters or less')
        return v.strip()

    @validator('address')
    def validate_address(cls, v):
        if not v or not isinstance(v, str):
            raise ValueError('Address is required')
        
        trimmed_address = v.strip()
        if not trimmed_address:
            raise ValueError('Address cannot be empty')
        
        return trimmed_address
    
    @validator('secondary_category')
    def validate_secondary_category(cls, v):
        if v is None or v == "":
            return None  # Allow empty/null secondary category
        
        # Use the same categories list as the main category
        categories = [
            'food-drink', 'music', 'arts', 'sports', 'automotive', 'airshows', 'vehicle-sports', 
            'community', 'religious', 'education', 'veteran', 'cookout', 'networking',
            'fair-festival', 'diving', 'shopping', 'health', 'outdoors', 'photography', 'family', 
            'gaming', 'real-estate', 'adventure', 'seasonal', 'agriculture', 'other'
        ]
        
        if v not in categories:
            raise ValueError(f'Secondary category must be one of: {", ".join(categories)}')
        
        return v

class EventCreate(EventBase):
    pass

class EventResponse(EventBase):
    id: int
    created_by: int
    created_at: str
    updated_at: Optional[str] = None
    start_datetime: Optional[str] = None
    end_datetime: Optional[str] = None
    interest_count: Optional[int] = 0
    view_count: Optional[int] = 0

    class Config:
        from_attributes = True

class UserBase(BaseModel):
    email: EmailStr

class UserCreate(UserBase):
    password: str
    role: UserRole = UserRole.USER

    @validator('password')
    def validate_password(cls, v):
        # Use the PasswordValidator to get validation results
        validation = PasswordValidator.validate_password(v)
        
        # If not valid, raise a validation error with all feedback
        if not validation['is_valid']:
            raise ValueError('\n'.join(validation['feedback']))
        
        return v

class UserResponse(UserBase):
    id: int
    role: UserRole

    class Config:
        from_attributes = True

class Token(BaseModel):
    access_token: str
    token_type: str

# Security Functions
def verify_password(plain_password, hashed_password):
    return pwd_context.verify(plain_password, hashed_password)

def get_password_hash(password):
    return pwd_context.hash(password)

def create_access_token(data: dict):
    to_encode = data.copy()
    expire = datetime.utcnow() + timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    to_encode.update({"exp": expire})
    return jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)

async def get_current_user(token: str = Depends(oauth2_scheme)):
    credentials_exception = HTTPException(
        status_code=status.HTTP_401_UNAUTHORIZED,
        detail="Could not validate credentials",
        headers={"WWW-Authenticate": "Bearer"},
    )
    
    if not token:
        raise credentials_exception
        
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        email: str = payload.get("sub")
        if email is None:
            raise credentials_exception
    except jwt.PyJWTError:
        raise credentials_exception
    
    try:
        with get_db() as conn:
            c = conn.cursor()
            placeholder = get_placeholder()
            c.execute(f"SELECT id, email, role FROM users WHERE email = {placeholder}", (email,))
            user = c.fetchone()
            
            if not user:
                raise credentials_exception
                
            return {
                "id": user[0] if isinstance(user, (tuple, list)) else user["id"],
                "email": user[1] if isinstance(user, (tuple, list)) else user["email"],
                "role": user[2] if isinstance(user, (tuple, list)) else user["role"]
            }
            
    except Exception as e:
        error_msg = str(e)
        logger.error(f"Database error during login user lookup: {error_msg}")
        
        if "timeout" in error_msg.lower() or "deadlock" in error_msg.lower():
            raise HTTPException(
                status_code=408, 
                detail="Login request timed out. Please try again."
            )
        else:
            raise HTTPException(
                status_code=500,
                detail="Internal server error during login",
                headers={"WWW-Authenticate": "Bearer"},
            )

async def get_current_user_optional(token: str = Depends(oauth2_scheme)):
    """Optional user dependency - returns None if not authenticated"""
    try:
        return await get_current_user(token)
    except HTTPException:
        return None

async def get_current_user_optional_no_exception(authorization: str = Header(None)):
    """Optional user dependency that doesn't raise exceptions"""
    if not authorization or not authorization.startswith("Bearer "):
        return None
    
    token = authorization.split(" ")[1]
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        email: str = payload.get("sub")
        if email is None:
            return None
    except jwt.PyJWTError:
        return None
    
    try:
        with get_db() as conn:
            c = conn.cursor()
            placeholder = get_placeholder()
            c.execute(f"SELECT id, email, role FROM users WHERE email = {placeholder}", (email,))
            user = c.fetchone()
            
            if not user:
                return None
                
            return {
                "id": user[0] if isinstance(user, (tuple, list)) else user["id"],
                "email": user[1] if isinstance(user, (tuple, list)) else user["email"],
                "role": user[2] if isinstance(user, (tuple, list)) else user["role"]
            }
            
    except Exception as e:
        logger.error(f"Error in optional user auth: {str(e)}")
        return None
# Authentication Endpoints
@app.post("/token")
async def login(form_data: OAuth2PasswordRequestForm = Depends()):
    placeholder = get_placeholder()
    logger.info(f"Login attempt for user: {form_data.username}")
    
    try:
        with get_db() as conn:
            c = conn.cursor()
            
            # First check connection health with a simple query
            try:
                c.execute("SELECT 1")
            except Exception as e:
                logger.error(f"Database health check failed during login: {str(e)}")
                raise HTTPException(
                    status_code=503, 
                    detail="Database connection issues. Please try again later."
                )
            
            # Find user by email - with timeout handling
            user = None
            try:
                c.execute(f"SELECT * FROM users WHERE email = {placeholder}", (form_data.username,))
                user = c.fetchone()
                
                if not user:
                    logger.warning(f"Login failed for user: {form_data.username} - User not found")
                    raise HTTPException(
                        status_code=status.HTTP_401_UNAUTHORIZED,
                        detail="Incorrect email or password",
                        headers={"WWW-Authenticate": "Bearer"},
                    )
                
                if not verify_password(form_data.password, user["hashed_password"]):
                    logger.warning(f"Login failed for user: {form_data.username} - Invalid credentials")
                    raise HTTPException(
                        status_code=status.HTTP_401_UNAUTHORIZED,
                        detail="Incorrect email or password",
                        headers={"WWW-Authenticate": "Bearer"},
                    )
            except HTTPException:
                raise
            except Exception as e:
                error_msg = str(e)
                logger.error(f"Database error during login user lookup: {error_msg}")
                
                if "timeout" in error_msg.lower() or "deadlock" in error_msg.lower():
                    raise HTTPException(
                        status_code=408, 
                        detail="Login request timed out. Please try again."
                    )
                else:
                    raise HTTPException(
                        status_code=500,
                        detail="Internal server error during login",
                        headers={"WWW-Authenticate": "Bearer"},
                    )
            
            # Generate access token
            if user:  # Extra safety check
                try:
                    logger.info(f"Generating access token for user: {form_data.username}")
                    access_token = create_access_token(data={"sub": user["email"]})
                    logger.info(f"Login successful for user: {form_data.username}")
                    return {"access_token": access_token, "token_type": "bearer"}
                except Exception as e:
                    logger.error(f"Error generating access token: {str(e)}")
                    raise HTTPException(
                        status_code=500,
                        detail="Error generating authentication token",
                        headers={"WWW-Authenticate": "Bearer"},
                    )
            else:
                # This should not happen due to the checks above, but as a fallback
                raise HTTPException(
                    status_code=status.HTTP_401_UNAUTHORIZED,
                    detail="Authentication failed",
                    headers={"WWW-Authenticate": "Bearer"},
                )
                
    except HTTPException as http_ex:
        # Pass through HTTP exceptions
        raise http_ex
    except Exception as e:
        error_msg = str(e)
        logger.error(f"Unexpected error during login: {error_msg}")
        
        # Return more specific error messages
        if "timeout" in error_msg.lower():
            raise HTTPException(
                status_code=408, 
                detail="Login request timed out. Please try again later."
            )
        elif "connection" in error_msg.lower():
            raise HTTPException(
                status_code=503, 
                detail="Database connection issues. Please try again later."
            )
        else:
            # Catch all for any other unexpected errors
            raise HTTPException(
                status_code=500,
                detail="Internal server error during login",
                headers={"WWW-Authenticate": "Bearer"},
            )

@app.post("/users", response_model=UserResponse)
async def create_user(user: UserCreate):
    # Validate password (this will raise an error if invalid)
    password_validation = PasswordValidator.validate_password(user.password)
    
    placeholder = get_placeholder()
    
    # Pre-hash password outside the database transaction to reduce transaction time
    hashed_password = get_password_hash(user.password)
    
    # Log the registration attempt
    logger.info(f"Registration attempt for email: {user.email}")
    
    try:
        with get_db() as conn:
            c = conn.cursor()
            
            # First check connection health with a simple query
            try:
                c.execute("SELECT 1")
            except Exception as e:
                logger.error(f"Database health check failed during registration: {str(e)}")
                raise HTTPException(
                    status_code=503, 
                    detail="Database connection issues. Please try again later."
                )
            
            # Check if email exists - with timeout handling
            try:
                c.execute(f"SELECT id FROM users WHERE email = {placeholder}", (user.email,))
                existing_user = c.fetchone()
                if existing_user:
                    logger.info(f"Registration failed: Email already exists - {user.email}")
                    raise HTTPException(status_code=400, detail="Email already registered")
            except Exception as e:
                if "timeout" in str(e).lower() or "deadlock" in str(e).lower():
                    logger.error(f"Timeout checking for existing email: {str(e)}")
                    raise HTTPException(
                        status_code=408, 
                        detail="Registration request timed out. Please try again."
                    )
                raise
            
            # Insert user with optimized query - with timeout handling
            try:
                logger.info(f"Inserting new user into database: {user.email}")
                
                # Use RETURNING id for PostgreSQL, different approach for SQLite
                if IS_PRODUCTION and DB_URL:
                    # PostgreSQL with RETURNING
                    c.execute(
                        f"INSERT INTO users (email, hashed_password, role) VALUES ({placeholder}, {placeholder}, {placeholder}) RETURNING id",
                        (user.email, hashed_password, user.role)
                    )
                    result = c.fetchone()
                    last_id = result['id'] if result else None
                else:
                    # SQLite without RETURNING
                    c.execute(
                        f"INSERT INTO users (email, hashed_password, role) VALUES ({placeholder}, {placeholder}, {placeholder})",
                        (user.email, hashed_password, user.role)
                    )
                    last_id = c.lastrowid
                    
                if not last_id:
                    raise ValueError("Failed to get ID of created user")
                    
                conn.commit()
                logger.info(f"User inserted successfully: {user.email}")
            except Exception as e:
                conn.rollback()
                error_msg = str(e)
                logger.error(f"Error inserting user: {error_msg}")
                
                if "timeout" in error_msg.lower() or "deadlock" in error_msg.lower():
                    raise HTTPException(
                        status_code=408, 
                        detail="Registration request timed out. The server might be busy, please try again."
                    )
                elif "duplicate" in error_msg.lower() or "unique" in error_msg.lower():
                    raise HTTPException(status_code=400, detail="Email already registered")
                else:
                    raise HTTPException(status_code=500, detail="Error creating user")
            
            # Get the created user data
            try:
                    
                logger.info(f"Fetching created user with ID {last_id}")
                c.execute(f"SELECT * FROM users WHERE id = {placeholder}", (last_id,))
                user_data = dict(c.fetchone())
                
                # Return user data along with password strength
                logger.info(f"User registration successful: {user.email}")
                return {
                    **user_data,
                    "password_strength": password_validation['strength']
                }
            except Exception as e:
                logger.error(f"Error retrieving created user: {str(e)}")
                # Even if we can't fetch the user, registration was successful
                return {
                    "id": last_id,
                    "email": user.email,
                    "role": user.role,
                    "password_strength": password_validation['strength']
                }
                
    except HTTPException as http_ex:
        # Pass through HTTP exceptions
        raise http_ex
    except Exception as e:
        error_msg = str(e)
        logger.error(f"Unexpected error creating user: {error_msg}")
        
        # Return more specific error messages
        if "timeout" in error_msg.lower():
            raise HTTPException(
                status_code=408, 
                detail="Registration request timed out. Please try again later."
            )
        elif "connection" in error_msg.lower():
            raise HTTPException(
                status_code=503, 
                detail="Database connection issues. Please try again later."
            )
        else:
            raise HTTPException(status_code=500, detail="Error creating user")


# Password Strength Check Endpoint
@app.post("/password-strength")
async def check_password_strength(password: str):
    """
    Endpoint to check password strength without creating a user
    Useful for real-time password feedback
    """
    return PasswordValidator.validate_password(password)

# Password Reset Models
class PasswordResetRequest(BaseModel):
    email: EmailStr

class PasswordResetVerify(BaseModel):
    email: EmailStr
    reset_code: str

class PasswordReset(BaseModel):
    email: EmailStr
    reset_code: str
    new_password: str
    
    @validator('new_password')
    def validate_password(cls, v):
        # Use the PasswordValidator to get validation results
        validation = PasswordValidator.validate_password(v)
        
        # If not valid, raise a validation error with all feedback
        if not validation['is_valid']:
            raise ValueError('\n'.join(validation['feedback']))
        
        return v

# Password Reset Endpoints
@app.post("/request-password-reset")
async def request_password_reset(request: PasswordResetRequest):
    """Request a password reset code"""
    placeholder = get_placeholder()
    
    try:
        with get_db() as conn:
            c = conn.cursor()
            
            # Check if user exists
            c.execute(f"SELECT * FROM users WHERE email = {placeholder}", (request.email,))
            user = c.fetchone()
            
            if not user:
                # Always return success for security reasons
                # This prevents email enumeration attacks
                return {"detail": "If your email is registered, you will receive a reset code"}
            
            # Generate a 6-digit reset code
            import random
            reset_code = ''.join(random.choices('0123456789', k=6))
            reset_expiry = datetime.utcnow() + timedelta(minutes=30)
            
            # Store the reset code in the database
            # First, create a password_resets table if it doesn't exist
            if IS_PRODUCTION and DB_URL:
                c.execute('''
                    CREATE TABLE IF NOT EXISTS password_resets (
                        id SERIAL PRIMARY KEY,
                        email TEXT NOT NULL,
                        reset_code TEXT NOT NULL,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        expires_at TIMESTAMP NOT NULL
                    )
                ''')
            else:
                c.execute('''
                    CREATE TABLE IF NOT EXISTS password_resets (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        email TEXT NOT NULL,
                        reset_code TEXT NOT NULL,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        expires_at TIMESTAMP NOT NULL
                    )
                ''')
            
            # Delete any existing reset codes for this user
            c.execute(f"DELETE FROM password_resets WHERE email = {placeholder}", (request.email,))
            
            # Insert the new reset code
            c.execute(
                f"INSERT INTO password_resets (email, reset_code, expires_at) VALUES ({placeholder}, {placeholder}, {placeholder})",
                (request.email, reset_code, reset_expiry)
            )
            
            # Send password reset email
            try:
                from email_config import email_service
                
                # Extract user name from email for personalization
                user_name = request.email.split('@')[0].title()
                
                email_sent = email_service.send_password_reset_email(
                    request.email, 
                    reset_code, 
                    user_name
                )
                
                if email_sent:
                    logger.info(f"‚úÖ Password reset email sent to {request.email}")
                else:
                    logger.error(f"‚ùå Failed to send password reset email to {request.email}")
                    
            except Exception as e:
                logger.error(f"‚ùå Email service error: {str(e)}")
            
            # For development, also log the reset code
            if not IS_PRODUCTION:
                logger.info(f"Password reset code for {request.email}: {reset_code}")
                return {
                    "detail": "Reset code generated and email sent",
                    "reset_code": reset_code
                }
            
            return {"detail": "If your email is registered, you will receive a reset code"}
            
    except Exception as e:
        logger.error(f"Error requesting password reset: {str(e)}")
        raise HTTPException(status_code=500, detail="Error requesting password reset")

@app.post("/verify-reset-code")
async def verify_reset_code(request: PasswordResetVerify):
    """Verify a password reset code"""
    placeholder = get_placeholder()
    
    try:
        with get_db() as conn:
            c = conn.cursor()
            
            # Check if reset code exists and is valid
            c.execute(
                f"SELECT * FROM password_resets WHERE email = {placeholder} AND reset_code = {placeholder} AND expires_at > {placeholder}",
                (request.email, request.reset_code, datetime.utcnow())
            )
            reset = c.fetchone()
            
            if not reset:
                raise HTTPException(status_code=400, detail="Invalid or expired reset code")
            
            return {"detail": "Reset code verified"}
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error verifying reset code: {str(e)}")
        raise HTTPException(status_code=500, detail="Error verifying reset code")
@app.post("/reset-password")
async def reset_password(request: PasswordReset):
    """Reset password with a valid reset code"""
    placeholder = get_placeholder()
    
    try:
        with get_db() as conn:
            c = conn.cursor()
            
            # Check if reset code exists and is valid
            c.execute(
                f"SELECT * FROM password_resets WHERE email = {placeholder} AND reset_code = {placeholder} AND expires_at > {placeholder}",
                (request.email, request.reset_code, datetime.utcnow())
            )
            reset = c.fetchone()
            
            if not reset:
                raise HTTPException(status_code=400, detail="Invalid or expired reset code")
            
            # Update the user's password
            hashed_password = get_password_hash(request.new_password)
            c.execute(
                f"UPDATE users SET hashed_password = {placeholder} WHERE email = {placeholder}",
                (hashed_password, request.email)
            )
            
            # Delete all reset codes for this user
            c.execute(f"DELETE FROM password_resets WHERE email = {placeholder}", (request.email,))
            
            return {"detail": "Password has been reset successfully"}
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error resetting password: {str(e)}")
        raise HTTPException(status_code=500, detail="Error resetting password")

# Event Endpoints
@app.get("/events", response_model=List[EventResponse])
async def list_events(
    category: Optional[str] = None,
    date: Optional[str] = None,
    limit: Optional[int] = 500,  # Increased default limit to 500
    offset: Optional[int] = 0,  # Add pagination
    lat: Optional[float] = None,  # Add location filtering
    lng: Optional[float] = None,  # Add location filtering
    radius: Optional[float] = 25.0  # Add radius filtering (miles)
):
    """
    Retrieve events with optional filtering by category and date.
    Open to all users, no authentication required.
    Optimized for performance with pagination, location filtering, and caching.
    """
    placeholder = get_placeholder()
    
    # Validate and limit pagination parameters  
    limit = min(max(limit or 500, 1), 1000)  # Between 1 and 1000
    offset = max(offset or 0, 0)
    
    # Create cache key for this request
    cache_key = f"events:{category or 'all'}:{date or 'all'}:{limit}:{offset}:{lat}:{lng}:{radius}"
    
    # Try to get from cache first (for mobile performance)
    cached_result = event_cache.get(cache_key)
    if cached_result is not None:
        logger.info(f"Returning cached events for key: {cache_key} - {len(cached_result)} events")
        logger.info(f"Cache stats: {event_cache.stats()}")
        return cached_result
    
    try:
        # Use connection manager with optimized query
        with get_db() as conn:
            cursor = conn.cursor()
            
            # Build optimized query with proper indexing hints
            where_conditions = []
            params = []
            
            # Category filter
            if category and category != 'all':
                where_conditions.append(f"category = {placeholder}")
                params.append(category)
            
            # Date filter
            if date:
                where_conditions.append(f"date = {placeholder}")
                params.append(date)
            
            # Location filter (if coordinates provided)
            location_select = ""
            if lat is not None and lng is not None:
                # Add distance calculation for location-based filtering
                location_select = f"""
                    , (6371 * acos(cos(radians({lat})) * cos(radians(lat)) * 
                      cos(radians(lng) - radians({lng})) + sin(radians({lat})) * 
                      sin(radians(lat)))) * 0.621371 as distance_miles
                """
            
            # Construct WHERE clause
            where_clause = ""
            if where_conditions:
                where_clause = "WHERE " + " AND ".join(where_conditions)
            
            # Database-specific date comparison logic
            if IS_PRODUCTION and DB_URL:
                # PostgreSQL syntax
                date_comparison = "date::date >= CURRENT_DATE"
            else:
                # SQLite syntax
                date_comparison = "date >= date('now')"
            
            # Optimized query with specific columns and LIMIT, including ALL fields
            base_query = f"""
                SELECT id, title, description, short_description, date, start_time, end_time, end_date, 
                       category, address, city, state, country, lat, lng, recurring, frequency, created_by, created_at,
                       COALESCE(interest_count, 0) as interest_count,
                       COALESCE(view_count, 0) as view_count,
                       fee_required, price, currency, event_url, host_name, organizer_url, slug, is_published,
                       start_datetime, end_datetime, updated_at, verified
                       {location_select}
                FROM events 
                {where_clause}
                ORDER BY 
                    CASE 
                        WHEN {date_comparison} THEN 0  -- Future/today events first
                        ELSE 1                           -- Past events later  
                    END,
                    date ASC, start_time ASC
                LIMIT {placeholder} OFFSET {placeholder}
            """
            
            params.extend([limit, offset])
            
            # Execute optimized query
            cursor.execute(base_query, params)
            events = cursor.fetchall()
            
            # Process results efficiently
            result = []
            for event in events:
                try:
                    # Convert to dict efficiently
                    if hasattr(event, '_asdict'):
                        event_dict = event._asdict()
                    elif isinstance(event, dict):
                        event_dict = dict(event)
                    else:
                        # Handle tuple/list results with known schema including ALL fields
                        column_names = [
                            'id', 'title', 'description', 'short_description', 'date', 'start_time', 'end_time',
                            'end_date', 'category', 'address', 'city', 'state', 'country', 'lat', 'lng', 'recurring',
                            'frequency', 'created_by', 'created_at', 'interest_count', 'view_count',
                            'fee_required', 'price', 'currency', 'event_url', 'host_name', 'organizer_url', 'slug', 'is_published',
                            'start_datetime', 'end_datetime', 'updated_at', 'verified'
                        ]
                        if location_select:
                            column_names.append('distance_miles')
                        
                        event_dict = dict(zip(column_names, event))
                    
                    # Convert datetime objects and ensure proper field types
                    event_dict = convert_event_datetime_fields(event_dict)
                    
                    # Ensure counters are integers
                    event_dict['interest_count'] = int(event_dict.get('interest_count', 0) or 0)
                    event_dict['view_count'] = int(event_dict.get('view_count', 0) or 0)
                    
                    # Filter by distance if location provided
                    if lat is not None and lng is not None and 'distance_miles' in event_dict:
                        if event_dict['distance_miles'] <= radius:
                            result.append(event_dict)
                    else:
                        result.append(event_dict)
                    
                except Exception as event_error:
                    logger.warning(f"Error processing event {event}: {event_error}")
                    continue
            
            # Cache the result for mobile performance (shorter TTL for real-time updates)
            event_cache.set(cache_key, result)
            logger.info(f"Cached {len(result)} events for key: {cache_key}")
            logger.info(f"Database query returned {len(events)} raw events, processed {len(result)} events")
            logger.info(f"Cache stats: {event_cache.stats()}")
            
            return result
            
    except Exception as e:
        # Handle any other exceptions
        error_msg = str(e)
        logger.error(f"Error retrieving events: {error_msg}")
        
        if "timeout" in error_msg.lower():
            raise HTTPException(status_code=504, detail="Database query timed out")
        elif "connection" in error_msg.lower():
            raise HTTPException(status_code=503, detail="Database connection issues")
        else:
            raise HTTPException(status_code=500, detail="Error retrieving events")

@app.get("/events/{event_id}", response_model=EventResponse)
async def read_event(event_id: int):
    """
    Retrieve a specific event by its ID.
    Open to all users, no authentication required.
    """
    placeholder = get_placeholder()
    try:
        with get_db() as conn:
            c = conn.cursor()
            # Use COALESCE to handle NULL values and include all fields
            c.execute(f"""SELECT id, title, description, short_description, date, start_time, end_time, end_date, 
                         category, address, city, state, country, lat, lng, recurring, frequency, created_by, created_at,
                         COALESCE(interest_count, 0) as interest_count,
                         COALESCE(view_count, 0) as view_count,
                         fee_required, price, currency, event_url, host_name, organizer_url, slug, is_published,
                         start_datetime, end_datetime, updated_at, verified
                         FROM events WHERE id = {placeholder}""", (event_id,))
            event = c.fetchone()
            
            if not event:
                raise HTTPException(status_code=404, detail="Event not found")
            
            # Convert to dict and handle datetime fields
            event_dict = dict(event)
            
            # Convert datetime objects and ensure proper field types
            event_dict = convert_event_datetime_fields(event_dict)
            
            return event_dict
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error retrieving event {event_id}: {str(e)}")
        raise HTTPException(status_code=500, detail="Error retrieving event")

def ensure_unique_slug(cursor, base_slug: str, event_id: int = None) -> str:
    """Ensure slug uniqueness by appending event ID if needed"""
    if not base_slug:
        # Generate a fallback slug if none provided
        import time
        return f"event-{int(time.time())}"
    
    try:
        # Use database-agnostic placeholder
        placeholder = get_placeholder()
        
        # Check if slug already exists
        if event_id:
            # For updates, exclude current event from check
            if placeholder == "?":
                # SQLite style
                cursor.execute("SELECT COUNT(*) FROM events WHERE slug = ? AND id != ?", (base_slug, event_id))
            else:
                # PostgreSQL style (use %s with psycopg2)
                cursor.execute("SELECT COUNT(*) FROM events WHERE slug = %s AND id != %s", (base_slug, event_id))
        else:
            if placeholder == "?":
                # SQLite style
                cursor.execute("SELECT COUNT(*) FROM events WHERE slug = ?", (base_slug,))
            else:
                # PostgreSQL style (use %s with psycopg2)
                cursor.execute("SELECT COUNT(*) FROM events WHERE slug = %s", (base_slug,))
        
        result = cursor.fetchone()
        count = result[0] if result else 0
        
        if count > 0:
            # Slug exists, append a number or event ID
            if event_id:
                return f"{base_slug}-{event_id}"
            else:
                # For new events, append a timestamp-based suffix
                import time
                suffix = str(int(time.time()))[-6:]  # Last 6 digits of timestamp
                return f"{base_slug}-{suffix}"
        else:
            return base_slug
            
    except Exception as e:
        logger.error(f"Error ensuring unique slug for '{base_slug}': {type(e).__name__}: {str(e)}")
        # Fallback: append current timestamp
        import time
        timestamp_suffix = str(int(time.time()))[-6:]
        fallback_slug = f"{base_slug}-{timestamp_suffix}" if base_slug else f"event-{timestamp_suffix}"
        logger.info(f"Using fallback slug: {fallback_slug}")
        return fallback_slug

def get_actual_table_columns(cursor, table_name: str = 'events') -> List[str]:
    """Get actual columns that exist in the database table with enhanced error handling for production"""
    try:
        if IS_PRODUCTION and DB_URL:
            # PostgreSQL: Enhanced approach with better error handling
            try:
                # Method 1: Direct column query with table existence check
                cursor.execute("""
                    SELECT column_name, data_type, is_nullable 
                    FROM information_schema.columns 
                    WHERE table_name = %s 
                    AND table_schema = 'public'
                    ORDER BY ordinal_position
                """, (table_name,))
                
                columns_info = cursor.fetchall()
                if columns_info:
                    columns = [row[0] for row in columns_info]
                    logger.info(f"‚úÖ PostgreSQL schema detection: Found {len(columns)} columns via information_schema")
                    logger.debug(f"Column details: {[(row[0], row[1], row[2]) for row in columns_info]}")
                    return columns
                else:
                    logger.warning("‚ö†Ô∏è information_schema query returned no results")
            except Exception as e:
                logger.debug(f"‚ùå information_schema query failed (fallback available): {type(e).__name__}")
            
            try:
                # Method 2: Use PostgreSQL system catalogs with error handling
                cursor.execute("""
                    SELECT a.attname, t.typname, NOT a.attnotnull as is_nullable
                    FROM pg_class c
                    JOIN pg_attribute a ON a.attrelid = c.oid
                    JOIN pg_type t ON t.oid = a.atttypid
                    JOIN pg_namespace n ON n.oid = c.relnamespace
                    WHERE c.relname = %s 
                    AND n.nspname = 'public'
                    AND a.attnum > 0 
                    AND NOT a.attisdropped
                    ORDER BY a.attnum
                """, (table_name,))
                
                columns_info = cursor.fetchall()
                if columns_info:
                    columns = [row[0] for row in columns_info]
                    logger.info(f"‚úÖ PostgreSQL schema detection: Found {len(columns)} columns via pg_class")
                    logger.debug(f"System catalog columns: {[(row[0], row[1], row[2]) for row in columns_info]}")
                    return columns
                else:
                    logger.warning("‚ö†Ô∏è pg_class query returned no results")
            except Exception as e:
                logger.debug(f"‚ùå pg_class query failed (fallback available): {type(e).__name__}")
                
            try:
                # Method 3: Simple table existence and structure check
                cursor.execute(f"SELECT * FROM {table_name} LIMIT 0")
                if cursor.description:
                    columns = [desc[0] for desc in cursor.description]
                    logger.info(f"‚úÖ PostgreSQL schema detection: Found {len(columns)} columns via SELECT LIMIT 0")
                    return columns
                else:
                    logger.error("‚ùå No column description available from SELECT query")
            except Exception as e:
                logger.error(f"‚ùå SELECT LIMIT 0 query failed: {e}")
        else:
            # SQLite: Use PRAGMA table_info with enhanced error handling
            try:
                cursor.execute(f"PRAGMA table_info({table_name})")
                columns_info = cursor.fetchall()
                if columns_info:
                    columns = [row[1] for row in columns_info]  # row[1] is column name
                    logger.info(f"‚úÖ SQLite schema detection: Found {len(columns)} columns")
                    return columns
                else:
                    logger.warning(f"‚ö†Ô∏è PRAGMA table_info returned no results for {table_name}")
            except Exception as e:
                logger.error(f"‚ùå SQLite PRAGMA query failed: {e}")
                
    except Exception as e:
        logger.error(f"‚ùå Critical error in get_actual_table_columns for {table_name}: {e}")
    
    # Enhanced fallback with all known columns
    fallback_columns = [
        'id', 'title', 'description', 'date', 'start_time', 'end_time', 
        'category', 'address', 'lat', 'lng', 'recurring', 'frequency',
        'created_by', 'created_at', 'updated_at', 'end_date',
        'short_description', 'city', 'state', 'country', 
        'interest_count', 'view_count', 'fee_required', 'event_url', 
        'host_name', 'organizer_url', 'slug', 'price', 'currency',
        'start_datetime', 'end_datetime', 'geo_hash', 'is_published',
        'verified'
    ]
    logger.warning(f"üîÑ Using enhanced fallback columns ({len(fallback_columns)}) for {table_name}")
    return fallback_columns
def auto_populate_seo_fields(event_data: dict) -> dict:
    """Auto-populate SEO fields for new events using enhanced logic"""
    import re
    import unicodedata
    
    def slugify_local(title, city=""):
        """Enhanced slugify function using title and city"""
        if not title:
            return ""
        
        # Combine title and city for better uniqueness
        base = f"{title} {city}".lower() if city else title.lower()
        
        # Normalize Unicode characters
        base = unicodedata.normalize('NFKD', base)
        
        # Remove non-word characters (keep letters, numbers, spaces, hyphens)
        base = re.sub(r'[^\w\s-]', '', base)
        
        # Replace multiple spaces/underscores/hyphens with single hyphen
        base = re.sub(r'[\s_-]+', '-', base)
        
        # Remove leading/trailing hyphens
        return base.strip('-')
    
    def extract_city_state_local(address):
        """Enhanced city/state extraction from address"""
        if not address:
            return None, None
        
        # Strategy A: [City], [STATE], USA format
        match = re.search(r'([^,]+),\s*([A-Z]{2}),?\s*USA?$', address, re.IGNORECASE)
        if match:
            return match.group(1).strip(), match.group(2).upper()
        
        # Strategy B: [City], [STATE] format (without USA)
        match = re.search(r'([^,]+),\s*([A-Z]{2})$', address, re.IGNORECASE)
        if match:
            return match.group(1).strip(), match.group(2).upper()
        
        # Strategy C: Find any 2-letter state code
        match = re.search(r'\b([A-Z]{2})\b', address)
        if match:
            state = match.group(1)
            # Try to find city before state
            city_pattern = r'([^,\d]+)(?:,\s*)?' + re.escape(state)
            city_match = re.search(city_pattern, address, re.IGNORECASE)
            if city_match:
                city = city_match.group(1).strip()
                # Clean up city name
                city = re.sub(r'^\d+\s+', '', city)  # Remove leading numbers
                return city, state
        
        return None, None
    
    def normalize_price_local(fee_required):
        """Enhanced price normalization"""
        if not fee_required or not isinstance(fee_required, str):
            return 0.0
        
        fee_lower = fee_required.lower().strip()
        
        # Check for free indicators
        free_indicators = ['free', 'no charge', 'no cost', 'gratis', 'complimentary', 'n/a', 'none']
        if any(indicator in fee_lower for indicator in free_indicators):
            return 0.0
        
        # Extract numeric values with various currency symbols
        currency_pattern = r'[\$‚Ç¨¬£¬•‚Çπ]?[\s]*([0-9]+(?:[.,][0-9]{1,2})?)'
        match = re.search(currency_pattern, fee_required)
        if match:
            price_str = match.group(1).replace(',', '')
            try:
                return float(price_str)
            except ValueError:
                pass
        
        # Try to find "number dollars/euros/etc" pattern
        word_pattern = r'([0-9]+(?:\.[0-9]{1,2})?)\s*(?:dollars?|euros?|pounds?|bucks?)'
        match = re.search(word_pattern, fee_lower)
        if match:
            try:
                return float(match.group(1))
            except ValueError:
                pass
        
        return 0.0
    
    def build_datetimes_local(date_str, start_time_str, end_time_str, end_date_str=None):
        """Enhanced datetime building with end_time inference"""
        if not date_str or not start_time_str:
            return None, None
        
        try:
            # Build start datetime
            start_dt_str = f"{date_str}T{start_time_str}:00"
            
            # Build end datetime - ensure we always have an end_time
            if not end_time_str:
                # Infer end_time as 2 hours after start_time
                from datetime import datetime, timedelta
                try:
                    start_time_obj = datetime.strptime(start_time_str, "%H:%M")
                    end_time_obj = start_time_obj + timedelta(hours=2)
                    end_time_str = end_time_obj.strftime("%H:%M")
                except:
                    end_time_str = "18:00"  # fallback
            
            # Now build the end datetime string
            if end_date_str:
                end_dt_str = f"{end_date_str}T{end_time_str}:00"
            else:
                end_dt_str = f"{date_str}T{end_time_str}:00"
            
            return start_dt_str, end_dt_str
        except Exception:
            return None, None
    
    def make_short_description_local(description):
        """Enhanced short description generation"""
        if not description:
            return ""
        
        # Remove extra whitespace and normalize
        cleaned = ' '.join(description.split())
        
        # If short enough, return as-is
        if len(cleaned) <= 160:
            return cleaned
        
        # Truncate at word boundary near 157 characters
        truncated = cleaned[:157]
        last_space = truncated.rfind(' ')
        if last_space > 120:  # Only truncate at word if reasonable
            return cleaned[:last_space] + "..."
        else:
            return cleaned[:157] + "..."
    
    # Generate slug
    city = event_data.get('city', '') or ''
    base_slug = slugify_local(event_data.get('title', ''), city)
    event_data['slug'] = base_slug  # Will be made unique later if needed
    
    # Extract city/state from address
    if not event_data.get('city') or not event_data.get('state'):
        city, state = extract_city_state_local(event_data.get('address', ''))
        if city:
            event_data['city'] = city
        if state:
            event_data['state'] = state
    
    # Normalize price
    event_data['price'] = normalize_price_local(event_data.get('fee_required', ''))
    
    # Generate datetime fields
    start_dt, end_dt = build_datetimes_local(
        event_data.get('date', ''),
        event_data.get('start_time', ''),
        event_data.get('end_time', ''),
        event_data.get('end_date')
    )
    event_data['start_datetime'] = start_dt
    event_data['end_datetime'] = end_dt
    
    # Generate short description
    if not event_data.get('short_description'):
        event_data['short_description'] = make_short_description_local(
            event_data.get('description', '')
        )
    
    # Set defaults
    event_data['currency'] = event_data.get('currency', 'USD')
    event_data['country'] = event_data.get('country', 'USA')
    event_data['is_published'] = event_data.get('is_published', True)
    event_data['updated_at'] = datetime.utcnow().isoformat()
    
    return event_data

@app.post("/events", response_model=EventResponse)
async def create_event(event: EventCreate, current_user: dict = Depends(get_current_user)):
    """
    Create a new event. Requires user authentication.
    Auto-populates SEO fields using enhanced migration logic.
    """
    placeholder = get_placeholder()
    
    # Convert Pydantic model to dict for processing
    event_data = event.dict()
    
    # Validate recurring events for premium users only
    event_data = validate_recurring_event(event_data, current_user['role'])
    
    # Check premium event limits if this is a premium event
    if event_data.get('is_premium_event', False):
        is_premium = current_user['role'] in ['premium', 'admin']
        is_enterprise = current_user['role'] == 'enterprise'
        
        if not (is_premium or is_enterprise):
            raise HTTPException(
                status_code=403, 
                detail="Premium subscription required to create premium events"
            )
        
        # Check monthly premium event limits
        try:
            with get_db() as conn:
                cursor = conn.cursor()
                
                # Count premium events created this month
                from datetime import datetime
                start_of_month = datetime.now().replace(day=1, hour=0, minute=0, second=0, microsecond=0)
                
                if IS_PRODUCTION and DB_URL:
                    cursor.execute("""
                        SELECT COUNT(*) as count
                        FROM events 
                        WHERE created_by = %s 
                        AND created_at >= %s
                        AND verified = TRUE
                    """, (current_user['id'], start_of_month))
                else:
                    cursor.execute("""
                        SELECT COUNT(*) as count
                        FROM events 
                        WHERE created_by = ? 
                        AND created_at >= ?
                        AND verified = TRUE
                    """, (current_user['id'], start_of_month))
                
                result = cursor.fetchone()
                current_premium_events = result['count'] if result else 0
                
                # Determine event limits based on role
                if is_enterprise:
                    event_limit = 250
                else:  # premium
                    event_limit = 10
                
                if current_premium_events >= event_limit:
                    raise HTTPException(
                        status_code=403, 
                        detail=f"Premium event limit reached ({current_premium_events}/{event_limit}). Upgrade to Enterprise for higher limits."
                    )
                    
        except HTTPException:
            raise
        except Exception as e:
            logger.error(f"Error checking premium event limits: {str(e)}")
            # Continue with event creation if limit check fails
    
    # Log the original event data for debugging
    logger.info(f"Creating event: {event_data}")
    
    # Auto-populate SEO fields
    try:
        event_data = auto_populate_seo_fields(event_data)
        logger.info(f"Auto-populated SEO fields: slug={event_data.get('slug')}, city={event_data.get('city')}, state={event_data.get('state')}, price={event_data.get('price')}")
    except Exception as e:
        logger.warning(f"SEO auto-population failed: {e}")
        # Continue with original data if auto-population fails
    
    try:
        # Use database connection with proper transaction control
        with get_db_transaction() as conn:
            cursor = conn.cursor()
            
            try:
                # Start transaction (autocommit is already properly disabled in get_db_transaction)
                cursor.execute("BEGIN")
                
                # More robust duplicate check with coordinate tolerance (1 meter)
                # Round coordinates to 6 decimal places (~1 meter precision)
                lat_rounded = round(event_data['lat'], 6)
                lng_rounded = round(event_data['lng'], 6)
                
                duplicate_check = f"""
                    SELECT id FROM events 
                    WHERE TRIM(LOWER(title)) = TRIM(LOWER({placeholder}))
                    AND date = {placeholder} 
                    AND start_time = {placeholder} 
                    AND ABS(lat - {placeholder}) < 0.000001
                    AND ABS(lng - {placeholder}) < 0.000001
                    AND category = {placeholder}
                """
                
                cursor.execute(duplicate_check, (
                    event_data['title'].strip(), 
                    event_data['date'], 
                    event_data['start_time'], 
                    lat_rounded, 
                    lng_rounded, 
                    event_data['category']
                ))
                
                duplicate = cursor.fetchone()
                if duplicate:
                    cursor.execute("ROLLBACK")
                    logger.warning(f"Duplicate event prevented: {event_data['title']} by user {current_user['id']}")
                    raise HTTPException(
                        status_code=409, 
                        detail="An event with these details already exists at this location and time"
                    )
                
                # Ensure unique slug before inserting
                base_slug = event_data.get('slug', '')
                if base_slug:
                    unique_slug = ensure_unique_slug(cursor, base_slug)
                    event_data['slug'] = unique_slug
                    logger.info(f"Generated unique slug: {unique_slug}")
                
                # Use dynamic schema detection to handle different database structures
                actual_columns = get_actual_table_columns(cursor, 'events')
                logger.debug(f"Detected {len(actual_columns)} columns in events table")
                
                # Filter event data to only include fields that exist in the actual database
                filtered_event_data = {}
                for key, value in event_data.items():
                    if key in actual_columns:
                        filtered_event_data[key] = value
                    else:
                        logger.debug(f"Skipping field '{key}' - not in database schema")
                
                # Add required fields that might be missing from the form data
                filtered_event_data['created_by'] = current_user["id"]
                filtered_event_data['created_at'] = datetime.utcnow().isoformat()
                
                # Auto-verify events for premium users
                if current_user.get('role') in ['premium', 'admin']:
                    filtered_event_data['verified'] = True
                
                if 'lat' not in filtered_event_data:
                    filtered_event_data['lat'] = lat_rounded
                if 'lng' not in filtered_event_data:
                    filtered_event_data['lng'] = lng_rounded
                if 'interest_count' not in filtered_event_data and 'interest_count' in actual_columns:
                    filtered_event_data['interest_count'] = 0
                if 'view_count' not in filtered_event_data and 'view_count' in actual_columns:
                    filtered_event_data['view_count'] = 0
                
                # Build dynamic INSERT query
                columns = [col for col in filtered_event_data.keys() if col in actual_columns]
                placeholders_str = ', '.join([placeholder] * len(columns))
                columns_str = ', '.join(columns)
                
                if IS_PRODUCTION and DB_URL:
                    insert_query = f"INSERT INTO events ({columns_str}) VALUES ({placeholders_str}) RETURNING id"
                else:
                    insert_query = f"INSERT INTO events ({columns_str}) VALUES ({placeholders_str})"
                
                values = [filtered_event_data[col] for col in columns]
                
                logger.debug(f"Dynamic INSERT with {len(columns)} columns: {columns[:5]}{'...' if len(columns) > 5 else ''}")
                
                # For PostgreSQL, use RETURNING to get the ID in one step
                if IS_PRODUCTION and DB_URL:
                    cursor.execute(insert_query, values)
                    result = cursor.fetchone()
                    event_id = result['id'] if result else None
                else:
                    # SQLite doesn't support RETURNING, so we use lastrowid
                    cursor.execute(insert_query.replace(" RETURNING id", ""), values)
                    event_id = cursor.lastrowid
                
                if not event_id:
                    cursor.execute("ROLLBACK")
                    raise ValueError("Failed to get ID of created event")
                
                # Fetch the created event
                fetch_query = f"SELECT * FROM events WHERE id = {placeholder}"
                cursor.execute(fetch_query, (event_id,))
                created_event_data = cursor.fetchone()
                
                if not created_event_data:
                    cursor.execute("ROLLBACK")
                    raise ValueError("Created event not found")
                
                # Commit the transaction
                cursor.execute("COMMIT")
                
                # Convert to dict and process datetime objects
                event_dict = dict(created_event_data)
                
                # Convert datetime objects and ensure proper field types
                event_dict = convert_event_datetime_fields(event_dict)
                
                # Clear event cache since a new event was created
                event_cache.clear()
                logger.info(f"Successfully created event {event_id}: {event_data['title']} with SEO fields populated")
                
                return event_dict
                
            except HTTPException as http_ex:
                # Attempt to roll back the transaction
                try:
                    cursor.execute("ROLLBACK")
                except Exception as rollback_error:
                    logger.error(f"Transaction rollback failed: {str(rollback_error)}")
                
                # Pass through HTTP exceptions without modification
                raise http_ex
            except Exception as e:
                # Attempt to roll back the transaction
                try:
                    cursor.execute("ROLLBACK")
                except Exception as rollback_error:
                    logger.error(f"Transaction rollback failed: {str(rollback_error)}")
                
                # Log the detailed error
                error_msg = str(e)
                logger.error(f"Database error in create_event: {error_msg}")
                
                # Wrap in HTTP exception
                raise HTTPException(
                    status_code=500,
                    detail=f"Database error: {error_msg}"
                ) from None
    
    except HTTPException as http_ex:
        # Pass through HTTP exceptions
        raise http_ex
    except Exception as e:
        # Catch any remaining errors
        error_msg = str(e)
        logger.error(f"Error creating event: {error_msg}")
        raise HTTPException(
            status_code=400, 
            detail=f"Could not create event: {error_msg}"
        ) from None
@app.get("/events/manage", response_model=List[EventResponse])
async def list_user_events(current_user: dict = Depends(get_current_user)):
    """
    Retrieve events created by the current user for management (robust version).
    Requires authentication.
    """
    placeholder = get_placeholder()
    try:
        with get_db() as conn:
            c = conn.cursor()
            
            # Query events created by the current user with proper column selection
            actual_columns = get_actual_table_columns(c, 'events')
            
            # Select only columns that exist
            if 'verified' in actual_columns:
                query = f"SELECT * FROM events WHERE created_by = {placeholder} ORDER BY date, start_time"
            else:
                # Exclude verified column if it doesn't exist
                columns = [col for col in actual_columns if col != 'verified']
                column_str = ', '.join(columns)
                query = f"SELECT {column_str} FROM events WHERE created_by = {placeholder} ORDER BY date, start_time"
            
            c.execute(query, (current_user["id"],))
            events = c.fetchall()
            
            # Convert to list of dictionaries and apply datetime conversion
            event_list = []
            for event in events:
                event_dict = dict(event)
                
                # Ensure all required fields exist for EventResponse model
                if 'verified' not in event_dict:
                    event_dict['verified'] = False
                if 'interest_count' not in event_dict:
                    event_dict['interest_count'] = 0
                if 'view_count' not in event_dict:
                    event_dict['view_count'] = 0
                if 'secondary_category' not in event_dict:
                    event_dict['secondary_category'] = None
                    
                event_dict = convert_event_datetime_fields(event_dict)
                event_list.append(event_dict)
            
            return event_list
            
    except Exception as e:
        logger.error(f"Error retrieving user events: {str(e)}")
        raise HTTPException(status_code=500, detail="Error retrieving user events")
@app.put("/events/{event_id}", response_model=EventResponse)
async def update_event(
    event_id: int, 
    event: EventCreate, 
    current_user: dict = Depends(get_current_user)
):
    """
    Update an existing event.
    Only the event creator or an admin can update the event.
    Auto-updates SEO fields based on new data.
    """
    placeholder = get_placeholder()
    
    # Convert Pydantic model to dict for processing
    event_data = event.dict()
    
    # Auto-populate SEO fields
    try:
        event_data = auto_populate_seo_fields(event_data)
        logger.info(f"Auto-populated SEO fields for update: slug={event_data.get('slug')}, city={event_data.get('city')}, state={event_data.get('state')}, price={event_data.get('price')}")
    except Exception as e:
        logger.warning(f"SEO auto-population failed during update: {e}")
        # Continue with original data if auto-population fails
    
    try:
        with get_db() as conn:
            try:
                cursor = conn.cursor()
                
                # Start transaction
                cursor.execute("BEGIN")
                
                # First, check if the event exists and who created it
                cursor.execute(f"SELECT * FROM events WHERE id = {placeholder}", (event_id,))
                existing_event = cursor.fetchone()
                
                if not existing_event:
                    cursor.execute("ROLLBACK")
                    raise HTTPException(status_code=404, detail="Event not found")
                
                # Check if current user is the creator or an admin
                if (current_user['id'] != existing_event['created_by'] and 
                    current_user['role'] != UserRole.ADMIN):
                    cursor.execute("ROLLBACK")
                    raise HTTPException(status_code=403, detail="Not authorized to update this event")
                
                # Ensure unique slug before updating
                base_slug = event_data.get('slug', '')
                if base_slug:
                    unique_slug = ensure_unique_slug(cursor, base_slug, event_id)
                    event_data['slug'] = unique_slug
                    logger.info(f"Generated unique slug for update: {unique_slug}")
                
                # Use dynamic schema detection for update as well
                actual_columns = get_actual_table_columns(cursor, 'events')
                logger.debug(f"Detected {len(actual_columns)} columns for update")
                
                # Filter event data to only include fields that exist in the actual database
                # Exclude fields that shouldn't be updated
                excluded_fields = {'id', 'created_at', 'created_by'}
                filtered_event_data = {}
                for key, value in event_data.items():
                    if key in actual_columns and key not in excluded_fields:
                        filtered_event_data[key] = value
                    elif key in excluded_fields:
                        logger.debug(f"Skipping field '{key}' - excluded from updates")
                    else:
                        logger.debug(f"Skipping field '{key}' - not in database schema")
                
                # Add updated_at timestamp if the column exists
                if 'updated_at' in actual_columns:
                    filtered_event_data['updated_at'] = datetime.utcnow().isoformat()
                
                # Build dynamic UPDATE query
                update_columns = list(filtered_event_data.keys())
                set_clause = ', '.join([f"{col} = {placeholder}" for col in update_columns])
                query = f"UPDATE events SET {set_clause} WHERE id = {placeholder}"
                values = list(filtered_event_data.values()) + [event_id]
                
                logger.debug(f"Dynamic UPDATE with {len(update_columns)} columns: {update_columns[:5]}{'...' if len(update_columns) > 5 else ''}")
                
                cursor.execute(query, values)
                
                # Fetch and return the updated event
                cursor.execute(f"SELECT * FROM events WHERE id = {placeholder}", (event_id,))
                updated_event = cursor.fetchone()
                
                if not updated_event:
                    cursor.execute("ROLLBACK")
                    raise HTTPException(status_code=404, detail="Event not found after update")
                
                # Commit the transaction
                cursor.execute("COMMIT")
                
                # Convert to dict and handle datetime objects
                event_dict = dict(updated_event)
                
                # Convert datetime objects and ensure proper field types
                event_dict = convert_event_datetime_fields(event_dict)
                
                # Clear event cache since an event was updated
                event_cache.clear()
                logger.info("Cleared event cache after updating event")
                
                return event_dict
                
            except HTTPException as http_ex:
                # Attempt to roll back the transaction
                try:
                    cursor.execute("ROLLBACK")
                except Exception as rollback_error:
                    logger.error(f"Transaction rollback failed: {str(rollback_error)}")
                
                # Re-raise the HTTP exception
                raise http_ex
            except Exception as e:
                # Attempt to roll back the transaction
                try:
                    cursor.execute("ROLLBACK")
                except Exception as rollback_error:
                    logger.error(f"Transaction rollback failed: {str(rollback_error)}")
                
                # Log error and raise HTTP exception
                error_msg = str(e)
                logger.error(f"Error updating event {event_id}: {error_msg}")
                raise HTTPException(status_code=500, detail=f"Error updating event: {error_msg}")
                
    except HTTPException as http_ex:
        # Pass through HTTP exceptions
        raise http_ex
    except Exception as e:
        # Handle other exceptions
        error_msg = str(e)
        logger.error(f"Error updating event {event_id}: {error_msg}")
        raise HTTPException(status_code=500, detail=f"Error updating event: {error_msg}")

@app.delete("/events/{event_id}")
async def delete_event(
    event_id: int, 
    current_user: dict = Depends(get_current_user)
):
    """
    Delete an event.
    Only the event creator or an admin can delete the event.
    """
    placeholder = get_placeholder()
    
    try:
        with get_db() as conn:
            try:
                cursor = conn.cursor()
                
                # Start transaction
                cursor.execute("BEGIN")
                
                # First, check if the event exists and who created it
                cursor.execute(f"SELECT * FROM events WHERE id = {placeholder}", (event_id,))
                existing_event = cursor.fetchone()
                
                if not existing_event:
                    cursor.execute("ROLLBACK")
                    raise HTTPException(status_code=404, detail="Event not found")
                
                # Check if current user is the creator or an admin
                if (current_user['id'] != existing_event['created_by'] and 
                    current_user['role'] != UserRole.ADMIN):
                    cursor.execute("ROLLBACK")
                    raise HTTPException(status_code=403, detail="Not authorized to delete this event")
                
                # Delete the event
                cursor.execute(f"DELETE FROM events WHERE id = {placeholder}", (event_id,))
                
                # Commit the transaction
                cursor.execute("COMMIT")
                
                # Clear event cache since an event was deleted
                event_cache.clear()
                
                # Also clean up any specific cache entries for this event
                event_cache.delete(f"event:{event_id}")
                event_cache.delete(f"event_interest:{event_id}")
                event_cache.delete(f"event_view:{event_id}")
                
                logger.info("Cleared event cache after deleting event")
                
                return {"detail": "Event successfully deleted"}
                
            except HTTPException as http_ex:
                # Attempt to roll back the transaction
                try:
                    cursor.execute("ROLLBACK")
                except Exception as rollback_error:
                    logger.error(f"Transaction rollback failed: {str(rollback_error)}")
                
                # Re-raise the HTTP exception
                raise http_ex
            except Exception as e:
                # Attempt to roll back the transaction
                try:
                    cursor.execute("ROLLBACK")
                except Exception as rollback_error:
                    logger.error(f"Transaction rollback failed: {str(rollback_error)}")
                
                # Log error and raise HTTP exception
                error_msg = str(e)
                logger.error(f"Error deleting event {event_id}: {error_msg}")
                raise HTTPException(status_code=500, detail=f"Error deleting event: {error_msg}")
                
    except HTTPException as http_ex:
        # Pass through HTTP exceptions
        raise http_ex
    except Exception as e:
        # Handle other exceptions
        error_msg = str(e)
        logger.error(f"Error deleting event {event_id}: {error_msg}")
        raise HTTPException(status_code=500, detail=f"Error deleting event: {error_msg}")

# Optional: Admin endpoint to get all users (for admin dashboard)
@app.get("/admin/users", response_model=List[UserResponse])
async def list_users(current_user: dict = Depends(get_current_user)):
    """
    List all users (admin-only endpoint)
    """
    if current_user['role'] != UserRole.ADMIN:
        raise HTTPException(status_code=403, detail="Not authorized")
    
    try:
        with get_db() as conn:
            c = conn.cursor()
            c.execute("SELECT * FROM users")
            users = c.fetchall()
            
            return [dict(user) for user in users]
    except Exception as e:
        logger.error(f"Error retrieving users: {str(e)}")
        raise HTTPException(status_code=500, detail="Error retrieving users")

# Health check endpoint
@app.get("/health")
async def health_check():
    """
    Health check endpoint that returns app status.
    """
    try:
        # Test database connection
        with get_db() as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT 1")
            cursor.fetchone()
        
        # Get cache stats for monitoring
        cache_stats = event_cache.stats()
        
        return {
            "status": "healthy",
            "timestamp": datetime.utcnow().isoformat(),
            "database": "connected",
            "cache": cache_stats,
            "memory_optimization": "enabled"
        }
    except Exception as e:
        logger.error(f"Health check failed: {str(e)}")
        return {
            "status": "unhealthy",
            "timestamp": datetime.utcnow().isoformat(),
            "database": "error",
            "error": str(e),
            "cache": event_cache.stats() if event_cache else None
        }

# Root endpoint
@app.get("/")
async def root():
    """
    Root endpoint for API
    """
    return {
        "message": "EventFinder API is running",
        "docs": "/docs",
        "health": "/health"
    }

# New Admin Endpoints
@app.delete("/admin/users/{user_id}")
async def delete_user(
    user_id: int, 
    current_user: dict = Depends(get_current_user)
):
    """
    Delete a user (admin-only)
    """
    if current_user['role'] != UserRole.ADMIN:
        raise HTTPException(status_code=403, detail="Not authorized")
    
    placeholder = get_placeholder()
    try:
        with get_db() as conn:
            c = conn.cursor()
            
            # Check if user exists
            c.execute(f"SELECT * FROM users WHERE id = {placeholder}", (user_id,))
            user = c.fetchone()
            
            if not user:
                raise HTTPException(status_code=404, detail="User not found")
            
            # Prevent deleting the last admin
            c.execute(f"SELECT COUNT(*) as admin_count FROM users WHERE role = {placeholder}", (UserRole.ADMIN,))
            admin_count = c.fetchone()['admin_count']
            
            if admin_count <= 1 and user['role'] == UserRole.ADMIN:
                raise HTTPException(status_code=400, detail="Cannot delete the last admin")
            
            # Delete user
            c.execute(f"DELETE FROM users WHERE id = {placeholder}", (user_id,))
            
            # Optionally, delete user's events
            c.execute(f"DELETE FROM events WHERE created_by = {placeholder}", (user_id,))
            
            conn.commit()
            
            return {"detail": "User successfully deleted"}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error deleting user {user_id}: {str(e)}")
        raise HTTPException(status_code=500, detail="Error deleting user")

@app.put("/admin/users/{user_id}/role")
async def update_user_role(
    user_id: int, 
    role: UserRole,
    current_user: dict = Depends(get_current_user)
):
    """
    Update user role (admin-only)
    """
    if current_user['role'] != UserRole.ADMIN:
        raise HTTPException(status_code=403, detail="Not authorized")
    
    placeholder = get_placeholder()
    try:
        with get_db() as conn:
            c = conn.cursor()
            
            # Check if user exists
            c.execute(f"SELECT * FROM users WHERE id = {placeholder}", (user_id,))
            user = c.fetchone()
            
            if not user:
                raise HTTPException(status_code=404, detail="User not found")
            
            # Prevent removing last admin
            if user['role'] == UserRole.ADMIN:
                c.execute(f"SELECT COUNT(*) as admin_count FROM users WHERE role = {placeholder}", (UserRole.ADMIN,))
                admin_count = c.fetchone()['admin_count']
                
                if admin_count <= 1:
                    raise HTTPException(status_code=400, detail="Cannot remove the last admin")
            
            # Update role
            c.execute(f"UPDATE users SET role = {placeholder} WHERE id = {placeholder}", (role, user_id))
            conn.commit()
            
            return {"detail": "User role updated successfully"}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error updating user role {user_id}: {str(e)}")
        raise HTTPException(status_code=500, detail="Error updating user role")

class AdminPasswordReset(BaseModel):
    new_password: str
    
    @validator('new_password')
    def validate_password(cls, v):
        # Use the PasswordValidator to get validation results
        validation = PasswordValidator.validate_password(v)
        
        # If not valid, raise a validation error with all feedback
        if not validation['is_valid']:
            raise ValueError('\n'.join(validation['feedback']))
        
        return v

@app.put("/admin/users/{user_id}/password")
async def admin_reset_user_password(
    user_id: int,
    password_data: AdminPasswordReset,
    current_user: dict = Depends(get_current_user)
):
    """
    Reset a user's password (admin-only)
    """
    if current_user['role'] != UserRole.ADMIN:
        raise HTTPException(status_code=403, detail="Not authorized")
    
    placeholder = get_placeholder()
    try:
        with get_db() as conn:
            c = conn.cursor()
            
            # Check if user exists
            c.execute(f"SELECT * FROM users WHERE id = {placeholder}", (user_id,))
            user = c.fetchone()
            
            if not user:
                raise HTTPException(status_code=404, detail="User not found")
            
            # Hash the new password
            hashed_password = get_password_hash(password_data.new_password)
            
            # Update the user's password
            c.execute(f"UPDATE users SET hashed_password = {placeholder} WHERE id = {placeholder}", (hashed_password, user_id))
            conn.commit()
            
            # Log the activity
            log_activity(current_user['id'], "admin_password_reset", f"Reset password for user {user['email']}")
            
            return {"detail": "Password reset successfully"}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error resetting password for user {user_id}: {str(e)}")
        raise HTTPException(status_code=500, detail="Error resetting password")

@app.get("/admin/activity-logs")
async def get_activity_logs(
    current_user: dict = Depends(get_current_user),
    limit: int = 500,
    offset: int = 0
):
    """
    Retrieve activity logs (admin-only)
    """
    if current_user['role'] != UserRole.ADMIN:
        raise HTTPException(status_code=403, detail="Not authorized")
    
    placeholder = get_placeholder()
    try:
        with get_db() as conn:
            c = conn.cursor()
            
            # Create an activity logs table if it doesn't exist
            if IS_PRODUCTION and DB_URL:
                c.execute('''CREATE TABLE IF NOT EXISTS activity_logs (
                    id SERIAL PRIMARY KEY,
                    user_id INTEGER,
                    action TEXT NOT NULL,
                    details TEXT,
                    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY(user_id) REFERENCES users(id)
                )''')
            else:
                c.execute('''CREATE TABLE IF NOT EXISTS activity_logs (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    user_id INTEGER,
                    action TEXT NOT NULL,
                    details TEXT,
                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY(user_id) REFERENCES users(id)
                )''')
            
            # Fetch logs with optional pagination
            c.execute(f"""
                SELECT al.*, u.email 
                FROM activity_logs al
                LEFT JOIN users u ON al.user_id = u.id
                ORDER BY al.timestamp DESC
                LIMIT {placeholder} OFFSET {placeholder}
            """, (limit, offset))
            
            logs = [dict(log) for log in c.fetchall()]
            
            return logs
    except Exception as e:
        logger.error(f"Error retrieving activity logs: {str(e)}")
        raise HTTPException(status_code=500, detail="Error retrieving activity logs")

# Utility function to log activities
def log_activity(user_id: int, action: str, details: str = None):
    """
    Log user activities
    """
    placeholder = get_placeholder()
    try:
        with get_db() as conn:
            c = conn.cursor()
            
            # Ensure activity_logs table exists
            if IS_PRODUCTION and DB_URL:
                c.execute('''CREATE TABLE IF NOT EXISTS activity_logs (
                    id SERIAL PRIMARY KEY,
                    user_id INTEGER,
                    action TEXT NOT NULL,
                    details TEXT,
                    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY(user_id) REFERENCES users(id)
                )''')
            else:
                c.execute('''CREATE TABLE IF NOT EXISTS activity_logs (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    user_id INTEGER,
                    action TEXT NOT NULL,
                    details TEXT,
                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY(user_id) REFERENCES users(id)
                )''')
            
            # Insert log entry
            c.execute(
                f"INSERT INTO activity_logs (user_id, action, details) VALUES ({placeholder}, {placeholder}, {placeholder})",
                (user_id, action, details)
            )
            conn.commit()
    except Exception as e:
        logger.error(f"Error logging activity: {str(e)}")

# Helper functions for AI event processing
def _generate_ai_summary(event_dict):
    """Generate AI-friendly summary with proper time handling"""
    start_time = event_dict['start_time']
    end_time = event_dict.get('end_time')
    end_date = event_dict.get('end_date')
    
    # Build time string
    if end_time:
        time_str = f"from {start_time} to {end_time}"
    else:
        time_str = f"at {start_time}"
        
    # Build date string
    if end_date and end_date != event_dict['date']:
        date_str = f"from {event_dict['date']} to {end_date}"
    else:
        date_str = f"on {event_dict['date']}"
        
    description = event_dict['description'][:200]
    if len(event_dict['description']) > 200:
        description += "..."
        
    return f"{event_dict['title']} - {event_dict['category']} event {date_str} {time_str} in {event_dict['address']}. {description}"

def _generate_structured_data(event_dict):
    """Generate structured data with proper date/time handling"""
    start_datetime = f"{event_dict['date']}T{event_dict['start_time']}:00"
    
    # Calculate end datetime
    end_date = event_dict.get('end_date') or event_dict['date']
    end_time = event_dict.get('end_time') or event_dict['start_time']
    end_datetime = f"{end_date}T{end_time}:00"
    
    return {
        "@type": "Event",
        "name": event_dict['title'],
        "startDate": start_datetime,
        "endDate": end_datetime,
        "location": {
            "@type": "Place",
            "address": event_dict['address']
        },
        "description": event_dict['description'],
        "eventStatus": "EventScheduled"
    }

# AI Search API Endpoints
@app.get("/api/v1/local-events")
async def get_local_events_for_ai(
    lat: Optional[float] = None,
    lng: Optional[float] = None,
    radius: Optional[float] = 25.0,  # miles
    category: Optional[str] = None,
    limit: Optional[int] = 200
):
    """
    Public API endpoint for AI search tools to discover local events.
    
    This endpoint provides structured event data that AI tools can use
    to answer queries like "local events near me" or "events this weekend".
    """
    placeholder = get_placeholder()
    
    try:
        with get_db() as conn:
            c = conn.cursor()
            
            # Build base query including UX fields
            query = """
                SELECT id, title, description, date, start_time, end_time, end_date, category, 
                       address, lat, lng, created_at, fee_required, event_url, host_name
                FROM events 
                WHERE date::date >= CURRENT_DATE
            """
            params = []
            
            # Add category filter
            if category:
                query += f" AND category = {placeholder}"
                params.append(category)
            
            # Add location-based filtering if coordinates provided
            if lat is not None and lng is not None:
                # Use Haversine formula for distance calculation
                query += f"""
                    AND (
                        3959 * acos(
                            cos(radians({placeholder})) * cos(radians(lat)) * 
                            cos(radians(lng) - radians({placeholder})) + 
                            sin(radians({placeholder})) * sin(radians(lat))
                        )
                    ) <= {placeholder}
                """
                params.extend([lat, lng, lat, radius])
            
            # Order by date and limit results
            query += f" ORDER BY date, start_time LIMIT {placeholder}"
            params.append(limit)
            
            c.execute(query, params)
            events = c.fetchall()
            
            # Format response for AI consumption
            ai_response = {
                "status": "success",
                "message": "Local events discovered",
                "search_context": {
                    "query_type": "local_events_near_me",
                    "location": {"lat": lat, "lng": lng} if lat and lng else None,
                    "radius_miles": radius,
                    "category_filter": category,
                    "results_count": len(events)
                },
                "events": [],
                "metadata": {
                    "platform": "todo-events.com",
                    "description": "Real-time local event discovery platform",
                    "last_updated": datetime.utcnow().isoformat(),
                    "categories": ["food-drink", "music", "arts", "sports", "community"],
                    "coverage_area": "United States",
                    "features": [
                        "Location-based event search",
                        "Real-time event updates", 
                        "Community-driven content",
                        "Interactive event mapping",
                        "Category-based filtering"
                    ]
                }
            }
            
            # Process each event with AI-friendly formatting
            for event in events:
                event_dict = dict(event)
                
                # Calculate distance if location provided
                distance = None
                if lat is not None and lng is not None:
                    try:
                        from math import radians, cos, sin, asin, sqrt
                        
                        def haversine(lat1, lon1, lat2, lon2):
                            # Convert to radians
                            lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])
                            
                            # Haversine formula
                            dlat = lat2 - lat1
                            dlon = lon2 - lon1
                            a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2
                            c = 2 * asin(sqrt(a))
                            r = 3959  # Earth radius in miles
                            return c * r
                        
                        distance = round(haversine(lat, lng, event_dict['lat'], event_dict['lng']), 1)
                    except:
                        distance = None
                
                # Format event for AI consumption
                ai_event = {
                    "id": event_dict['id'],
                    "title": event_dict['title'],
                    "description": event_dict['description'],
                    "date": event_dict['date'],
                    "start_time": event_dict['start_time'],
                    "end_time": event_dict['end_time'],
                    "end_date": event_dict['end_date'],
                    "category": event_dict['category'],
                    "location": {
                        "address": event_dict['address'],
                        "coordinates": {
                            "lat": event_dict['lat'],
                            "lng": event_dict['lng']
                        },
                        "distance_miles": distance
                    },
                    "url": f"https://todo-events.com/?event={event_dict['id']}",
                    "ai_summary": _generate_ai_summary(event_dict),
                    "structured_data": _generate_structured_data(event_dict)
                }
                
                ai_response["events"].append(ai_event)
            
            # Add helpful context for AI tools
            if len(events) == 0:
                ai_response["message"] = "No local events found matching your criteria"
                ai_response["suggestions"] = [
                    "Try expanding your search radius",
                    "Remove category filters to see all event types", 
                    "Check for events on different dates",
                    "Visit todo-events.com to create the first event in your area"
                ]
            
            return ai_response
            
    except Exception as e:
        logger.error(f"Error in AI events API: {str(e)}")
        return {
            "status": "error",
            "message": "Unable to retrieve local events",
            "error": str(e),
            "metadata": {
                "platform": "todo-events.com",
                "support": "Visit https://todo-events.com for manual event discovery"
            }
        }

# Automation Control Endpoints
@app.get("/api/v1/automation/status")
async def get_automation_status():
    """Get status of automated tasks"""
    return {
        "automation": {
            "enabled": IS_PRODUCTION,
            "scheduler_running": task_manager.scheduler.running if IS_PRODUCTION else False,
            "last_sitemap_update": task_manager.last_sitemap_update.isoformat() if task_manager.last_sitemap_update else None,
            "last_event_refresh": task_manager.last_event_refresh.isoformat() if task_manager.last_event_refresh else None,
            "task_status": task_manager.task_status,
            "next_runs": {
                job.id: job.next_run_time.isoformat() if job.next_run_time else None
                for job in task_manager.scheduler.get_jobs()
            } if IS_PRODUCTION and task_manager.scheduler.running else {}
        },
        "system": {
            "environment": "production" if IS_PRODUCTION else "development",
            "timestamp": datetime.utcnow().isoformat()
        }
    }
@app.post("/api/v1/automation/trigger/{task_name}")
async def trigger_automated_task(task_name: str, background_tasks: BackgroundTasks):
    """Manually trigger automated tasks"""
    
    if not IS_PRODUCTION:
        raise HTTPException(status_code=400, detail="Automation only available in production")
    
    task_functions = {
        "sitemap": task_manager.generate_sitemap_automatically,
        "events": task_manager.refresh_event_data,
        "ai_sync": task_manager.sync_with_ai_tools,
        "cleanup": task_manager.cleanup_expired_events
    }
    
    if task_name not in task_functions:
        raise HTTPException(
            status_code=400, 
            detail=f"Unknown task: {task_name}. Available: {list(task_functions.keys())}"
        )
    
    # Trigger the task in the background
    background_tasks.add_task(task_functions[task_name])
    
    return {
        "message": f"Task '{task_name}' triggered successfully",
        "task": task_name,
        "triggered_at": datetime.utcnow().isoformat(),
        "note": "Task is running in the background"
    }

@app.get("/sitemap.xml")
async def get_dynamic_sitemap():
    """Serve dynamically generated sitemap"""
    try:
        # If we have a cached sitemap from automation, serve it
        if hasattr(task_manager, 'current_sitemap') and task_manager.current_sitemap:
            return Response(
                content=task_manager.current_sitemap,
                media_type="application/xml",
                headers={"Cache-Control": "public, max-age=3600"}  # Cache for 1 hour
            )
        
        # Otherwise, generate on-demand
        events = await task_manager.get_current_events()
        sitemap_content = await task_manager.build_sitemap_content(events)
        
        return Response(
            content=sitemap_content,
            media_type="application/xml",
            headers={"Cache-Control": "public, max-age=1800"}  # Cache for 30 minutes
        )
        
    except Exception as e:
        logger.error(f"Error serving dynamic sitemap: {str(e)}")
        raise HTTPException(status_code=500, detail="Error generating sitemap")
@app.put("/admin/events/{event_id}/verification")
async def toggle_event_verification(
    event_id: int,
    verification_data: dict,
    current_user: dict = Depends(get_current_user)
):
    """
    Toggle event verification status (admin-only)
    """
    if current_user['role'] != UserRole.ADMIN:
        raise HTTPException(status_code=403, detail="Not authorized")
    
    verified = verification_data.get('verified', False)
    placeholder = get_placeholder()
    
    try:
        with get_db() as conn:
            c = conn.cursor()
            
            # Check if event exists
            c.execute(f"SELECT * FROM events WHERE id = {placeholder}", (event_id,))
            event = c.fetchone()
            
            if not event:
                raise HTTPException(status_code=404, detail="Event not found")
            
            # Update verification status
            c.execute(f"UPDATE events SET verified = {placeholder} WHERE id = {placeholder}", (verified, event_id))
            conn.commit()
            
            return {"detail": f"Event verification {'enabled' if verified else 'disabled'} successfully"}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error updating event verification {event_id}: {str(e)}")
        raise HTTPException(status_code=500, detail="Error updating event verification")

@app.get("/admin/activity-logs")
def generate_browser_fingerprint(request: Request) -> str:
    """Generate a browser fingerprint for anonymous users"""
    import hashlib
    
    # Get client info
    user_agent = request.headers.get("user-agent", "")
    client_ip = request.client.host if request.client else "unknown"
    accept_language = request.headers.get("accept-language", "")
    accept_encoding = request.headers.get("accept-encoding", "")
import os
import re
import sqlite3
import logging
import time
import json
import traceback
from typing import Optional, List, Dict, Any
from datetime import datetime, timedelta
from contextlib import contextmanager
from enum import Enum
import asyncio
import threading
import stripe

# Import SEO utilities
try:
    from seo_utils import SEOEventProcessor, generate_event_json_ld, generate_seo_metadata, slugify
except ImportError:
    print("‚ö†Ô∏è SEO utilities not available - install seo_utils.py for full SEO features")
    def slugify(text): return text.lower().replace(' ', '-')
    SEOEventProcessor = None

import uvicorn
from dotenv import load_dotenv

from fastapi import FastAPI, HTTPException, Depends, status, BackgroundTasks, Request, Header

# MissionOps import
try:
    from missionops_endpoints import missionops_router
except ImportError:
    missionops_router = None
from fastapi.security import OAuth2PasswordBearer, OAuth2PasswordRequestForm
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse, Response, JSONResponse

from pydantic import BaseModel, EmailStr, validator
from passlib.context import CryptContext
import jwt

# Scheduler imports
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.interval import IntervalTrigger
import httpx

# Load environment variables
load_dotenv()

# Security configuration
SECRET_KEY = os.getenv("SECRET_KEY", "fallback-secret-key-for-development-only")
ALGORITHM = os.getenv("ALGORITHM", "HS256")
ACCESS_TOKEN_EXPIRE_MINUTES = int(os.getenv("ACCESS_TOKEN_EXPIRE_MINUTES", 30))

# Environment configuration 
IS_PRODUCTION = os.getenv("RENDER", False) or os.getenv("RAILWAY_ENVIRONMENT", False)
DB_URL = os.getenv("DATABASE_URL", None)

# Stripe configuration
stripe.api_key = os.getenv("STRIPE_SECRET_KEY")
STRIPE_PUBLISHABLE_KEY = os.getenv("STRIPE_PUBLISHABLE_KEY")
STRIPE_WEBHOOK_SECRET = os.getenv("STRIPE_WEBHOOK_SECRET")
STRIPE_PRICE_ID = os.getenv("STRIPE_PRICE_ID")  # Monthly subscription price ID

# Password hashing setup
pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")
oauth2_scheme = OAuth2PasswordBearer(tokenUrl="token")

# Logging setup
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Create FastAPI app
app = FastAPI(title="EventFinder API")

# Register MissionOps router
if missionops_router:
    app.include_router(missionops_router)
    print("‚úÖ MissionOps endpoints registered")

# Dynamic CORS origins based on environment
def get_cors_origins():
    base_origins = [
        "http://localhost:5173",  # Local Vite dev server
        "http://127.0.0.1:5173",
        "http://localhost:5174",  # Admin dashboard local
        "http://127.0.0.1:5174",
    ]
    
    if IS_PRODUCTION:
        # In production, add common Render.com patterns including the actual deployed URL
        production_origins = [
            "https://todoevents.onrender.com",  # Actual frontend URL
            "https://eventfinder-api.onrender.com",
            "https://eventfinder-web.onrender.com", 
            "https://eventfinder-admin.onrender.com",
            "https://todoevents-1.onrender.com",
            "https://todoevents-1-frontend.onrender.com", 
            "https://todoevents-1-web.onrender.com",
            "https://todoevents-api.onrender.com",
            "https://todoevents-frontend.onrender.com",
            "https://todoevents-web.onrender.com",
        ]
        return base_origins + production_origins
    else:
        return base_origins

# Custom CORS middleware for flexible Render.com handling
@app.middleware("http")
async def cors_handler(request, call_next):
    # Log the request for debugging (reduced logging for performance)
    if request.url.path == "/api/report-event":
        logger.info(f"üö® CORS MIDDLEWARE: {request.method} {request.url.path}")
        logger.info(f"üö® Headers: {dict(request.headers)}")
    origin = request.headers.get("origin")
    
    # Handle preflight requests
    if request.method == "OPTIONS":
        # Determine allowed origin
        allowed_origin = "*"  # Default fallback
        
        if origin:
            # Allow localhost and 127.0.0.1 for development
            if ("localhost" in origin or "127.0.0.1" in origin):
                allowed_origin = origin
            # Allow Render.com domains for production
            elif ".onrender.com" in origin:
                allowed_origin = origin
            # Allow the actual domain
            elif "todo-events.com" in origin:
                allowed_origin = origin
            else:
                allowed_origin = origin
        
        # Return proper preflight response
        return Response(
            status_code=200,
            headers={
                "Access-Control-Allow-Origin": allowed_origin,
                "Access-Control-Allow-Methods": "GET, POST, PUT, DELETE, OPTIONS",
                "Access-Control-Allow-Headers": "Content-Type, Authorization, X-Requested-With, Cache-Control, Pragma",
                "Access-Control-Allow-Credentials": "true",
                "Access-Control-Max-Age": "86400",
            }
        )
    
    try:
        # Call the next middleware or endpoint handler
        response = await call_next(request)
        
        # Add CORS headers to successful responses
        if origin:
            # Determine allowed origin for response
            allowed_origin = "*"
            if ("localhost" in origin or "127.0.0.1" in origin or 
                ".onrender.com" in origin or "todo-events.com" in origin):
                allowed_origin = origin
            
            response.headers["Access-Control-Allow-Origin"] = allowed_origin
            response.headers["Access-Control-Allow-Credentials"] = "true"
            response.headers["Access-Control-Allow-Methods"] = "GET, POST, PUT, DELETE, OPTIONS"
            response.headers["Access-Control-Allow-Headers"] = "Content-Type, Authorization, X-Requested-With, Cache-Control, Pragma"
        
        return response
        
    except Exception as e:
        # Handle exceptions and ensure CORS headers are added to error responses
        logger.error(f"Error during request processing: {str(e)}")
        
        # Create response with error details
        status_code = 500
        if isinstance(e, HTTPException):
            status_code = e.status_code
            
        error_response = JSONResponse(
            status_code=status_code,
            content={"detail": str(e)},
        )
        
        # Add CORS headers to error response
        if origin:
            allowed_origin = "*"
            if ("localhost" in origin or "127.0.0.1" in origin or 
                ".onrender.com" in origin or "todo-events.com" in origin):
                allowed_origin = origin
                
            error_response.headers["Access-Control-Allow-Origin"] = allowed_origin
            error_response.headers["Access-Control-Allow-Credentials"] = "true"
            
        return error_response

# CORS middleware with dynamic origins (as fallback)
# Commenting out the built-in CORS middleware since we have custom CORS handling
# app.add_middleware(
#     CORSMiddleware,
#     allow_origins=get_cors_origins(),
#     allow_credentials=True,
#     allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
#     allow_headers=["*"],
#     max_age=86400,  # Cache preflight requests for 24 hours
# )

# Database initialization
# Force production database migration for interest/view tracking - v2.1
def init_db():
    try:
        with get_db() as conn:
            c = conn.cursor()
            
            if IS_PRODUCTION and DB_URL:
                # PostgreSQL table creation
                
                # Create users table
                c.execute('''
                    CREATE TABLE IF NOT EXISTS users (
                        id SERIAL PRIMARY KEY,
                        email TEXT UNIQUE NOT NULL,
                        hashed_password TEXT NOT NULL,
                        role TEXT NOT NULL,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                ''')
                
                # Create events table with user relationship
                c.execute('''
                    CREATE TABLE IF NOT EXISTS events (
                        id SERIAL PRIMARY KEY,
                        title TEXT NOT NULL,
                        description TEXT NOT NULL,
                        date TEXT NOT NULL,
                        start_time TEXT NOT NULL,
                        end_time TEXT,
                        end_date TEXT,
                        category TEXT NOT NULL,
                        secondary_category TEXT,
                        address TEXT NOT NULL,
                        lat REAL NOT NULL,
                        lng REAL NOT NULL,
                        recurring BOOLEAN NOT NULL DEFAULT FALSE,
                        frequency TEXT,
                        created_by INTEGER,
                        interest_count INTEGER DEFAULT 0,
                        view_count INTEGER DEFAULT 0,
                        fee_required TEXT,
                        event_url TEXT,
                        host_name TEXT,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        verified BOOLEAN DEFAULT FALSE,
                        FOREIGN KEY(created_by) REFERENCES users(id)
                    )
                ''')
                
                # Add migration for existing events to have start_time and end_time
                try:
                    # Use PostgreSQL-compatible column checking method with fallback
                    def column_exists(table_name, column_name):
                        try:
                            # Try information_schema first
                            c.execute("""
                                SELECT column_name FROM information_schema.columns 
                                WHERE table_name = %s AND column_name = %s
                            """, (table_name, column_name))
                            return c.fetchone() is not None
                        except Exception:
                            # Fallback: try to add column and catch error if it exists
                            try:
                                c.execute(f'ALTER TABLE {table_name} ADD COLUMN {column_name}_test_temp TEXT')
                                c.execute(f'ALTER TABLE {table_name} DROP COLUMN {column_name}_test_temp')
                                # If we got here, column doesn't exist
                                return False
                            except Exception:
                                # If adding temp column failed, assume original column exists
                                return True
                    
                    # Check if we need to migrate old 'time' column to 'start_time'
                    if column_exists('events', 'time'):
                        # Migrate old 'time' column to 'start_time'
                        c.execute('ALTER TABLE events RENAME COLUMN time TO start_time')
                        logger.info("‚úÖ Migrated 'time' column to 'start_time'")
                        conn.commit()
                        
                    # Add start_time column if it doesn't exist
                    if not column_exists('events', 'start_time'):
                        c.execute('ALTER TABLE events ADD COLUMN start_time TEXT DEFAULT \'12:00\'')
                        logger.info("‚úÖ Added 'start_time' column")
                        conn.commit()
                        
                    # Add end_time column if it doesn't exist
                    if not column_exists('events', 'end_time'):
                        c.execute('ALTER TABLE events ADD COLUMN end_time TEXT')
                        logger.info("‚úÖ Added 'end_time' column")
                        conn.commit()
                        
                    # Add end_date column if it doesn't exist
                    if not column_exists('events', 'end_date'):
                        c.execute('ALTER TABLE events ADD COLUMN end_date TEXT')
                        logger.info("‚úÖ Added 'end_date' column")
                        conn.commit()
                    
                    # Add interest_count column if it doesn't exist
                    if not column_exists('events', 'interest_count'):
                        c.execute('ALTER TABLE events ADD COLUMN interest_count INTEGER DEFAULT 0')
                        logger.info("‚úÖ Added 'interest_count' column")
                        conn.commit()
                    
                    # Add view_count column if it doesn't exist
                    if not column_exists('events', 'view_count'):
                        c.execute('ALTER TABLE events ADD COLUMN view_count INTEGER DEFAULT 0')
                        logger.info("‚úÖ Added 'view_count' column")
                        conn.commit()
                    
                    # Add new UX enhancement columns
                    if not column_exists('events', 'fee_required'):
                        c.execute('ALTER TABLE events ADD COLUMN fee_required TEXT')
                        logger.info("‚úÖ Added 'fee_required' column")
                        conn.commit()
                    
                    if not column_exists('events', 'event_url'):
                        c.execute('ALTER TABLE events ADD COLUMN event_url TEXT')
                        logger.info("‚úÖ Added 'event_url' column")
                        conn.commit()
                    
                    if not column_exists('events', 'host_name'):
                        c.execute('ALTER TABLE events ADD COLUMN host_name TEXT')
                        logger.info("‚úÖ Added 'host_name' column")
                        conn.commit()
                    
                    # Add secondary_category column if it doesn't exist
                    if not column_exists('events', 'secondary_category'):
                        c.execute('ALTER TABLE events ADD COLUMN secondary_category TEXT')
                        logger.info("‚úÖ Added 'secondary_category' column")
                        conn.commit()
                        
                    # Add verified column if it doesn't exist
                    if not column_exists('events', 'verified'):
                        c.execute('ALTER TABLE events ADD COLUMN verified BOOLEAN DEFAULT FALSE')
                        logger.info("‚úÖ Added 'verified' column")
                        conn.commit()
                        
                except Exception as migration_error:
                    logger.error(f"‚ùå Schema fix error: {migration_error}")
                    # Don't fail the entire initialization, just log the error
                    try:
                        conn.rollback()
                    except:
                        pass
                
                # Create activity_logs table
                c.execute('''
                    CREATE TABLE IF NOT EXISTS activity_logs (
                        id SERIAL PRIMARY KEY,
                        user_id INTEGER,
                        action TEXT NOT NULL,
                        details TEXT,
                        timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        FOREIGN KEY(user_id) REFERENCES users(id)
                    )
                ''')
                
                # Create interest tracking table
                c.execute('''CREATE TABLE IF NOT EXISTS event_interests (
                            id SERIAL PRIMARY KEY,
                            event_id INTEGER NOT NULL,
                            user_id INTEGER,
                            browser_fingerprint TEXT NOT NULL DEFAULT 'legacy',
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            UNIQUE(event_id, user_id, browser_fingerprint),
                            FOREIGN KEY(event_id) REFERENCES events(id) ON DELETE CASCADE,
                            FOREIGN KEY(user_id) REFERENCES users(id) ON DELETE SET NULL
                        )''')
                
                # Create view tracking table
                c.execute('''CREATE TABLE IF NOT EXISTS event_views (
                            id SERIAL PRIMARY KEY,
                            event_id INTEGER NOT NULL,
                            user_id INTEGER,
                            browser_fingerprint TEXT NOT NULL DEFAULT 'legacy',
                            viewed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            UNIQUE(event_id, user_id, browser_fingerprint),
                            FOREIGN KEY(event_id) REFERENCES events(id) ON DELETE CASCADE,
                            FOREIGN KEY(user_id) REFERENCES users(id) ON DELETE SET NULL
                        )''')
                
                # Create page visits tracking table
                c.execute('''CREATE TABLE IF NOT EXISTS page_visits (
                            id SERIAL PRIMARY KEY,
                            page_type TEXT NOT NULL,
                            page_path TEXT NOT NULL,
                            user_id INTEGER,
                            browser_fingerprint TEXT NOT NULL DEFAULT 'anonymous',
                            visited_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            FOREIGN KEY(user_id) REFERENCES users(id) ON DELETE SET NULL
                        )''')

                # Create privacy requests table for CCPA compliance
                c.execute('''CREATE TABLE IF NOT EXISTS privacy_requests (
                            id SERIAL PRIMARY KEY,
                            request_type TEXT NOT NULL CHECK (request_type IN ('access', 'delete', 'opt_out')),
                            email TEXT NOT NULL,
                            full_name TEXT,
                            verification_info TEXT,
                            details TEXT,
                            status TEXT DEFAULT 'pending' CHECK (status IN ('pending', 'in_progress', 'completed', 'denied')),
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            completed_at TIMESTAMP,
                            admin_notes TEXT
                        )''')

                # Create event reports table for reporting functionality
                c.execute('''CREATE TABLE IF NOT EXISTS event_reports (
                            id SERIAL PRIMARY KEY,
                            event_id INTEGER NOT NULL,
                            event_title TEXT,
                            event_address TEXT,
                            event_date TEXT,
                            reason TEXT NOT NULL,
                            category TEXT NOT NULL,
                            description TEXT NOT NULL,
                            reporter_email TEXT NOT NULL,
                            reporter_name TEXT,
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            status TEXT DEFAULT 'pending',
                            FOREIGN KEY(event_id) REFERENCES events(id) ON DELETE CASCADE
                        )''')
                
                # Ensure tables are committed
                conn.commit()
                logger.info("‚úÖ Interest, view tracking, privacy requests, and event reports tables created/verified")
            else:
                # SQLite table creation (existing code)
                c.execute('''CREATE TABLE IF NOT EXISTS users (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            email TEXT UNIQUE NOT NULL,
                            hashed_password TEXT NOT NULL,
                            role TEXT NOT NULL,
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                        )''')
                
                # Check if events table exists and needs migration
                c.execute('''PRAGMA table_info(events)''')
                columns = [column[1] for column in c.fetchall()]
                
                if 'events' not in [table[0] for table in c.execute("SELECT name FROM sqlite_master WHERE type='table'").fetchall()]:
                    # Create new table with updated schema
                    c.execute('''CREATE TABLE events (
                                id INTEGER PRIMARY KEY AUTOINCREMENT,
                                title TEXT NOT NULL,
                                description TEXT NOT NULL,
                                date TEXT NOT NULL,
                                start_time TEXT NOT NULL,
                                end_time TEXT,
                                end_date TEXT,
                                category TEXT NOT NULL,
                                secondary_category TEXT,
                                address TEXT NOT NULL,
                                lat REAL NOT NULL,
                                lng REAL NOT NULL,
                                recurring BOOLEAN NOT NULL DEFAULT 0,
                                frequency TEXT,
                                created_by INTEGER,
                                interest_count INTEGER DEFAULT 0,
                                view_count INTEGER DEFAULT 0,
                                fee_required TEXT,
                                event_url TEXT,
                                host_name TEXT,
                                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                                verified BOOLEAN DEFAULT FALSE,
                                FOREIGN KEY(created_by) REFERENCES users(id)
                            )''')
                else:
                    # Migrate existing table
                    if 'time' in columns and 'start_time' not in columns:
                        # Rename time to start_time
                        c.execute('''ALTER TABLE events RENAME COLUMN time TO start_time''')
                        logger.info("Migrated 'time' column to 'start_time'")
                    
                    if 'start_time' not in columns:
                        # Add start_time column with default value from time column if it exists
                        if 'time' in columns:
                            c.execute('''ALTER TABLE events ADD COLUMN start_time TEXT''')
                            c.execute('''UPDATE events SET start_time = time WHERE start_time IS NULL''')
                            logger.info("Added start_time column and migrated from time")
                        else:
                            c.execute('''ALTER TABLE events ADD COLUMN start_time TEXT DEFAULT "12:00"''')
                            logger.info("Added start_time column with default value")
                    
                    if 'end_time' not in columns:
                        c.execute('''ALTER TABLE events ADD COLUMN end_time TEXT''')
                        logger.info("Added end_time column")
                    
                    # Add interest_count and view_count columns if they don't exist
                    if 'interest_count' not in columns:
                        c.execute('''ALTER TABLE events ADD COLUMN interest_count INTEGER DEFAULT 0''')
                        logger.info("Added interest_count column")
                    
                    if 'view_count' not in columns:
                        c.execute('''ALTER TABLE events ADD COLUMN view_count INTEGER DEFAULT 0''')
                        logger.info("Added view_count column")
                    
                    # Add new UX enhancement columns for SQLite
                    if 'fee_required' not in columns:
                        c.execute('''ALTER TABLE events ADD COLUMN fee_required TEXT''')
                        logger.info("Added fee_required column")
                    
                    if 'event_url' not in columns:
                        c.execute('''ALTER TABLE events ADD COLUMN event_url TEXT''')
                        logger.info("Added event_url column")
                    
                    if 'host_name' not in columns:
                        c.execute('''ALTER TABLE events ADD COLUMN host_name TEXT''')
                        logger.info("Added host_name column")
                    
                    # Add secondary_category column if it doesn't exist  
                    if 'secondary_category' not in columns:
                        c.execute('''ALTER TABLE events ADD COLUMN secondary_category TEXT''')
                        logger.info("Added secondary_category column")
                    
                    # Add verified column if it doesn't exist
                    if 'verified' not in columns:
                        c.execute('''ALTER TABLE events ADD COLUMN verified BOOLEAN DEFAULT FALSE''')
                        logger.info("Added verified column")
                
                c.execute('''CREATE TABLE IF NOT EXISTS activity_logs (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            user_id INTEGER,
                            action TEXT NOT NULL,
                            details TEXT,
                            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                            FOREIGN KEY(user_id) REFERENCES users(id)
                        )''')
                
                # Create interest tracking table
                c.execute('''CREATE TABLE IF NOT EXISTS event_interests (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            event_id INTEGER NOT NULL,
                            user_id INTEGER,
                            browser_fingerprint TEXT NOT NULL DEFAULT 'legacy',
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            UNIQUE(event_id, user_id, browser_fingerprint),
                            FOREIGN KEY(event_id) REFERENCES events(id) ON DELETE CASCADE,
                            FOREIGN KEY(user_id) REFERENCES users(id) ON DELETE SET NULL
                        )''')
                
                # Create view tracking table
                c.execute('''CREATE TABLE IF NOT EXISTS event_views (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            event_id INTEGER NOT NULL,
                            user_id INTEGER,
                            browser_fingerprint TEXT NOT NULL DEFAULT 'legacy',
                            viewed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            UNIQUE(event_id, user_id, browser_fingerprint),
                            FOREIGN KEY(event_id) REFERENCES events(id) ON DELETE CASCADE,
                            FOREIGN KEY(user_id) REFERENCES users(id) ON DELETE SET NULL
                        )''')
                
                # Create page visits tracking table
                c.execute('''CREATE TABLE IF NOT EXISTS page_visits (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            page_type TEXT NOT NULL,
                            page_path TEXT NOT NULL,
                            user_id INTEGER,
                            browser_fingerprint TEXT NOT NULL DEFAULT 'anonymous',
                            visited_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            FOREIGN KEY(user_id) REFERENCES users(id) ON DELETE SET NULL
                        )''')

                # Create privacy requests table for CCPA compliance
                c.execute('''CREATE TABLE IF NOT EXISTS privacy_requests (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            request_type TEXT NOT NULL CHECK (request_type IN ('access', 'delete', 'opt_out')),
                            email TEXT NOT NULL,
                            full_name TEXT,
                            verification_info TEXT,
                            details TEXT,
                            status TEXT DEFAULT 'pending' CHECK (status IN ('pending', 'in_progress', 'completed', 'denied')),
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            completed_at TIMESTAMP,
                            admin_notes TEXT
                        )''')

                # Create event reports table for reporting functionality
                c.execute('''CREATE TABLE IF NOT EXISTS event_reports (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            event_id INTEGER NOT NULL,
                            event_title TEXT,
                            event_address TEXT,
                            event_date TEXT,
                            reason TEXT NOT NULL,
                            category TEXT NOT NULL,
                            description TEXT NOT NULL,
                            reporter_email TEXT NOT NULL,
                            reporter_name TEXT,
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            status TEXT DEFAULT 'pending',
                            FOREIGN KEY(event_id) REFERENCES events(id) ON DELETE CASCADE
                        )''')
                
                # SQLite does not have information_schema, so we skip the schema fixes
                # The tracking, privacy, and reporting tables are created correctly above for SQLite
            
            # Ensure password_resets table exists
            if IS_PRODUCTION and DB_URL:
                c.execute('''
                    CREATE TABLE IF NOT EXISTS password_resets (
                        id SERIAL PRIMARY KEY,
                        email TEXT NOT NULL,
                        reset_code TEXT NOT NULL,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        expires_at TIMESTAMP NOT NULL
                    )
                ''')
            else:
                c.execute('''
                    CREATE TABLE IF NOT EXISTS password_resets (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        email TEXT NOT NULL,
                        reset_code TEXT NOT NULL,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        expires_at TIMESTAMP NOT NULL
                    )
                ''')
            
            # Check for default admin user and create if needed
            # create_default_admin_user(conn)  # Moved to after security functions are defined
            
    except Exception as e:
        logger.error(f"Error initializing database: {str(e)}")
        # Don't raise the exception here - this allows the app to start even if DB init fails
        # We'll handle DB errors at the endpoint level

def create_default_admin_user(conn):
    """Create default admin user if none exists"""
    try:
        cursor = conn.cursor()
        
        # Check if any admin users exist
        cursor.execute("SELECT COUNT(*) FROM users WHERE role = 'admin'")
        admin_count = get_count_from_result(cursor.fetchone())
        
        if admin_count == 0:
            logger.info("No admin users found. Creating default admin user...")
            
            # Use a secure random password that must be changed
            import secrets
            import string
            
            # Generate a random 16-character password
            alphabet = string.ascii_letters + string.digits + "!@#$%^&*"
            admin_password = ''.join(secrets.choice(alphabet) for i in range(16))
            
            hashed_password = get_password_hash(admin_password)
            
            cursor.execute(
                "INSERT INTO users (email, hashed_password, role) VALUES (?, ?, ?)",
                ("admin@todo-events.com", hashed_password, "admin")
            )
            conn.commit()
            
            logger.info("‚úÖ Default admin user created:")
            logger.info(f"   üìß Email: admin@todo-events.com")
            logger.info(f"   üîë Password: {admin_password}")
            logger.info("‚ö†Ô∏è IMPORTANT: Change this password after first login!")
            
        else:
            logger.info(f"Found {admin_count} existing admin users. Default admin creation skipped.")
            
    except Exception as e:
        logger.error(f"Error creating default admin user: {str(e)}")
        return False
    
    return True

# Initialize database
try:
    init_db()
except Exception as e:
    logger.error(f"Database initialization failed: {str(e)}")

# =====================================================
# AUTOMATED AI SYNC SYSTEM
# =====================================================

class AutomatedTaskManager:
    """
    Manages automated tasks for AI search optimization:
    - Sitemap generation and updates
    - Event data refresh
    - AI tool synchronization
    - Performance monitoring
    """
    
    def __init__(self):
        self.scheduler = BackgroundScheduler()
        self.last_sitemap_update = None
        self.last_event_refresh = None
        self.task_status = {
            "sitemap_generation": {"status": "pending", "last_run": None, "next_run": None},
            "event_refresh": {"status": "pending", "last_run": None, "next_run": None},
            "ai_sync": {"status": "pending", "last_run": None, "next_run": None}
        }
        
    async def generate_sitemap_automatically(self):
        """Generate sitemap with current event data"""
        try:
            logger.info("üîÑ Starting automated sitemap generation...")
            
            # Get current events from database
            events = await self.get_current_events()
            
            # Generate sitemap content
            sitemap_content = await self.build_sitemap_content(events)
            
            # Save to file (in production, this could be uploaded to S3/CDN)
            await self.save_sitemap(sitemap_content)
            
            # Update task status
            self.task_status["sitemap_generation"]["status"] = "completed"
            self.task_status["sitemap_generation"]["last_run"] = datetime.utcnow().isoformat()
            self.last_sitemap_update = datetime.utcnow()
            
            logger.info("‚úÖ Automated sitemap generation completed successfully")
            
            # Notify search engines about the update
            await self.ping_search_engines()
            
        except Exception as e:
            logger.error(f"‚ùå Automated sitemap generation failed: {str(e)}")
            self.task_status["sitemap_generation"]["status"] = "failed"
    
    async def refresh_event_data(self):
        """Refresh and optimize event data for AI consumption"""
        try:
            logger.info("üîÑ Starting automated event data refresh...")
            
            # Clean up expired events
            await self.cleanup_expired_events()
            
            # Update event search index (if implemented)
            await self.update_search_index()
            
            # Cache popular queries for faster AI responses
            await self.cache_popular_queries()
            
            self.task_status["event_refresh"]["status"] = "completed"
            self.task_status["event_refresh"]["last_run"] = datetime.utcnow().isoformat()
            self.last_event_refresh = datetime.utcnow()
            
            logger.info("‚úÖ Automated event data refresh completed")
            
        except Exception as e:
            logger.error(f"‚ùå Automated event data refresh failed: {str(e)}")
            self.task_status["event_refresh"]["status"] = "failed"
    
    async def sync_with_ai_tools(self):
        """Optimize data specifically for AI tool consumption"""
        try:
            logger.info("ü§ñ Starting AI tools synchronization...")
            
            # Update AI-optimized cache
            await self.update_ai_cache()
            
            # Test AI endpoint responsiveness
            await self.test_ai_endpoint()
            
            # Update structured data if needed
            await self.validate_structured_data()
            
            self.task_status["ai_sync"]["status"] = "completed"
            self.task_status["ai_sync"]["last_run"] = datetime.utcnow().isoformat()
            
            logger.info("‚úÖ AI tools synchronization completed")
            
        except Exception as e:
            logger.error(f"‚ùå AI tools synchronization failed: {str(e)}")
            self.task_status["ai_sync"]["status"] = "failed"
    
    async def get_current_events(self):
        """Get current and upcoming events from database"""
        try:
            with get_db() as conn:
                c = conn.cursor()
                
                # Use proper date comparison for both PostgreSQL and SQLite
                if IS_PRODUCTION and DB_URL:
                    # PostgreSQL - cast date text to date for comparison
                    c.execute("""
                        SELECT id, title, description, date, start_time, end_time, end_date, category, 
                               address, lat, lng, created_at, slug, is_published
                        FROM events 
                        WHERE CAST(date AS DATE) >= (CURRENT_DATE - INTERVAL '30 days')::DATE 
                        AND (is_published = true OR is_published IS NULL)
                        ORDER BY CAST(date AS DATE), start_time
                    """)
                else:
                    # SQLite
                    c.execute("""
                        SELECT id, title, description, date, start_time, end_time, end_date, category, 
                               address, lat, lng, created_at, slug, is_published
                        FROM events 
                        WHERE date::date >= CURRENT_DATE AND (is_published = 1 OR is_published IS NULL)
                        ORDER BY date, start_time
                    """)
                return [dict(row) for row in c.fetchall()]
        except Exception as e:
            logger.error(f"Error fetching events for sitemap: {str(e)}")
            return []
    
    async def build_sitemap_content(self, events):
        """Build sitemap XML content with current events"""
        current_date = datetime.utcnow().strftime('%Y-%m-%d')
        domain = "https://todo-events.com"
        
        # Start with static sitemap structure
        sitemap = f'''<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9"
        xmlns:image="http://www.google.com/schemas/sitemap-image/1.1">

  <!-- Homepage - Primary landing page -->
  <url>
    <loc>{domain}/</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>daily</changefreq>
    <priority>1.0</priority>
    <image:image>
      <image:loc>{domain}/images/pin-logo.svg</image:loc>
      <image:caption>todo-events logo - Local event discovery platform</image:caption>
    </image:image>
  </url>

  <!-- Main navigation pages -->
  <url>
    <loc>{domain}/hosts</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>weekly</changefreq>
    <priority>0.8</priority>
  </url>
  <url>
    <loc>{domain}/creators</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>weekly</changefreq>
    <priority>0.8</priority>
  </url>'''
        
        # Add category pages - both query param and SEO-friendly formats
        categories = [
    'food-drink', 'music', 'arts', 'sports', 'automotive', 'airshows', 'vehicle-sports', 
    'community', 'religious', 'education', 'veteran', 'cookout', 'networking',
    'fair-festival', 'diving', 'shopping', 'health', 'outdoors', 'photography', 'family', 
    'gaming', 'real-estate', 'adventure', 'seasonal', 'other'
]
        sitemap += f'''

  <!-- Category pages -->'''
        
        for category in categories:
            sitemap += f'''
  <url>
    <loc>{domain}/events/{category}</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>daily</changefreq>
    <priority>0.8</priority>
  </url>'''
        
        # Add individual event pages with multiple URL formats
        sitemap += f'''

  <!-- Individual Event Pages -->'''
        
        for event in events[:100]:  # Limit to avoid huge sitemaps
            # Get dynamic lastmod date from event
            event_lastmod = current_date
            if event.get('updated_at'):
                try:
                    parsed_date = datetime.fromisoformat(str(event['updated_at']).replace('Z', '+00:00'))
                    event_lastmod = parsed_date.strftime('%Y-%m-%d')
                except:
                    pass
            elif event.get('created_at'):
                try:
                    parsed_date = datetime.fromisoformat(str(event['created_at']).replace('Z', '+00:00'))
                    event_lastmod = parsed_date.strftime('%Y-%m-%d')
                except:
                    pass
            
            # Check if event has a slug for SEO-friendly URL
            if event.get('slug'):
                event_slug = event['slug']
                
                # Generate canonical URL based on location data
                if event.get('city') and event.get('state'):
                    # Location-based URL: /us/state/city/events/slug
                    state_lower = event.get('state', '').lower()

                    city_slug = slugify(event.get('city', ''))
                    canonical_url = f"{domain}/us/{state_lower}/{city_slug}/events/{event_slug}"
                    priority = "0.9"  # Higher priority for geo-specific URLs
                else:
                    # Fallback URL: /events/slug
                    canonical_url = f"{domain}/events/{event_slug}"
                    priority = "0.85"
                
                sitemap += f'''
  <url>
    <loc>{canonical_url}</loc>
    <lastmod>{event_lastmod}</lastmod>
    <changefreq>monthly</changefreq>
    <priority>{priority}</priority>
  </url>'''
                
                # Add short URL for compatibility
                sitemap += f'''
  <url>
    <loc>{domain}/e/{event_slug}</loc>
    <lastmod>{event_lastmod}</lastmod>
    <changefreq>monthly</changefreq>
    <priority>0.7</priority>
  </url>'''
                
                # Add date-indexed URL if we have event date
                try:
                    event_date_obj = datetime.fromisoformat(str(event.get('date', current_date)))
                    year = event_date_obj.strftime('%Y')
                    month = event_date_obj.strftime('%m')
                    day = event_date_obj.strftime('%d')
                    
                    sitemap += f'''
  <url>
    <loc>{domain}/events/{year}/{month}/{day}/{event_slug}</loc>
    <lastmod>{event_lastmod}</lastmod>
    <changefreq>monthly</changefreq>
    <priority>0.75</priority>
  </url>'''
                except:
                    pass  # Skip date-indexed URL if date parsing fails
        
        # Add time-based event discovery pages
        sitemap += f'''

  <!-- Time-based Event Discovery -->
  <url>
    <loc>{domain}/events-today</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>hourly</changefreq>
    <priority>0.9</priority>
  </url>
  <url>
    <loc>{domain}/events-tonight</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>hourly</changefreq>
    <priority>0.8</priority>
  </url>
  <url>
    <loc>{domain}/events-this-weekend</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>daily</changefreq>
    <priority>0.9</priority>
  </url>
  <url>
    <loc>{domain}/events-tomorrow</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>daily</changefreq>
    <priority>0.8</priority>
  </url>
  <url>
    <loc>{domain}/events-this-week</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>daily</changefreq>
    <priority>0.8</priority>
  </url>'''

        # Add location-based discovery pages for major cities
        major_cities = [
            'new-york', 'los-angeles', 'chicago', 'houston', 'phoenix', 'philadelphia',
            'san-antonio', 'san-diego', 'dallas', 'san-jose', 'austin', 'jacksonville',
            'fort-worth', 'columbus', 'charlotte', 'san-francisco', 'indianapolis',
            'seattle', 'denver', 'washington', 'boston', 'nashville', 'detroit',
            'portland', 'memphis', 'las-vegas', 'miami', 'atlanta', 'milwaukee'
        ]

        sitemap += f'''

  <!-- "This Weekend in [City]" Pages -->'''
        
        for city in major_cities[:25]:  # Limit to top 25 cities
            sitemap += f'''
  <url>
    <loc>{domain}/this-weekend-in-{city}</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>daily</changefreq>
    <priority>0.75</priority>
  </url>'''

        sitemap += f'''

  <!-- "Free Events in [City]" Pages -->'''
        
        for city in major_cities[:25]:  # Limit to top 25 cities
            sitemap += f'''
  <url>
    <loc>{domain}/free-events-in-{city}</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>daily</changefreq>
    <priority>0.7</priority>
  </url>'''

        sitemap += f'''

  <!-- "Today in [City]" Pages -->'''
        
        for city in major_cities[:15]:  # Limit to top 15 cities for today
            sitemap += f'''
  <url>
    <loc>{domain}/today-in-{city}</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>hourly</changefreq>
    <priority>0.8</priority>
  </url>'''

        sitemap += f'''

  <!-- "Tonight in [City]" Pages -->'''
        
        for city in major_cities[:15]:  # Limit to top 15 cities for tonight
            sitemap += f'''
  <url>
    <loc>{domain}/tonight-in-{city}</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>hourly</changefreq>
    <priority>0.75</priority>
  </url>'''

        # Add general discovery and AI-optimized pages
        sitemap += f'''

  <!-- General Discovery Pages -->
  <url>
    <loc>{domain}/local-events-near-me</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>daily</changefreq>
    <priority>0.9</priority>
  </url>
  <url>
    <loc>{domain}/near-me</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>daily</changefreq>
    <priority>0.8</priority>
  </url>
  <url>
    <loc>{domain}/free-events-near-me</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>daily</changefreq>
    <priority>0.8</priority>
  </url>
  <url>
    <loc>{domain}/live-music-near-me</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>daily</changefreq>
    <priority>0.7</priority>
  </url>
  <url>
    <loc>{domain}/food-festivals-near-me</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>weekly</changefreq>
    <priority>0.7</priority>
  </url>
  <url>
    <loc>{domain}/art-events-near-me</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>weekly</changefreq>
    <priority>0.7</priority>
  </url>
  <url>
    <loc>{domain}/outdoor-events</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>daily</changefreq>
    <priority>0.7</priority>
  </url>
  <url>
    <loc>{domain}/family-friendly-events</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>daily</changefreq>
    <priority>0.7</priority>
  </url>
  <url>
    <loc>{domain}/discover</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>weekly</changefreq>
    <priority>0.6</priority>
  </url>

  <!-- Static Pages -->
  <url>
    <loc>{domain}/about</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>monthly</changefreq>
    <priority>0.6</priority>
  </url>
  <url>
    <loc>{domain}/how-it-works</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>monthly</changefreq>
    <priority>0.6</priority>
  </url>
  <url>
    <loc>{domain}/create-event</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>weekly</changefreq>
    <priority>0.7</priority>
  </url>
  <url>
    <loc>{domain}/contact</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>monthly</changefreq>
    <priority>0.5</priority>
  </url>
  <url>
    <loc>{domain}/privacy</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>yearly</changefreq>
    <priority>0.3</priority>
  </url>
  <url>
    <loc>{domain}/terms</loc>
    <lastmod>{current_date}</lastmod>
    <changefreq>yearly</changefreq>
    <priority>0.3</priority>
  </url>

</urlset>

<!-- Note: This sitemap was automatically generated on {current_date} -->
<!-- Contains {len(events)} individual events and comprehensive SEO URLs -->
'''
        
        return sitemap
    
    async def save_sitemap(self, content):
        """Save sitemap content to file"""
        # In production, you might want to save to a shared storage like S3
        # For now, we'll create an endpoint to serve the dynamic sitemap
        self.current_sitemap = content
        logger.info(f"Sitemap updated with {content.count('<url>')} URLs")
    
    async def ping_search_engines(self):
        """Notify search engines about sitemap updates"""
        urls = [
            "https://www.google.com/ping?sitemap=https://todo-events.com/sitemap.xml",
            "https://www.bing.com/ping?sitemap=https://todo-events.com/sitemap.xml"
        ]
        
        async with httpx.AsyncClient() as client:
            for url in urls:
                try:
                    response = await client.get(url, timeout=10)
                    logger.info(f"‚úÖ Pinged search engine: {url} - Status: {response.status_code}")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Failed to ping search engine: {url} - Error: {str(e)}")
    
    async def cleanup_expired_events(self):
        """Enhanced cleanup of expired events with 32-day archive policy"""
        try:
            logger.info("üßπ Starting enhanced automated event cleanup...")
            
            with get_db() as conn:
                cursor = conn.cursor()
                placeholder = get_placeholder()
                
                # Get current datetime and 32-day archive cutoff with full timestamp precision
                current_datetime = datetime.utcnow()
                current_date = current_datetime.date()
                archive_cutoff_datetime = current_datetime - timedelta(days=32)  # 32-day archive
                archive_cutoff = archive_cutoff_datetime.date()
                
                logger.info(f"Current datetime: {current_datetime.isoformat()}")
                logger.info(f"Current date: {current_date}")
                logger.info(f"Archive cutoff datetime: {archive_cutoff_datetime.isoformat()}")
                logger.info(f"Archive cutoff date: {archive_cutoff} (events before this will be deleted)")
                
                # First, get expired events for logging
                if placeholder == "?":
                    cursor.execute("""
                        SELECT id, title, date, created_at 
                        FROM events 
                        WHERE date < ? 
                        ORDER BY date DESC
                    """, (str(archive_cutoff),))
                else:
                    cursor.execute("""
                        SELECT id, title, date, created_at 
                        FROM events 
                        WHERE date < %s 
                        ORDER BY date DESC
                    """, (str(archive_cutoff),))
                
                expired_events = cursor.fetchall()
                
                if not expired_events:
                    logger.info("‚úÖ No expired events found beyond 32-day archive period")
                    return
                
                logger.info(f"üìä Found {len(expired_events)} events beyond 32-day archive:")
                for event in expired_events[:5]:  # Log first 5
                    if isinstance(event, dict):
                        event_data = event
                    else:
                        event_data = {
                            'id': event[0], 'title': event[1], 
                            'date': event[2], 'created_at': event[3]
                        }
                    logger.info(f"  - ID {event_data['id']}: '{event_data['title']}' (Date: {event_data['date']})")
                
                if len(expired_events) > 5:
                    logger.info(f"  ... and {len(expired_events) - 5} more")
                
                # Delete expired events and related data
                expired_ids = [event[0] if not isinstance(event, dict) else event['id'] for event in expired_events]
                
                # Delete in transaction for data integrity
                cursor.execute("BEGIN")
                
                try:
                    # Delete related data first (to avoid foreign key issues)
                    for event_id in expired_ids:
                        # Delete event interests
                        if placeholder == "?":
                            cursor.execute("DELETE FROM event_interests WHERE event_id = ?", (event_id,))
                            cursor.execute("DELETE FROM event_views WHERE event_id = ?", (event_id,))
                        else:
                            cursor.execute("DELETE FROM event_interests WHERE event_id = %s", (event_id,))
                            cursor.execute("DELETE FROM event_views WHERE event_id = %s", (event_id,))
                    
                    # Delete the events themselves
                    if placeholder == "?":
                        placeholders = ",".join(["?" for _ in expired_ids])
                        cursor.execute(f"DELETE FROM events WHERE id IN ({placeholders})", expired_ids)
                    else:
                        placeholders = ",".join(["%s" for _ in expired_ids])
                        cursor.execute(f"DELETE FROM events WHERE id IN ({placeholders})", expired_ids)
                    
                    # Commit the transaction
                    cursor.execute("COMMIT")
                    
                    # Clear caches
                    event_cache.clear()
                    
                    logger.info(f"‚úÖ Successfully cleaned up {len(expired_events)} expired events")
                    logger.info(f"üßπ Cleared event cache after cleanup")
                    
                    # Log archive policy  
                    placeholder = get_placeholder()
                    cursor.execute(f"""
                        SELECT COUNT(*) 
                        FROM events 
                        WHERE date >= {placeholder} AND date < {placeholder}
                    """, (str(archive_cutoff), str(current_date)))
                    
                    archived_count = get_count_from_result(cursor.fetchone())
                    logger.info(f"üìÅ {archived_count} events in 32-day archive (from {archive_cutoff} to {current_date})")
                    
                except Exception as cleanup_error:
                    cursor.execute("ROLLBACK")
                    logger.error(f"‚ùå Event cleanup failed, transaction rolled back: {cleanup_error}")
                    raise
                
        except Exception as e:
            logger.error(f"‚ùå Event cleanup automation error: {e}")

    async def update_search_index(self):
        """Update search index after cleanup"""
        try:
            logger.info("üîç Updating search index after event cleanup...")
            # Trigger sitemap regeneration to exclude deleted events
            await self.generate_sitemap_automatically()
            logger.info("‚úÖ Search index updated successfully")
        except Exception as e:
            logger.error(f"‚ùå Search index update error: {e}")
    
    async def cache_popular_queries(self):
        """Cache responses for popular AI queries"""
        popular_locations = [
            (40.7128, -74.0060),  # New York
            (34.0522, -118.2437), # Los Angeles
            (41.8781, -87.6298),  # Chicago
        ]
        
        # Pre-cache popular query responses
        for lat, lng in popular_locations:
            try:
                # This would call our AI endpoint internally to warm the cache
                pass
            except Exception as e:
                logger.warning(f"Failed to cache location {lat}, {lng}: {str(e)}")
    
    async def update_ai_cache(self):
        """Update cached data for AI consumption"""
        logger.info("ü§ñ AI cache updated")
    
    async def test_ai_endpoint(self):
        """Test AI endpoint responsiveness"""
        try:
            # Test our own AI endpoint
            async with httpx.AsyncClient() as client:
                response = await client.get(
                    f"http://localhost:{os.getenv('PORT', 8000)}/api/v1/local-events?limit=1",
                    timeout=5
                )
                if response.status_code == 200:
                    logger.info("‚úÖ AI endpoint test passed")
                else:
                    logger.warning(f"‚ö†Ô∏è AI endpoint test failed: {response.status_code}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è AI endpoint test error: {str(e)}")
    
    async def validate_structured_data(self):
        """Validate structured data markup"""
        logger.info("üìã Structured data validation completed")
    
    def start_scheduler(self):
        """Start the background scheduler"""
        try:
            # Schedule tasks every 6 hours (aligned with AI tool update patterns)
            
            # Create wrapper functions for async tasks
            def run_sitemap_generation():
                """Synchronous wrapper for sitemap generation"""
                try:
                    # Create new event loop if none exists
                    try:
                        loop = asyncio.get_event_loop()
                    except RuntimeError:
                        loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(loop)
                    
                    # Run the async function
                    loop.run_until_complete(self.generate_sitemap_automatically())
                except Exception as e:
                    logger.error(f"Error in sitemap generation wrapper: {str(e)}")

            def run_event_refresh():
                """Synchronous wrapper for event data refresh"""
                try:
                    # Create new event loop if none exists
                    try:
                        loop = asyncio.get_event_loop()
                    except RuntimeError:
                        loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(loop)
                    
                    # Run the async function
                    loop.run_until_complete(self.refresh_event_data())
                except Exception as e:
                    logger.error(f"Error in event refresh wrapper: {str(e)}")

            def run_ai_sync():
                """Synchronous wrapper for AI sync"""
                try:
                    # Create new event loop if none exists
                    try:
                        loop = asyncio.get_event_loop()
                    except RuntimeError:
                        loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(loop)
                    
                    # Run the async function
                    loop.run_until_complete(self.sync_with_ai_tools())
                except Exception as e:
                    logger.error(f"Error in AI sync wrapper: {str(e)}")
            
            # Sitemap generation - every 6 hours
            self.scheduler.add_job(
                func=run_sitemap_generation,
                trigger=IntervalTrigger(hours=6),
                id='sitemap_generation',
                name='Automated Sitemap Generation',
                replace_existing=True
            )
            
            # Event data refresh - every 6 hours (offset by 2 hours)
            self.scheduler.add_job(
                func=run_event_refresh,
                trigger=IntervalTrigger(hours=6, start_date=datetime.utcnow() + timedelta(hours=2)),
                id='event_refresh',
                name='Event Data Refresh',
                replace_existing=True
            )
            
            # AI sync - every 6 hours (offset by 4 hours)
            self.scheduler.add_job(
                func=run_ai_sync,
                trigger=IntervalTrigger(hours=6, start_date=datetime.utcnow() + timedelta(hours=4)),
                id='ai_sync',
                name='AI Tools Synchronization',
                replace_existing=True
            )
            
            # Event cleanup with 32-day archive - every 6 hours (offset by 3 hours)
            def run_event_cleanup():
                try:
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    loop.run_until_complete(self.cleanup_expired_events())
                    loop.close()
                except Exception as e:
                    logger.error(f"Scheduled event cleanup failed: {e}")
            
            self.scheduler.add_job(
                func=run_event_cleanup,
                trigger=IntervalTrigger(hours=6, start_date=datetime.utcnow() + timedelta(hours=3)),
                id='event_cleanup',
                name='Event Cleanup (32-day Archive)',
                replace_existing=True
            )
            
            # Health check - every hour
            self.scheduler.add_job(
                func=self.health_check,
                trigger=IntervalTrigger(hours=1),
                id='health_check',
                name='System Health Check',
                replace_existing=True
            )
            
            self.scheduler.start()
            logger.info("üöÄ Automated task scheduler started successfully")
            logger.info("üìÖ Next sitemap generation: 6 hours")
            logger.info("üìÖ Next event refresh: 8 hours") 
            logger.info("üìÖ Next AI sync: 10 hours")
            logger.info("üìÖ Next event cleanup: 9 hours (32-day archive policy)")
            
        except Exception as e:
            logger.error(f"Failed to start scheduler: {str(e)}")
    
    def health_check(self):
        """Periodic health check for automated tasks"""
        current_time = datetime.utcnow()
        
        # Check if tasks are running on schedule
        for task_name, status in self.task_status.items():
            if status["last_run"]:
                last_run = datetime.fromisoformat(status["last_run"])
                hours_since_last_run = (current_time - last_run).total_seconds() / 3600
                
                if hours_since_last_run > 8:  # Alert if task hasn't run in 8+ hours
                    logger.warning(f"‚ö†Ô∏è Task '{task_name}' hasn't run in {hours_since_last_run:.1f} hours")
        
        logger.info("üíì Automated task health check completed")
    
    def stop_scheduler(self):
        """Stop the background scheduler"""
        if self.scheduler.running:
            self.scheduler.shutdown()
            logger.info("üõë Automated task scheduler stopped")

# Initialize the automated task manager
task_manager = AutomatedTaskManager()

# Start scheduler when in production
if IS_PRODUCTION:
    task_manager.start_scheduler()
    logger.info("ü§ñ AI sync automation enabled for production environment")
else:
    logger.info("üîß AI sync automation disabled in development mode")

# Create default admin user now that all functions are available
try:
    with get_db() as conn:
        create_default_admin_user(conn)
        logger.info("‚úÖ Default admin user initialization completed")
except Exception as e:
    logger.error(f"‚ùå Error creating default admin user during startup: {str(e)}")

# Password Validation System
class PasswordValidator:
    """
    Comprehensive password validation system with detailed feedback
    """
    @staticmethod
    def validate_password(password: str) -> dict:
        """
        Validate password and return detailed feedback
        
        Returns a dictionary with:
        - is_valid: Boolean indicating if password meets all criteria
        - feedback: List of specific validation messages
        - strength: Strength rating (weak/medium/strong)
        """
        # Initialize validation results
        validation_result = {
            "is_valid": True,
            "feedback": [],
            "strength": "weak"
        }
        
        # Check length - reduced requirement
        if len(password) < 6:
            validation_result["is_valid"] = False
            validation_result["feedback"].append(
                "Password must be at least 6 characters long"
            )
        
        # Count character types present
        character_types = 0
        character_type_messages = []
        
        # Check for uppercase letters
        if re.search(r'[A-Z]', password):
            character_types += 1
        else:
            character_type_messages.append("uppercase letter")
        
        # Check for lowercase letters
        if re.search(r'[a-z]', password):
            character_types += 1
        else:
            character_type_messages.append("lowercase letter")
        
        # Check for numbers
        if re.search(r'\d', password):
            character_types += 1
        else:
            character_type_messages.append("number")
        
        # Check for special characters
        if re.search(r'[!@#$%^&*(),.?":{}|<>]', password):
            character_types += 1
        else:
            character_type_messages.append("special character (!@#$%^&*(),.?\":{}|<>)")
        
        # Require at least 3 out of 4 character types (more flexible)
        if character_types < 3:
            validation_result["is_valid"] = False
            missing_types = 4 - character_types
            validation_result["feedback"].append(
                f"Password must contain at least 3 different character types. "
                f"Missing {missing_types} types from: {', '.join(character_type_messages)}"
            )
        
        # Check for common weak passwords
        common_weak_passwords = [
            "password", "123456", "qwerty", "admin", 
            "letmein", "welcome", "monkey", "abc123"
        ]
        if password.lower() in common_weak_passwords:
            validation_result["is_valid"] = False
            validation_result["feedback"].append(
                "Password is too common and easily guessable"
            )
        
        # Determine strength based on length and character variety
        strength_criteria = 0
        if len(password) >= 8:
            strength_criteria += 1
        if len(password) >= 12:
            strength_criteria += 1
        strength_criteria += character_types
        
        if strength_criteria <= 2:
            validation_result["strength"] = "weak"
        elif strength_criteria <= 4:
            validation_result["strength"] = "medium"
        else:
            validation_result["strength"] = "strong"
        
        return validation_result
# Pydantic Models
class EventBase(BaseModel):
    title: str
    description: str
    short_description: Optional[str] = None
    date: str
    start_time: str
    end_time: Optional[str] = None
    end_date: Optional[str] = None
    category: str
    secondary_category: Optional[str] = None
    address: str
    city: Optional[str] = None
    state: Optional[str] = None
    country: Optional[str] = "USA"
    lat: float
    lng: float
    recurring: bool = False
    frequency: Optional[str] = None
    # Pricing fields
    fee_required: Optional[str] = None  # Details about tickets/fees
    price: Optional[float] = 0.0        # Normalized price
    currency: Optional[str] = "USD"     # Currency code
    # Organization fields
    event_url: Optional[str] = None     # External event URL
    host_name: Optional[str] = None     # Organization/host name
    organizer_url: Optional[str] = None # Organizer website
    # Premium features
    verified: Optional[bool] = False    # Event verification status
    # SEO fields
    slug: Optional[str] = None          # URL-friendly slug
    is_published: Optional[bool] = True # Publication status

    @validator('date')
    def validate_date(cls, v):
        try:
            # Try parsing the date to ensure it's in a valid format
            datetime.strptime(v, '%Y-%m-%d')
            return v
        except ValueError:
            raise ValueError('Date must be in YYYY-MM-DD format')

    @validator('start_time')
    def validate_start_time(cls, v):
        try:
            # Try parsing the start_time to ensure it's in a valid format
            datetime.strptime(v, '%H:%M')
            return v
        except ValueError:
            raise ValueError('Start time must be in HH:MM 24-hour format')

    @validator('end_time')
    def validate_end_time(cls, v):
        if v is None or v == "":
            return v
        try:
            # Try parsing the end_time to ensure it's in a valid format
            datetime.strptime(v, '%H:%M')
            return v
        except ValueError:
            raise ValueError('End time must be in HH:MM 24-hour format')

    @validator('end_date')
    def validate_end_date(cls, v):
        if v is None or v == "":
            return v
        try:
            # Try parsing the date to ensure it's in a valid format
            datetime.strptime(v, '%Y-%m-%d')
            return v
        except ValueError:
            raise ValueError('End date must be in YYYY-MM-DD format')

    @validator('fee_required')
    def validate_fee_required(cls, v):
        if v is None:
            return ""  # Convert None to empty string
        if v == "":
            return v
        # Limit fee description to reasonable length
        if len(v) > 500:
            raise ValueError('Fee description must be 500 characters or less')
        return v.strip()

    @validator('event_url')
    def validate_event_url(cls, v):
        if v is None:
            return ""  # Convert None to empty string
        
        # Strip whitespace and check if empty
        v_stripped = v.strip() if isinstance(v, str) else ""
        if v_stripped == "":
            return ""  # Return empty string for None, empty, or whitespace-only values
            
        # Basic URL validation
        if not v_stripped.startswith(('http://', 'https://')):
            v_stripped = 'https://' + v_stripped  # Add https if missing
        
        # Simple URL pattern check
        import re
        url_pattern = re.compile(
            r'^https?://'  # http:// or https://
            r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\.)+[A-Z]{2,6}\.?|'  # domain...
            r'localhost|'  # localhost...
            r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3})'  # ...or ip
            r'(?::\d+)?'  # optional port
            r'(?:/?|[/?]\S+)$', re.IGNORECASE)
        
        if not url_pattern.match(v_stripped):
            raise ValueError('Please enter a valid URL')
        
        # Limit URL length
        if len(v_stripped) > 2000:
            raise ValueError('URL must be 2000 characters or less')
        
        return v_stripped

    @validator('host_name')
    def validate_host_name(cls, v):
        if v is None:
            return ""  # Convert None to empty string
        if v == "":
            return v
        # Limit host name to reasonable length
        if len(v) > 200:
            raise ValueError('Host name must be 200 characters or less')
        return v.strip()

    @validator('address')
    def validate_address(cls, v):
        if not v or not isinstance(v, str):
            raise ValueError('Address is required')
        
        trimmed_address = v.strip()
        if not trimmed_address:
            raise ValueError('Address cannot be empty')
        
        return trimmed_address
    
    @validator('secondary_category')
    def validate_secondary_category(cls, v):
        if v is None or v == "":
            return None  # Allow empty/null secondary category
        
        # Use the same categories list as the main category
        categories = [
            'food-drink', 'music', 'arts', 'sports', 'automotive', 'airshows', 'vehicle-sports', 
            'community', 'religious', 'education', 'veteran', 'cookout', 'networking',
            'fair-festival', 'diving', 'shopping', 'health', 'outdoors', 'photography', 'family', 
            'gaming', 'real-estate', 'adventure', 'seasonal', 'agriculture', 'other'
        ]
        
        if v not in categories:
            raise ValueError(f'Secondary category must be one of: {", ".join(categories)}')
        
        return v

class EventCreate(EventBase):
    pass

class EventResponse(EventBase):
    id: int
    created_by: int
    created_at: str
    updated_at: Optional[str] = None
    start_datetime: Optional[str] = None
    end_datetime: Optional[str] = None
    interest_count: Optional[int] = 0
    view_count: Optional[int] = 0

    class Config:
        from_attributes = True

class UserBase(BaseModel):
    email: EmailStr

class UserCreate(UserBase):
    password: str
    role: UserRole = UserRole.USER

    @validator('password')
    def validate_password(cls, v):
        # Use the PasswordValidator to get validation results
        validation = PasswordValidator.validate_password(v)
        
        # If not valid, raise a validation error with all feedback
        if not validation['is_valid']:
            raise ValueError('\n'.join(validation['feedback']))
        
        return v

class UserResponse(UserBase):
    id: int
    role: UserRole

    class Config:
        from_attributes = True

class Token(BaseModel):
    access_token: str
    token_type: str

# Security Functions
def verify_password(plain_password, hashed_password):
    return pwd_context.verify(plain_password, hashed_password)

def get_password_hash(password):
    return pwd_context.hash(password)

def create_access_token(data: dict):
    to_encode = data.copy()
    expire = datetime.utcnow() + timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    to_encode.update({"exp": expire})
    return jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)

async def get_current_user(token: str = Depends(oauth2_scheme)):
    credentials_exception = HTTPException(
        status_code=status.HTTP_401_UNAUTHORIZED,
        detail="Could not validate credentials",
        headers={"WWW-Authenticate": "Bearer"},
    )
    
    if not token:
        raise credentials_exception
        
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        email: str = payload.get("sub")
        if email is None:
            raise credentials_exception
    except jwt.PyJWTError:
        raise credentials_exception
    
    try:
        with get_db() as conn:
            c = conn.cursor()
            placeholder = get_placeholder()
            c.execute(f"SELECT id, email, role FROM users WHERE email = {placeholder}", (email,))
            user = c.fetchone()
            
            if not user:
                raise credentials_exception
                
            return {
                "id": user[0] if isinstance(user, (tuple, list)) else user["id"],
                "email": user[1] if isinstance(user, (tuple, list)) else user["email"],
                "role": user[2] if isinstance(user, (tuple, list)) else user["role"]
            }
            
    except Exception as e:
        error_msg = str(e)
        logger.error(f"Database error during login user lookup: {error_msg}")
        
        if "timeout" in error_msg.lower() or "deadlock" in error_msg.lower():
            raise HTTPException(
                status_code=408, 
                detail="Login request timed out. Please try again."
            )
        else:
            raise HTTPException(
                status_code=500,
                detail="Internal server error during login",
                headers={"WWW-Authenticate": "Bearer"},
            )

async def get_current_user_optional(token: str = Depends(oauth2_scheme)):
    """Optional user dependency - returns None if not authenticated"""
    try:
        return await get_current_user(token)
    except HTTPException:
        return None

async def get_current_user_optional_no_exception(authorization: str = Header(None)):
    """Optional user dependency that doesn't raise exceptions"""
    if not authorization or not authorization.startswith("Bearer "):
        return None
    
    token = authorization.split(" ")[1]
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        email: str = payload.get("sub")
        if email is None:
            return None
    except jwt.PyJWTError:
        return None
    
    try:
        with get_db() as conn:
            c = conn.cursor()
            placeholder = get_placeholder()
            c.execute(f"SELECT id, email, role FROM users WHERE email = {placeholder}", (email,))
            user = c.fetchone()
            
            if not user:
                return None
                
            return {
                "id": user[0] if isinstance(user, (tuple, list)) else user["id"],
                "email": user[1] if isinstance(user, (tuple, list)) else user["email"],
                "role": user[2] if isinstance(user, (tuple, list)) else user["role"]
            }
            
    except Exception as e:
        logger.error(f"Error in optional user auth: {str(e)}")
        return None

# Authentication Endpoints
@app.post("/token")
async def login(form_data: OAuth2PasswordRequestForm = Depends()):
    placeholder = get_placeholder()
    logger.info(f"Login attempt for user: {form_data.username}")
    
    try:
        with get_db() as conn:
            c = conn.cursor()
            
            # First check connection health with a simple query
            try:
                c.execute("SELECT 1")
            except Exception as e:
                logger.error(f"Database health check failed during login: {str(e)}")
                raise HTTPException(
                    status_code=503, 
                    detail="Database connection issues. Please try again later."
                )
            
            # Find user by email - with timeout handling
            user = None
            try:
                c.execute(f"SELECT * FROM users WHERE email = {placeholder}", (form_data.username,))
                user = c.fetchone()
                
                if not user:
                    logger.warning(f"Login failed for user: {form_data.username} - User not found")
                    raise HTTPException(
                        status_code=status.HTTP_401_UNAUTHORIZED,
                        detail="Incorrect email or password",
                        headers={"WWW-Authenticate": "Bearer"},
                    )
                
                if not verify_password(form_data.password, user["hashed_password"]):
                    logger.warning(f"Login failed for user: {form_data.username} - Invalid credentials")
                    raise HTTPException(
                        status_code=status.HTTP_401_UNAUTHORIZED,
                        detail="Incorrect email or password",
                        headers={"WWW-Authenticate": "Bearer"},
                    )
            except HTTPException:
                raise
            except Exception as e:
                error_msg = str(e)
                logger.error(f"Database error during login user lookup: {error_msg}")
                
                if "timeout" in error_msg.lower() or "deadlock" in error_msg.lower():
                    raise HTTPException(
                        status_code=408, 
                        detail="Login request timed out. Please try again."
                    )
                else:
                    raise HTTPException(
                        status_code=500,
                        detail="Internal server error during login",
                        headers={"WWW-Authenticate": "Bearer"},
                    )
            
            # Generate access token
            if user:  # Extra safety check
                try:
                    logger.info(f"Generating access token for user: {form_data.username}")
                    access_token = create_access_token(data={"sub": user["email"]})
                    logger.info(f"Login successful for user: {form_data.username}")
                    return {"access_token": access_token, "token_type": "bearer"}
                except Exception as e:
                    logger.error(f"Error generating access token: {str(e)}")
                    raise HTTPException(
                        status_code=500,
                        detail="Error generating authentication token",
                        headers={"WWW-Authenticate": "Bearer"},
                    )
            else:
                # This should not happen due to the checks above, but as a fallback
                raise HTTPException(
                    status_code=status.HTTP_401_UNAUTHORIZED,
                    detail="Authentication failed",
                    headers={"WWW-Authenticate": "Bearer"},
                )
                
    except HTTPException as http_ex:
        # Pass through HTTP exceptions
        raise http_ex
    except Exception as e:
        error_msg = str(e)
        logger.error(f"Unexpected error during login: {error_msg}")
        
        # Return more specific error messages
        if "timeout" in error_msg.lower():
            raise HTTPException(
                status_code=408, 
                detail="Login request timed out. Please try again later."
            )
        elif "connection" in error_msg.lower():
            raise HTTPException(
                status_code=503, 
                detail="Database connection issues. Please try again later."
            )
        else:
            # Catch all for any other unexpected errors
            raise HTTPException(
                status_code=500,
                detail="Internal server error during login",
                headers={"WWW-Authenticate": "Bearer"},
            )

@app.post("/users", response_model=UserResponse)
async def create_user(user: UserCreate):
    # Validate password (this will raise an error if invalid)
    password_validation = PasswordValidator.validate_password(user.password)
    
    placeholder = get_placeholder()
    
    # Pre-hash password outside the database transaction to reduce transaction time
    hashed_password = get_password_hash(user.password)
    
    # Log the registration attempt
    logger.info(f"Registration attempt for email: {user.email}")
    
    try:
        with get_db() as conn:
            c = conn.cursor()
            
            # First check connection health with a simple query
            try:
                c.execute("SELECT 1")
            except Exception as e:
                logger.error(f"Database health check failed during registration: {str(e)}")
                raise HTTPException(
                    status_code=503, 
                    detail="Database connection issues. Please try again later."
                )
            
            # Check if email exists - with timeout handling
            try:
                c.execute(f"SELECT id FROM users WHERE email = {placeholder}", (user.email,))
                existing_user = c.fetchone()
                if existing_user:
                    logger.info(f"Registration failed: Email already exists - {user.email}")
                    raise HTTPException(status_code=400, detail="Email already registered")
            except Exception as e:
                if "timeout" in str(e).lower() or "deadlock" in str(e).lower():
                    logger.error(f"Timeout checking for existing email: {str(e)}")
                    raise HTTPException(
                        status_code=408, 
                        detail="Registration request timed out. Please try again."
                    )
                raise
            
            # Insert user with optimized query - with timeout handling
            try:
                logger.info(f"Inserting new user into database: {user.email}")
                
                # Use RETURNING id for PostgreSQL, different approach for SQLite
                if IS_PRODUCTION and DB_URL:
                    # PostgreSQL with RETURNING
                    c.execute(
                        f"INSERT INTO users (email, hashed_password, role) VALUES ({placeholder}, {placeholder}, {placeholder}) RETURNING id",
                        (user.email, hashed_password, user.role)
                    )
                    result = c.fetchone()
                    last_id = result['id'] if result else None
                else:
                    # SQLite without RETURNING
                    c.execute(
                        f"INSERT INTO users (email, hashed_password, role) VALUES ({placeholder}, {placeholder}, {placeholder})",
                        (user.email, hashed_password, user.role)
                    )
                    last_id = c.lastrowid
                    
                if not last_id:
                    raise ValueError("Failed to get ID of created user")
                    
                conn.commit()
                logger.info(f"User inserted successfully: {user.email}")
            except Exception as e:
                conn.rollback()
                error_msg = str(e)
                logger.error(f"Error inserting user: {error_msg}")
                
                if "timeout" in error_msg.lower() or "deadlock" in error_msg.lower():
                    raise HTTPException(
                        status_code=408, 
                        detail="Registration request timed out. The server might be busy, please try again."
                    )
                elif "duplicate" in error_msg.lower() or "unique" in error_msg.lower():
                    raise HTTPException(status_code=400, detail="Email already registered")
                else:
                    raise HTTPException(status_code=500, detail="Error creating user")
            
            # Get the created user data
            try:
                    
                logger.info(f"Fetching created user with ID {last_id}")
                c.execute(f"SELECT * FROM users WHERE id = {placeholder}", (last_id,))
                user_data = dict(c.fetchone())
                
                # Return user data along with password strength
                logger.info(f"User registration successful: {user.email}")
                return {
                    **user_data,
                    "password_strength": password_validation['strength']
                }
            except Exception as e:
                logger.error(f"Error retrieving created user: {str(e)}")
                # Even if we can't fetch the user, registration was successful
                return {
                    "id": last_id,
                    "email": user.email,
                    "role": user.role,
                    "password_strength": password_validation['strength']
                }
                
    except HTTPException as http_ex:
        # Pass through HTTP exceptions
        raise http_ex
    except Exception as e:
        error_msg = str(e)
        logger.error(f"Unexpected error creating user: {error_msg}")
        
        # Return more specific error messages
        if "timeout" in error_msg.lower():
            raise HTTPException(
                status_code=408, 
                detail="Registration request timed out. Please try again later."
            )
        elif "connection" in error_msg.lower():
            raise HTTPException(
                status_code=503, 
                detail="Database connection issues. Please try again later."
            )
        else:
            raise HTTPException(status_code=500, detail="Error creating user")


# Password Strength Check Endpoint
@app.post("/password-strength")
async def check_password_strength(password: str):
    """
    Endpoint to check password strength without creating a user
    Useful for real-time password feedback
    """
    return PasswordValidator.validate_password(password)

# Password Reset Models
class PasswordResetRequest(BaseModel):
    email: EmailStr

class PasswordResetVerify(BaseModel):
    email: EmailStr
    reset_code: str

class PasswordReset(BaseModel):
    email: EmailStr
    reset_code: str
    new_password: str
    
    @validator('new_password')
    def validate_password(cls, v):
        # Use the PasswordValidator to get validation results
        validation = PasswordValidator.validate_password(v)
        
        # If not valid, raise a validation error with all feedback
        if not validation['is_valid']:
            raise ValueError('\n'.join(validation['feedback']))
        
        return v

# Password Reset Endpoints
@app.post("/request-password-reset")
async def request_password_reset(request: PasswordResetRequest):
    """Request a password reset code"""
    placeholder = get_placeholder()
    
    try:
        with get_db() as conn:
            c = conn.cursor()
            
            # Check if user exists
            c.execute(f"SELECT * FROM users WHERE email = {placeholder}", (request.email,))
            user = c.fetchone()
            
            if not user:
                # Always return success for security reasons
                # This prevents email enumeration attacks
                return {"detail": "If your email is registered, you will receive a reset code"}
            
            # Generate a 6-digit reset code
            import random
            reset_code = ''.join(random.choices('0123456789', k=6))
            reset_expiry = datetime.utcnow() + timedelta(minutes=30)
            
            # Store the reset code in the database
            # First, create a password_resets table if it doesn't exist
            if IS_PRODUCTION and DB_URL:
                c.execute('''
                    CREATE TABLE IF NOT EXISTS password_resets (
                        id SERIAL PRIMARY KEY,
                        email TEXT NOT NULL,
                        reset_code TEXT NOT NULL,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        expires_at TIMESTAMP NOT NULL
                    )
                ''')
            else:
                c.execute('''
                    CREATE TABLE IF NOT EXISTS password_resets (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        email TEXT NOT NULL,
                        reset_code TEXT NOT NULL,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        expires_at TIMESTAMP NOT NULL
                    )
                ''')
            
            # Delete any existing reset codes for this user
            c.execute(f"DELETE FROM password_resets WHERE email = {placeholder}", (request.email,))
            
            # Insert the new reset code
            c.execute(
                f"INSERT INTO password_resets (email, reset_code, expires_at) VALUES ({placeholder}, {placeholder}, {placeholder})",
                (request.email, reset_code, reset_expiry)
            )
            
            # Send password reset email
            try:
                from email_config import email_service
                
                # Extract user name from email for personalization
                user_name = request.email.split('@')[0].title()
                
                email_sent = email_service.send_password_reset_email(
                    request.email, 
                    reset_code, 
                    user_name
                )
                
                if email_sent:
                    logger.info(f"‚úÖ Password reset email sent to {request.email}")
                else:
                    logger.error(f"‚ùå Failed to send password reset email to {request.email}")
                    
            except Exception as e:
                logger.error(f"‚ùå Email service error: {str(e)}")
            
            # For development, also log the reset code
            if not IS_PRODUCTION:
                logger.info(f"Password reset code for {request.email}: {reset_code}")
                return {
                    "detail": "Reset code generated and email sent",
                    "reset_code": reset_code
                }
            
            return {"detail": "If your email is registered, you will receive a reset code"}
            
    except Exception as e:
        logger.error(f"Error requesting password reset: {str(e)}")
        raise HTTPException(status_code=500, detail="Error requesting password reset")

@app.post("/verify-reset-code")
async def verify_reset_code(request: PasswordResetVerify):
    """Verify a password reset code"""
    placeholder = get_placeholder()
    
    try:
        with get_db() as conn:
            c = conn.cursor()
            
            # Check if reset code exists and is valid
            c.execute(
                f"SELECT * FROM password_resets WHERE email = {placeholder} AND reset_code = {placeholder} AND expires_at > {placeholder}",
                (request.email, request.reset_code, datetime.utcnow())
            )
            reset = c.fetchone()
            
            if not reset:
                raise HTTPException(status_code=400, detail="Invalid or expired reset code")
            
            return {"detail": "Reset code verified"}
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error verifying reset code: {str(e)}")
        raise HTTPException(status_code=500, detail="Error verifying reset code")

@app.post("/reset-password")
async def reset_password(request: PasswordReset):
    """Reset password with a valid reset code"""
    placeholder = get_placeholder()
    
    try:
        with get_db() as conn:
            c = conn.cursor()
            
            # Check if reset code exists and is valid
            c.execute(
                f"SELECT * FROM password_resets WHERE email = {placeholder} AND reset_code = {placeholder} AND expires_at > {placeholder}",
                (request.email, request.reset_code, datetime.utcnow())
            )
            reset = c.fetchone()
            
            if not reset:
                raise HTTPException(status_code=400, detail="Invalid or expired reset code")
            
            # Update the user's password
            hashed_password = get_password_hash(request.new_password)
            c.execute(
                f"UPDATE users SET hashed_password = {placeholder} WHERE email = {placeholder}",
                (hashed_password, request.email)
            )
            
            # Delete all reset codes for this user
            c.execute(f"DELETE FROM password_resets WHERE email = {placeholder}", (request.email,))
            
            return {"detail": "Password has been reset successfully"}
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error resetting password: {str(e)}")
        raise HTTPException(status_code=500, detail="Error resetting password")

# Event Endpoints
@app.get("/events", response_model=List[EventResponse])
async def list_events(
    category: Optional[str] = None,
    date: Optional[str] = None,
    limit: Optional[int] = 500,  # Increased default limit to 500
    offset: Optional[int] = 0,  # Add pagination
    lat: Optional[float] = None,  # Add location filtering
    lng: Optional[float] = None,  # Add location filtering
    radius: Optional[float] = 25.0  # Add radius filtering (miles)
):
    """
    Retrieve events with optional filtering by category and date.
    Open to all users, no authentication required.
    Optimized for performance with pagination, location filtering, and caching.
    """
    placeholder = get_placeholder()
    
    # Validate and limit pagination parameters  
    limit = min(max(limit or 500, 1), 1000)  # Between 1 and 1000
    offset = max(offset or 0, 0)
    
    # Create cache key for this request
    cache_key = f"events:{category or 'all'}:{date or 'all'}:{limit}:{offset}:{lat}:{lng}:{radius}"
    
    # Try to get from cache first (for mobile performance)
    cached_result = event_cache.get(cache_key)
    if cached_result is not None:
        logger.info(f"Returning cached events for key: {cache_key} - {len(cached_result)} events")
        logger.info(f"Cache stats: {event_cache.stats()}")
        return cached_result
    
    try:
        # Use connection manager with optimized query
        with get_db() as conn:
            cursor = conn.cursor()
            
            # Build optimized query with proper indexing hints
            where_conditions = []
            params = []
            
            # Category filter
            if category and category != 'all':
                where_conditions.append(f"category = {placeholder}")
                params.append(category)
            
            # Date filter
            if date:
                where_conditions.append(f"date = {placeholder}")
                params.append(date)
            
            # Location filter (if coordinates provided)
            location_select = ""
            if lat is not None and lng is not None:
                # Add distance calculation for location-based filtering
                location_select = f"""
                    , (6371 * acos(cos(radians({lat})) * cos(radians(lat)) * 
                      cos(radians(lng) - radians({lng})) + sin(radians({lat})) * 
                      sin(radians(lat)))) * 0.621371 as distance_miles
                """
            
            # Construct WHERE clause
            where_clause = ""
            if where_conditions:
                where_clause = "WHERE " + " AND ".join(where_conditions)
            
            # Database-specific date comparison logic
            if IS_PRODUCTION and DB_URL:
                # PostgreSQL syntax
                date_comparison = "date::date >= CURRENT_DATE"
            else:
                # SQLite syntax
                date_comparison = "date >= date('now')"
            
            # Optimized query with specific columns and LIMIT, including ALL fields
            base_query = f"""
                SELECT id, title, description, short_description, date, start_time, end_time, end_date, 
                       category, address, city, state, country, lat, lng, recurring, frequency, created_by, created_at,
                       COALESCE(interest_count, 0) as interest_count,
                       COALESCE(view_count, 0) as view_count,
                       fee_required, price, currency, event_url, host_name, organizer_url, slug, is_published,
                       start_datetime, end_datetime, updated_at, verified
                       {location_select}
                FROM events 
                {where_clause}
                ORDER BY 
                    CASE 
                        WHEN {date_comparison} THEN 0  -- Future/today events first
                        ELSE 1                           -- Past events later  
                    END,
                    date ASC, start_time ASC
                LIMIT {placeholder} OFFSET {placeholder}
            """
            
            params.extend([limit, offset])
            
            # Execute optimized query
            cursor.execute(base_query, params)
            events = cursor.fetchall()
            
            # Process results efficiently
            result = []
            for event in events:
                try:
                    # Convert to dict efficiently
                    if hasattr(event, '_asdict'):
                        event_dict = event._asdict()
                    elif isinstance(event, dict):
                        event_dict = dict(event)
                    else:
                        # Handle tuple/list results with known schema including ALL fields
                        column_names = [
                            'id', 'title', 'description', 'short_description', 'date', 'start_time', 'end_time',
                            'end_date', 'category', 'address', 'city', 'state', 'country', 'lat', 'lng', 'recurring',
                            'frequency', 'created_by', 'created_at', 'interest_count', 'view_count',
                            'fee_required', 'price', 'currency', 'event_url', 'host_name', 'organizer_url', 'slug', 'is_published',
                            'start_datetime', 'end_datetime', 'updated_at', 'verified'
                        ]
                        if location_select:
                            column_names.append('distance_miles')
                        
                        event_dict = dict(zip(column_names, event))
                    
                    # Convert datetime objects and ensure proper field types
                    event_dict = convert_event_datetime_fields(event_dict)
                    
                    # Ensure counters are integers
                    event_dict['interest_count'] = int(event_dict.get('interest_count', 0) or 0)
                    event_dict['view_count'] = int(event_dict.get('view_count', 0) or 0)
                    
                    # Filter by distance if location provided
                    if lat is not None and lng is not None and 'distance_miles' in event_dict:
                        if event_dict['distance_miles'] <= radius:
                            result.append(event_dict)
                    else:
                        result.append(event_dict)
                    
                except Exception as event_error:
                    logger.warning(f"Error processing event {event}: {event_error}")
                    continue
            
            # Cache the result for mobile performance (shorter TTL for real-time updates)
            event_cache.set(cache_key, result)
            logger.info(f"Cached {len(result)} events for key: {cache_key}")
            logger.info(f"Database query returned {len(events)} raw events, processed {len(result)} events")
            logger.info(f"Cache stats: {event_cache.stats()}")
            
            return result
            
    except Exception as e:
        # Handle any other exceptions
        error_msg = str(e)
        logger.error(f"Error retrieving events: {error_msg}")
        
        if "timeout" in error_msg.lower():
            raise HTTPException(status_code=504, detail="Database query timed out")
        elif "connection" in error_msg.lower():
            raise HTTPException(status_code=503, detail="Database connection issues")
        else:
            raise HTTPException(status_code=500, detail="Error retrieving events")

@app.get("/events/{event_id}", response_model=EventResponse)
async def read_event(event_id: int):
    """
    Retrieve a specific event by its ID.
    Open to all users, no authentication required.
    """
    placeholder = get_placeholder()
    try:
        with get_db() as conn:
            c = conn.cursor()
            # Use COALESCE to handle NULL values and include all fields
            c.execute(f"""SELECT id, title, description, short_description, date, start_time, end_time, end_date, 
                         category, address, city, state, country, lat, lng, recurring, frequency, created_by, created_at,
                         COALESCE(interest_count, 0) as interest_count,
                         COALESCE(view_count, 0) as view_count,
                         fee_required, price, currency, event_url, host_name, organizer_url, slug, is_published,
                         start_datetime, end_datetime, updated_at, verified
                         FROM events WHERE id = {placeholder}""", (event_id,))
            event = c.fetchone()
            
            if not event:
                raise HTTPException(status_code=404, detail="Event not found")
            
            # Convert to dict and handle datetime fields
            event_dict = dict(event)
            
            # Convert datetime objects and ensure proper field types
            event_dict = convert_event_datetime_fields(event_dict)
            
            return event_dict
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error retrieving event {event_id}: {str(e)}")
        raise HTTPException(status_code=500, detail="Error retrieving event")

def ensure_unique_slug(cursor, base_slug: str, event_id: int = None) -> str:
    """Ensure slug uniqueness by appending event ID if needed"""
    if not base_slug:
        # Generate a fallback slug if none provided
        import time
        return f"event-{int(time.time())}"
    
    try:
        # Use database-agnostic placeholder
        placeholder = get_placeholder()
        
        # Check if slug already exists
        if event_id:
            # For updates, exclude current event from check
            if placeholder == "?":
                # SQLite style
                cursor.execute("SELECT COUNT(*) FROM events WHERE slug = ? AND id != ?", (base_slug, event_id))
            else:
                # PostgreSQL style (use %s with psycopg2)
                cursor.execute("SELECT COUNT(*) FROM events WHERE slug = %s AND id != %s", (base_slug, event_id))
        else:
            if placeholder == "?":
                # SQLite style
                cursor.execute("SELECT COUNT(*) FROM events WHERE slug = ?", (base_slug,))
            else:
                # PostgreSQL style (use %s with psycopg2)
                cursor.execute("SELECT COUNT(*) FROM events WHERE slug = %s", (base_slug,))
        
        result = cursor.fetchone()
        count = result[0] if result else 0
        
        if count > 0:
            # Slug exists, append a number or event ID
            if event_id:
                return f"{base_slug}-{event_id}"
            else:
                # For new events, append a timestamp-based suffix
                import time
                suffix = str(int(time.time()))[-6:]  # Last 6 digits of timestamp
                return f"{base_slug}-{suffix}"
        else:
            return base_slug
            
    except Exception as e:
        logger.error(f"Error ensuring unique slug for '{base_slug}': {type(e).__name__}: {str(e)}")
        # Fallback: append current timestamp
        import time
        timestamp_suffix = str(int(time.time()))[-6:]
        fallback_slug = f"{base_slug}-{timestamp_suffix}" if base_slug else f"event-{timestamp_suffix}"
        logger.info(f"Using fallback slug: {fallback_slug}")
        return fallback_slug

def auto_populate_seo_fields(event_data: dict) -> dict:
    """Auto-populate SEO fields for new events using enhanced logic"""
    import re
    import unicodedata
    
    def slugify_local(title, city=""):
        """Enhanced slugify function using title and city"""
        if not title:
            return ""
        
        # Combine title and city for better uniqueness
        base = f"{title} {city}".lower() if city else title.lower()
        
        # Normalize Unicode characters
        base = unicodedata.normalize('NFKD', base)
        
        # Remove non-word characters (keep letters, numbers, spaces, hyphens)
        base = re.sub(r'[^\w\s-]', '', base)
        
        # Replace multiple spaces/underscores/hyphens with single hyphen
        base = re.sub(r'[\s_-]+', '-', base)
        
        # Remove leading/trailing hyphens
        return base.strip('-')
    
    def extract_city_state_local(address):
        """Enhanced city/state extraction from address"""
        if not address:
            return None, None
        
        # Strategy A: [City], [STATE], USA format
        match = re.search(r'([^,]+),\s*([A-Z]{2}),?\s*USA?$', address, re.IGNORECASE)
        if match:
            return match.group(1).strip(), match.group(2).upper()
        
        # Strategy B: [City], [STATE] format (without USA)
        match = re.search(r'([^,]+),\s*([A-Z]{2})$', address, re.IGNORECASE)
        if match:
            return match.group(1).strip(), match.group(2).upper()
        
        # Strategy C: Find any 2-letter state code
        match = re.search(r'\b([A-Z]{2})\b', address)
        if match:
            state = match.group(1)
            # Try to find city before state
            city_pattern = r'([^,\d]+)(?:,\s*)?' + re.escape(state)
            city_match = re.search(city_pattern, address, re.IGNORECASE)
            if city_match:
                city = city_match.group(1).strip()
                # Clean up city name
                city = re.sub(r'^\d+\s+', '', city)  # Remove leading numbers
                return city, state
        
        return None, None
    
    def normalize_price_local(fee_required):
        """Enhanced price normalization"""
        if not fee_required or not isinstance(fee_required, str):
            return 0.0
        
        fee_lower = fee_required.lower().strip()
        
        # Check for free indicators
        free_indicators = ['free', 'no charge', 'no cost', 'gratis', 'complimentary', 'n/a', 'none']
        if any(indicator in fee_lower for indicator in free_indicators):
            return 0.0
        
        # Extract numeric values with various currency symbols
        currency_pattern = r'[\$‚Ç¨¬£¬•‚Çπ]?[\s]*([0-9]+(?:[.,][0-9]{1,2})?)'
        match = re.search(currency_pattern, fee_required)
        if match:
            price_str = match.group(1).replace(',', '')
            try:
                return float(price_str)
            except ValueError:
                pass
        
        # Try to find "number dollars/euros/etc" pattern
        word_pattern = r'([0-9]+(?:\.[0-9]{1,2})?)\s*(?:dollars?|euros?|pounds?|bucks?)'
        match = re.search(word_pattern, fee_lower)
        if match:
            try:
                return float(match.group(1))
            except ValueError:
                pass
        
        return 0.0
    
    def build_datetimes_local(date_str, start_time_str, end_time_str, end_date_str=None):
        """Enhanced datetime building with end_time inference"""
        if not date_str or not start_time_str:
            return None, None
        
        try:
            # Build start datetime
            start_dt_str = f"{date_str}T{start_time_str}:00"
            
            # Build end datetime - ensure we always have an end_time
            if not end_time_str:
                # Infer end_time as 2 hours after start_time
                from datetime import datetime, timedelta
                try:
                    start_time_obj = datetime.strptime(start_time_str, "%H:%M")
                    end_time_obj = start_time_obj + timedelta(hours=2)
                    end_time_str = end_time_obj.strftime("%H:%M")
                except:
                    end_time_str = "18:00"  # fallback
            
            # Now build the end datetime string
            if end_date_str:
                end_dt_str = f"{end_date_str}T{end_time_str}:00"
            else:
                end_dt_str = f"{date_str}T{end_time_str}:00"
            
            return start_dt_str, end_dt_str
        except Exception:
            return None, None
    
    def make_short_description_local(description):
        """Enhanced short description generation"""
        if not description:
            return ""
        
        # Remove extra whitespace and normalize
        cleaned = ' '.join(description.split())
        
        # If short enough, return as-is
        if len(cleaned) <= 160:
            return cleaned
        
        # Truncate at word boundary near 157 characters
        truncated = cleaned[:157]
        last_space = truncated.rfind(' ')
        if last_space > 120:  # Only truncate at word if reasonable
            return cleaned[:last_space] + "..."
        else:
            return cleaned[:157] + "..."
    
    # Generate slug
    city = event_data.get('city', '') or ''
    base_slug = slugify_local(event_data.get('title', ''), city)
    event_data['slug'] = base_slug  # Will be made unique later if needed
    
    # Extract city/state from address
    if not event_data.get('city') or not event_data.get('state'):
        city, state = extract_city_state_local(event_data.get('address', ''))
        if city:
            event_data['city'] = city
        if state:
            event_data['state'] = state
    
    # Normalize price
    event_data['price'] = normalize_price_local(event_data.get('fee_required', ''))
    
    # Generate datetime fields
    start_dt, end_dt = build_datetimes_local(
        event_data.get('date', ''),
        event_data.get('start_time', ''),
        event_data.get('end_time', ''),
        event_data.get('end_date')
    )
    event_data['start_datetime'] = start_dt
    event_data['end_datetime'] = end_dt
    
    # Generate short description
    if not event_data.get('short_description'):
        event_data['short_description'] = make_short_description_local(
            event_data.get('description', '')
        )
    
    # Set defaults
    event_data['currency'] = event_data.get('currency', 'USD')
    event_data['country'] = event_data.get('country', 'USA')
    event_data['is_published'] = event_data.get('is_published', True)
    event_data['updated_at'] = datetime.utcnow().isoformat()
    
    return event_data

@app.post("/events", response_model=EventResponse)
async def create_event(event: EventCreate, current_user: dict = Depends(get_current_user)):
    """
    Create a new event. Requires user authentication.
    Auto-populates SEO fields using enhanced migration logic.
    """
    placeholder = get_placeholder()
    
    # Convert Pydantic model to dict for processing
    event_data = event.dict()
    
    # Validate recurring events for premium users only
    event_data = validate_recurring_event(event_data, current_user['role'])
    
    # Log the original event data for debugging
    logger.info(f"Creating event: {event_data}")
    
    # Auto-populate SEO fields
    try:
        event_data = auto_populate_seo_fields(event_data)
        logger.info(f"Auto-populated SEO fields: slug={event_data.get('slug')}, city={event_data.get('city')}, state={event_data.get('state')}, price={event_data.get('price')}")
    except Exception as e:
        logger.warning(f"SEO auto-population failed: {e}")
        # Continue with original data if auto-population fails
    
    try:
        # Use database connection with proper transaction control
        with get_db_transaction() as conn:
            cursor = conn.cursor()
            
            try:
                # Start transaction (autocommit is already properly disabled in get_db_transaction)
                cursor.execute("BEGIN")
                
                # More robust duplicate check with coordinate tolerance (1 meter)
                # Round coordinates to 6 decimal places (~1 meter precision)
                lat_rounded = round(event_data['lat'], 6)
                lng_rounded = round(event_data['lng'], 6)
                
                duplicate_check = f"""
                    SELECT id FROM events 
                    WHERE TRIM(LOWER(title)) = TRIM(LOWER({placeholder}))
                    AND date = {placeholder} 
                    AND start_time = {placeholder} 
                    AND ABS(lat - {placeholder}) < 0.000001
                    AND ABS(lng - {placeholder}) < 0.000001
                    AND category = {placeholder}
                """
                
                cursor.execute(duplicate_check, (
                    event_data['title'].strip(), 
                    event_data['date'], 
                    event_data['start_time'], 
                    lat_rounded, 
                    lng_rounded, 
                    event_data['category']
                ))
                
                duplicate = cursor.fetchone()
                if duplicate:
                    cursor.execute("ROLLBACK")
                    logger.warning(f"Duplicate event prevented: {event_data['title']} by user {current_user['id']}")
                    raise HTTPException(
                        status_code=409, 
                        detail="An event with these details already exists at this location and time"
                    )
                
                # Ensure unique slug before inserting
                base_slug = event_data.get('slug', '')
                if base_slug:
                    unique_slug = ensure_unique_slug(cursor, base_slug)
                    event_data['slug'] = unique_slug
                    logger.info(f"Generated unique slug: {unique_slug}")
                
                # Use dynamic schema detection to handle different database structures
                actual_columns = get_actual_table_columns(cursor, 'events')
                logger.debug(f"Detected {len(actual_columns)} columns in events table")
                
                # Filter event data to only include fields that exist in the actual database
                filtered_event_data = {}
                for key, value in event_data.items():
                    if key in actual_columns:
                        filtered_event_data[key] = value
                    else:
                        logger.debug(f"Skipping field '{key}' - not in database schema")
                
                # Add required fields that might be missing from the form data
                filtered_event_data['created_by'] = current_user["id"]
                filtered_event_data['created_at'] = datetime.utcnow().isoformat()
                
                # Auto-verify events for premium users
                if current_user.get('role') in ['premium', 'admin']:
                    filtered_event_data['verified'] = True
                
                if 'lat' not in filtered_event_data:
                    filtered_event_data['lat'] = lat_rounded
                if 'lng' not in filtered_event_data:
                    filtered_event_data['lng'] = lng_rounded
                if 'interest_count' not in filtered_event_data and 'interest_count' in actual_columns:
                    filtered_event_data['interest_count'] = 0
                if 'view_count' not in filtered_event_data and 'view_count' in actual_columns:
                    filtered_event_data['view_count'] = 0
                
                # Build dynamic INSERT query
                columns = [col for col in filtered_event_data.keys() if col in actual_columns]
                placeholders_str = ', '.join([placeholder] * len(columns))
                columns_str = ', '.join(columns)
                
                if IS_PRODUCTION and DB_URL:
                    insert_query = f"INSERT INTO events ({columns_str}) VALUES ({placeholders_str}) RETURNING id"
                else:
                    insert_query = f"INSERT INTO events ({columns_str}) VALUES ({placeholders_str})"
                
                values = [filtered_event_data[col] for col in columns]
                
                logger.debug(f"Dynamic INSERT with {len(columns)} columns: {columns[:5]}{'...' if len(columns) > 5 else ''}")
                
                # For PostgreSQL, use RETURNING to get the ID in one step
                if IS_PRODUCTION and DB_URL:
                    cursor.execute(insert_query, values)
                    result = cursor.fetchone()
                    event_id = result['id'] if result else None
                else:
                    # SQLite doesn't support RETURNING, so we use lastrowid
                    cursor.execute(insert_query.replace(" RETURNING id", ""), values)
                    event_id = cursor.lastrowid
                
                if not event_id:
                    cursor.execute("ROLLBACK")
                    raise ValueError("Failed to get ID of created event")
                
                # Fetch the created event
                fetch_query = f"SELECT * FROM events WHERE id = {placeholder}"
                cursor.execute(fetch_query, (event_id,))
                created_event_data = cursor.fetchone()
                
                if not created_event_data:
                    cursor.execute("ROLLBACK")
                    raise ValueError("Created event not found")
                
                # Commit the transaction
                cursor.execute("COMMIT")
                
                # Convert to dict and process datetime objects
                event_dict = dict(created_event_data)
                
                # Convert datetime objects and ensure proper field types
                event_dict = convert_event_datetime_fields(event_dict)
                
                # Clear event cache since a new event was created
                event_cache.clear()
                logger.info(f"Successfully created event {event_id}: {event_data['title']} with SEO fields populated")
                
                return event_dict
                
            except HTTPException as http_ex:
                # Attempt to roll back the transaction
                try:
                    cursor.execute("ROLLBACK")
                except Exception as rollback_error:
                    logger.error(f"Transaction rollback failed: {str(rollback_error)}")
                
                # Pass through HTTP exceptions without modification
                raise http_ex
            except Exception as e:
                # Attempt to roll back the transaction
                try:
                    cursor.execute("ROLLBACK")
                except Exception as rollback_error:
                    logger.error(f"Transaction rollback failed: {str(rollback_error)}")
                
                # Log the detailed error
                error_msg = str(e)
                logger.error(f"Database error in create_event: {error_msg}")
                
                # Wrap in HTTP exception
                raise HTTPException(
                    status_code=500,
                    detail=f"Database error: {error_msg}"
                ) from None
    
    except HTTPException as http_ex:
        # Pass through HTTP exceptions
        raise http_ex
    except Exception as e:
        # Catch any remaining errors
        error_msg = str(e)
        logger.error(f"Error creating event: {error_msg}")
        raise HTTPException(
            status_code=400, 
            detail=f"Could not create event: {error_msg}"
        ) from None

@app.get("/events/manage", response_model=List[EventResponse])
async def list_user_events(current_user: dict = Depends(get_current_user)):
    """
    Retrieve events created by the current user for management (robust version).
    Requires authentication.
    """
    placeholder = get_placeholder()
    try:
        with get_db() as conn:
            c = conn.cursor()
            
            # Query events created by the current user with proper column selection
            actual_columns = get_actual_table_columns(c, 'events')
            
            # Select only columns that exist
            if 'verified' in actual_columns:
                query = f"SELECT * FROM events WHERE created_by = {placeholder} ORDER BY date, start_time"
            else:
                # Exclude verified column if it doesn't exist
                columns = [col for col in actual_columns if col != 'verified']
                column_str = ', '.join(columns)
                query = f"SELECT {column_str} FROM events WHERE created_by = {placeholder} ORDER BY date, start_time"
            
            c.execute(query, (current_user["id"],))
            events = c.fetchall()
            
            # Convert to list of dictionaries and apply datetime conversion
            event_list = []
            for event in events:
                event_dict = dict(event)
                
                # Ensure all required fields exist for EventResponse model
                if 'verified' not in event_dict:
                    event_dict['verified'] = False
                if 'interest_count' not in event_dict:
                    event_dict['interest_count'] = 0
                if 'view_count' not in event_dict:
                    event_dict['view_count'] = 0
                if 'secondary_category' not in event_dict:
                    event_dict['secondary_category'] = None
                    
                event_dict = convert_event_datetime_fields(event_dict)
                event_list.append(event_dict)
            
            return event_list
            
    except Exception as e:
        logger.error(f"Error retrieving user events: {str(e)}")
        raise HTTPException(status_code=500, detail="Error retrieving user events")
async def list_user_events(current_user: dict = Depends(get_current_user)):
    """
    Retrieve events created by the current user for management (robust version).
    Requires authentication.
    """
    placeholder = get_placeholder()
    try:
        with get_db() as conn:
            c = conn.cursor()
            
            # Query events created by the current user with proper column selection
            actual_columns = get_actual_table_columns(c, 'events')
            
            # Select only columns that exist
            if 'verified' in actual_columns:
                query = f"SELECT * FROM events WHERE created_by = {placeholder} ORDER BY date, start_time"
            else:
                # Exclude verified column if it doesn't exist
                columns = [col for col in actual_columns if col != 'verified']
                column_str = ', '.join(columns)
                query = f"SELECT {column_str} FROM events WHERE created_by = {placeholder} ORDER BY date, start_time"
            
            c.execute(query, (current_user["id"],))
            events = c.fetchall()
            
            # Convert to list of dictionaries and apply datetime conversion
            event_list = []
            for event in events:
                event_dict = dict(event)
                
                # Ensure all required fields exist for EventResponse model
                if 'verified' not in event_dict:
                    event_dict['verified'] = False
                if 'interest_count' not in event_dict:
                    event_dict['interest_count'] = 0
                if 'view_count' not in event_dict:
                    event_dict['view_count'] = 0
                if 'secondary_category' not in event_dict:
                    event_dict['secondary_category'] = None
                    
                event_dict = convert_event_datetime_fields(event_dict)
                event_list.append(event_dict)
            
            return event_list
            
    except Exception as e:
        logger.error(f"Error retrieving user events: {str(e)}")
        raise HTTPException(status_code=500, detail="Error retrieving user events")

@app.put("/events/{event_id}", response_model=EventResponse)
async def update_event(
    event_id: int, 
    event: EventCreate, 
    current_user: dict = Depends(get_current_user)
):
    """
    Update an existing event.
    Only the event creator or an admin can update the event.
    Auto-updates SEO fields based on new data.
    """
    placeholder = get_placeholder()
    
    # Convert Pydantic model to dict for processing
    event_data = event.dict()
    
    # Auto-populate SEO fields
    try:
        event_data = auto_populate_seo_fields(event_data)
        logger.info(f"Auto-populated SEO fields for update: slug={event_data.get('slug')}, city={event_data.get('city')}, state={event_data.get('state')}, price={event_data.get('price')}")
    except Exception as e:
        logger.warning(f"SEO auto-population failed during update: {e}")
        # Continue with original data if auto-population fails
    
    try:
        with get_db() as conn:
            try:
                cursor = conn.cursor()
                
                # Start transaction
                cursor.execute("BEGIN")
                
                # First, check if the event exists and who created it
                cursor.execute(f"SELECT * FROM events WHERE id = {placeholder}", (event_id,))
                existing_event = cursor.fetchone()
                
                if not existing_event:
                    cursor.execute("ROLLBACK")
                    raise HTTPException(status_code=404, detail="Event not found")
                
                # Check if current user is the creator or an admin
                if (current_user['id'] != existing_event['created_by'] and 
                    current_user['role'] != UserRole.ADMIN):
                    cursor.execute("ROLLBACK")
                    raise HTTPException(status_code=403, detail="Not authorized to update this event")
                
                # Ensure unique slug before updating
                base_slug = event_data.get('slug', '')
                if base_slug:
                    unique_slug = ensure_unique_slug(cursor, base_slug, event_id)
                    event_data['slug'] = unique_slug
                    logger.info(f"Generated unique slug for update: {unique_slug}")
                
                # Use dynamic schema detection for update as well
                actual_columns = get_actual_table_columns(cursor, 'events')
                logger.debug(f"Detected {len(actual_columns)} columns for update")
                
                # Filter event data to only include fields that exist in the actual database
                # Exclude fields that shouldn't be updated
                excluded_fields = {'id', 'created_at', 'created_by'}
                filtered_event_data = {}
                for key, value in event_data.items():
                    if key in actual_columns and key not in excluded_fields:
                        filtered_event_data[key] = value
                    elif key in excluded_fields:
                        logger.debug(f"Skipping field '{key}' - excluded from updates")
                    else:
                        logger.debug(f"Skipping field '{key}' - not in database schema")
                
                # Add updated_at timestamp if the column exists
                if 'updated_at' in actual_columns:
                    filtered_event_data['updated_at'] = datetime.utcnow().isoformat()
                
                # Build dynamic UPDATE query
                update_columns = list(filtered_event_data.keys())
                set_clause = ', '.join([f"{col} = {placeholder}" for col in update_columns])
                query = f"UPDATE events SET {set_clause} WHERE id = {placeholder}"
                values = list(filtered_event_data.values()) + [event_id]
                
                logger.debug(f"Dynamic UPDATE with {len(update_columns)} columns: {update_columns[:5]}{'...' if len(update_columns) > 5 else ''}")
                
                cursor.execute(query, values)
                
                # Fetch and return the updated event
                cursor.execute(f"SELECT * FROM events WHERE id = {placeholder}", (event_id,))
                updated_event = cursor.fetchone()
                
                if not updated_event:
                    cursor.execute("ROLLBACK")
                    raise HTTPException(status_code=404, detail="Event not found after update")
                
                # Commit the transaction
                cursor.execute("COMMIT")
                
                # Convert to dict and handle datetime objects
                event_dict = dict(updated_event)
                
                # Convert datetime objects and ensure proper field types
                event_dict = convert_event_datetime_fields(event_dict)
                
                # Clear event cache since an event was updated
                event_cache.clear()
                logger.info("Cleared event cache after updating event")
                
                return event_dict
                
            except HTTPException as http_ex:
                # Attempt to roll back the transaction
                try:
                    cursor.execute("ROLLBACK")
                except Exception as rollback_error:
                    logger.error(f"Transaction rollback failed: {str(rollback_error)}")
                
                # Re-raise the HTTP exception
                raise http_ex
            except Exception as e:
                # Attempt to roll back the transaction
                try:
                    cursor.execute("ROLLBACK")
                except Exception as rollback_error:
                    logger.error(f"Transaction rollback failed: {str(rollback_error)}")
                
                # Log error and raise HTTP exception
                error_msg = str(e)
                logger.error(f"Error updating event {event_id}: {error_msg}")
                raise HTTPException(status_code=500, detail=f"Error updating event: {error_msg}")
                
    except HTTPException as http_ex:
        # Pass through HTTP exceptions
        raise http_ex
    except Exception as e:
        # Handle other exceptions
        error_msg = str(e)
        logger.error(f"Error updating event {event_id}: {error_msg}")
        raise HTTPException(status_code=500, detail=f"Error updating event: {error_msg}")

@app.delete("/events/{event_id}")
async def delete_event(
    event_id: int, 
    current_user: dict = Depends(get_current_user)
):
    """
    Delete an event.
    Only the event creator or an admin can delete the event.
    """
    placeholder = get_placeholder()
    
    try:
        with get_db() as conn:
            try:
                cursor = conn.cursor()
                
                # Start transaction
                cursor.execute("BEGIN")
                
                # First, check if the event exists and who created it
                cursor.execute(f"SELECT * FROM events WHERE id = {placeholder}", (event_id,))
                existing_event = cursor.fetchone()
                
                if not existing_event:
                    cursor.execute("ROLLBACK")
                    raise HTTPException(status_code=404, detail="Event not found")
                
                # Check if current user is the creator or an admin
                if (current_user['id'] != existing_event['created_by'] and 
                    current_user['role'] != UserRole.ADMIN):
                    cursor.execute("ROLLBACK")
                    raise HTTPException(status_code=403, detail="Not authorized to delete this event")
                
                # Delete the event
                cursor.execute(f"DELETE FROM events WHERE id = {placeholder}", (event_id,))
                
                # Commit the transaction
                cursor.execute("COMMIT")
                
                # Clear event cache since an event was deleted
                event_cache.clear()
                
                # Also clean up any specific cache entries for this event
                event_cache.delete(f"event:{event_id}")
                event_cache.delete(f"event_interest:{event_id}")
                event_cache.delete(f"event_view:{event_id}")
                
                logger.info("Cleared event cache after deleting event")
                
                return {"detail": "Event successfully deleted"}
                
            except HTTPException as http_ex:
                # Attempt to roll back the transaction
                try:
                    cursor.execute("ROLLBACK")
                except Exception as rollback_error:
                    logger.error(f"Transaction rollback failed: {str(rollback_error)}")
                
                # Re-raise the HTTP exception
                raise http_ex
            except Exception as e:
                # Attempt to roll back the transaction
                try:
                    cursor.execute("ROLLBACK")
                except Exception as rollback_error:
                    logger.error(f"Transaction rollback failed: {str(rollback_error)}")
                
                # Log error and raise HTTP exception
                error_msg = str(e)
                logger.error(f"Error deleting event {event_id}: {error_msg}")
                raise HTTPException(status_code=500, detail=f"Error deleting event: {error_msg}")
                
    except HTTPException as http_ex:
        # Pass through HTTP exceptions
        raise http_ex
    except Exception as e:
        # Handle other exceptions
        error_msg = str(e)
        logger.error(f"Error deleting event {event_id}: {error_msg}")
        raise HTTPException(status_code=500, detail=f"Error deleting event: {error_msg}")

# Optional: Admin endpoint to get all users (for admin dashboard)
@app.get("/admin/users", response_model=List[UserResponse])
async def list_users(current_user: dict = Depends(get_current_user)):
    """
    List all users (admin-only endpoint)
    """
    if current_user['role'] != UserRole.ADMIN:
        raise HTTPException(status_code=403, detail="Not authorized")
    
    try:
        with get_db() as conn:
            c = conn.cursor()
            c.execute("SELECT * FROM users")
            users = c.fetchall()
            
            return [dict(user) for user in users]
    except Exception as e:
        logger.error(f"Error retrieving users: {str(e)}")
        raise HTTPException(status_code=500, detail="Error retrieving users")

# Health check endpoint
@app.get("/health")
async def health_check():
    """
    Health check endpoint that returns app status.
    """
    try:
        # Test database connection
        with get_db() as conn:
            cursor = conn.cursor()
            cursor.execute("SELECT 1")
            cursor.fetchone()
        
        # Get cache stats for monitoring
        cache_stats = event_cache.stats()
        
        return {
            "status": "healthy",
            "timestamp": datetime.utcnow().isoformat(),
            "database": "connected",
            "cache": cache_stats,
            "memory_optimization": "enabled"
        }
    except Exception as e:
        logger.error(f"Health check failed: {str(e)}")
        return {
            "status": "unhealthy",
            "timestamp": datetime.utcnow().isoformat(),
            "database": "error",
            "error": str(e),
            "cache": event_cache.stats() if event_cache else None
        }

# Root endpoint
@app.get("/")
async def root():
    """
    Root endpoint for API
    """
    return {
        "message": "EventFinder API is running",
        "docs": "/docs",
        "health": "/health"
    }

# New Admin Endpoints
@app.delete("/admin/users/{user_id}")
async def delete_user(
    user_id: int, 
    current_user: dict = Depends(get_current_user)
):
    """
    Delete a user (admin-only)
    """
    if current_user['role'] != UserRole.ADMIN:
        raise HTTPException(status_code=403, detail="Not authorized")
    
    placeholder = get_placeholder()
    try:
        with get_db() as conn:
            c = conn.cursor()
            
            # Check if user exists
            c.execute(f"SELECT * FROM users WHERE id = {placeholder}", (user_id,))
            user = c.fetchone()
            
            if not user:
                raise HTTPException(status_code=404, detail="User not found")
            
            # Prevent deleting the last admin
            c.execute(f"SELECT COUNT(*) as admin_count FROM users WHERE role = {placeholder}", (UserRole.ADMIN,))
            admin_count = c.fetchone()['admin_count']
            
            if admin_count <= 1 and user['role'] == UserRole.ADMIN:
                raise HTTPException(status_code=400, detail="Cannot delete the last admin")
            
            # Delete user
            c.execute(f"DELETE FROM users WHERE id = {placeholder}", (user_id,))
            
            # Optionally, delete user's events
            c.execute(f"DELETE FROM events WHERE created_by = {placeholder}", (user_id,))
            
            conn.commit()
            
            return {"detail": "User successfully deleted"}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error deleting user {user_id}: {str(e)}")
        raise HTTPException(status_code=500, detail="Error deleting user")

@app.put("/admin/users/{user_id}/role")
async def update_user_role(
    user_id: int, 
    role: UserRole,
    current_user: dict = Depends(get_current_user)
):
    """
    Update user role (admin-only)
    """
    if current_user['role'] != UserRole.ADMIN:
        raise HTTPException(status_code=403, detail="Not authorized")
    
    placeholder = get_placeholder()
    try:
        with get_db() as conn:
            c = conn.cursor()
            
            # Check if user exists
            c.execute(f"SELECT * FROM users WHERE id = {placeholder}", (user_id,))
            user = c.fetchone()
            
            if not user:
                raise HTTPException(status_code=404, detail="User not found")
            
            # Prevent removing last admin
            if user['role'] == UserRole.ADMIN:
                c.execute(f"SELECT COUNT(*) as admin_count FROM users WHERE role = {placeholder}", (UserRole.ADMIN,))
                admin_count = c.fetchone()['admin_count']
                
                if admin_count <= 1:
                    raise HTTPException(status_code=400, detail="Cannot remove the last admin")
            
            # Update role
            c.execute(f"UPDATE users SET role = {placeholder} WHERE id = {placeholder}", (role, user_id))
            conn.commit()
            
            return {"detail": "User role updated successfully"}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error updating user role {user_id}: {str(e)}")
        raise HTTPException(status_code=500, detail="Error updating user role")

class AdminPasswordReset(BaseModel):
    new_password: str
    
    @validator('new_password')
    def validate_password(cls, v):
        # Use the PasswordValidator to get validation results
        validation = PasswordValidator.validate_password(v)
        
        # If not valid, raise a validation error with all feedback
        if not validation['is_valid']:
            raise ValueError('\n'.join(validation['feedback']))
        
        return v

@app.put("/admin/users/{user_id}/password")
async def admin_reset_user_password(
    user_id: int,
    password_data: AdminPasswordReset,
    current_user: dict = Depends(get_current_user)
):
    """
    Reset a user's password (admin-only)
    """
    if current_user['role'] != UserRole.ADMIN:
        raise HTTPException(status_code=403, detail="Not authorized")
    
    placeholder = get_placeholder()
    try:
        with get_db() as conn:
            c = conn.cursor()
            
            # Check if user exists
            c.execute(f"SELECT * FROM users WHERE id = {placeholder}", (user_id,))
            user = c.fetchone()
            
            if not user:
                raise HTTPException(status_code=404, detail="User not found")
            
            # Hash the new password
            hashed_password = get_password_hash(password_data.new_password)
            
            # Update the user's password
            c.execute(f"UPDATE users SET hashed_password = {placeholder} WHERE id = {placeholder}", (hashed_password, user_id))
            conn.commit()
            
            # Log the activity
            log_activity(current_user['id'], "admin_password_reset", f"Reset password for user {user['email']}")
            
            return {"detail": "Password reset successfully"}
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error resetting password for user {user_id}: {str(e)}")
        raise HTTPException(status_code=500, detail="Error resetting password")

@app.get("/admin/activity-logs")
async def get_activity_logs(
    current_user: dict = Depends(get_current_user),
    limit: int = 500,
    offset: int = 0
):
    """
    Retrieve activity logs (admin-only)
    """
    if current_user['role'] != UserRole.ADMIN:
        raise HTTPException(status_code=403, detail="Not authorized")
    
    placeholder = get_placeholder()
    try:
        with get_db() as conn:
            c = conn.cursor()
            
            # Create an activity logs table if it doesn't exist
            if IS_PRODUCTION and DB_URL:
                c.execute('''CREATE TABLE IF NOT EXISTS activity_logs (
                    id SERIAL PRIMARY KEY,
                    user_id INTEGER,
                    action TEXT NOT NULL,
                    details TEXT,
                    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY(user_id) REFERENCES users(id)
                )''')
            else:
                c.execute('''CREATE TABLE IF NOT EXISTS activity_logs (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    user_id INTEGER,
                    action TEXT NOT NULL,
                    details TEXT,
                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY(user_id) REFERENCES users(id)
                )''')
            
            # Fetch logs with optional pagination
            c.execute(f"""
                SELECT al.*, u.email 
                FROM activity_logs al
                LEFT JOIN users u ON al.user_id = u.id
                ORDER BY al.timestamp DESC
                LIMIT {placeholder} OFFSET {placeholder}
            """, (limit, offset))
            
            logs = [dict(log) for log in c.fetchall()]
            
            return logs
    except Exception as e:
        logger.error(f"Error retrieving activity logs: {str(e)}")
        raise HTTPException(status_code=500, detail="Error retrieving activity logs")

# Utility function to log activities
def log_activity(user_id: int, action: str, details: str = None):
    """
    Log user activities
    """
    placeholder = get_placeholder()
    try:
        with get_db() as conn:
            c = conn.cursor()
            
            # Ensure activity_logs table exists
            if IS_PRODUCTION and DB_URL:
                c.execute('''CREATE TABLE IF NOT EXISTS activity_logs (
                    id SERIAL PRIMARY KEY,
                    user_id INTEGER,
                    action TEXT NOT NULL,
                    details TEXT,
                    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY(user_id) REFERENCES users(id)
                )''')
            else:
                c.execute('''CREATE TABLE IF NOT EXISTS activity_logs (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    user_id INTEGER,
                    action TEXT NOT NULL,
                    details TEXT,
                    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY(user_id) REFERENCES users(id)
                )''')
            
            # Insert log entry
            c.execute(
                f"INSERT INTO activity_logs (user_id, action, details) VALUES ({placeholder}, {placeholder}, {placeholder})",
                (user_id, action, details)
            )
            conn.commit()
    except Exception as e:
        logger.error(f"Error logging activity: {str(e)}")

# Helper functions for AI event processing
def _generate_ai_summary(event_dict):
    """Generate AI-friendly summary with proper time handling"""
    start_time = event_dict['start_time']
    end_time = event_dict.get('end_time')
    end_date = event_dict.get('end_date')
    
    # Build time string
    if end_time:
        time_str = f"from {start_time} to {end_time}"
    else:
        time_str = f"at {start_time}"
        
    # Build date string
    if end_date and end_date != event_dict['date']:
        date_str = f"from {event_dict['date']} to {end_date}"
    else:
        date_str = f"on {event_dict['date']}"
        
    description = event_dict['description'][:200]
    if len(event_dict['description']) > 200:
        description += "..."
        
    return f"{event_dict['title']} - {event_dict['category']} event {date_str} {time_str} in {event_dict['address']}. {description}"

def _generate_structured_data(event_dict):
    """Generate structured data with proper date/time handling"""
    start_datetime = f"{event_dict['date']}T{event_dict['start_time']}:00"
    
    # Calculate end datetime
    end_date = event_dict.get('end_date') or event_dict['date']
    end_time = event_dict.get('end_time') or event_dict['start_time']
    end_datetime = f"{end_date}T{end_time}:00"
    
    return {
        "@type": "Event",
        "name": event_dict['title'],
        "startDate": start_datetime,
        "endDate": end_datetime,
        "location": {
            "@type": "Place",
            "address": event_dict['address']
        },
        "description": event_dict['description'],
        "eventStatus": "EventScheduled"
    }

# AI Search API Endpoints
@app.get("/api/v1/local-events")
async def get_local_events_for_ai(
    lat: Optional[float] = None,
    lng: Optional[float] = None,
    radius: Optional[float] = 25.0,  # miles
    category: Optional[str] = None,
    limit: Optional[int] = 200
):
    """
    Public API endpoint for AI search tools to discover local events.
    
    This endpoint provides structured event data that AI tools can use
    to answer queries like "local events near me" or "events this weekend".
    """
    placeholder = get_placeholder()
    
    try:
        with get_db() as conn:
            c = conn.cursor()
            
            # Build base query including UX fields
            query = """
                SELECT id, title, description, date, start_time, end_time, end_date, category, 
                       address, lat, lng, created_at, fee_required, event_url, host_name
                FROM events 
                WHERE date::date >= CURRENT_DATE
            """
            params = []
            
            # Add category filter
            if category:
                query += f" AND category = {placeholder}"
                params.append(category)
            
            # Add location-based filtering if coordinates provided
            if lat is not None and lng is not None:
                # Use Haversine formula for distance calculation
                query += f"""
                    AND (
                        3959 * acos(
                            cos(radians({placeholder})) * cos(radians(lat)) * 
                            cos(radians(lng) - radians({placeholder})) + 
                            sin(radians({placeholder})) * sin(radians(lat))
                        )
                    ) <= {placeholder}
                """
                params.extend([lat, lng, lat, radius])
            
            # Order by date and limit results
            query += f" ORDER BY date, start_time LIMIT {placeholder}"
            params.append(limit)
            
            c.execute(query, params)
            events = c.fetchall()
            
            # Format response for AI consumption
            ai_response = {
                "status": "success",
                "message": "Local events discovered",
                "search_context": {
                    "query_type": "local_events_near_me",
                    "location": {"lat": lat, "lng": lng} if lat and lng else None,
                    "radius_miles": radius,
                    "category_filter": category,
                    "results_count": len(events)
                },
                "events": [],
                "metadata": {
                    "platform": "todo-events.com",
                    "description": "Real-time local event discovery platform",
                    "last_updated": datetime.utcnow().isoformat(),
                    "categories": ["food-drink", "music", "arts", "sports", "community"],
                    "coverage_area": "United States",
                    "features": [
                        "Location-based event search",
                        "Real-time event updates", 
                        "Community-driven content",
                        "Interactive event mapping",
                        "Category-based filtering"
                    ]
                }
            }
            
            # Process each event with AI-friendly formatting
            for event in events:
                event_dict = dict(event)
                
                # Calculate distance if location provided
                distance = None
                if lat is not None and lng is not None:
                    try:
                        from math import radians, cos, sin, asin, sqrt
                        
                        def haversine(lat1, lon1, lat2, lon2):
                            # Convert to radians
                            lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])
                            
                            # Haversine formula
                            dlat = lat2 - lat1
                            dlon = lon2 - lon1
                            a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2
                            c = 2 * asin(sqrt(a))
                            r = 3959  # Earth radius in miles
                            return c * r
                        
                        distance = round(haversine(lat, lng, event_dict['lat'], event_dict['lng']), 1)
                    except:
                        distance = None
                
                # Format event for AI consumption
                ai_event = {
                    "id": event_dict['id'],
                    "title": event_dict['title'],
                    "description": event_dict['description'],
                    "date": event_dict['date'],
                    "start_time": event_dict['start_time'],
                    "end_time": event_dict['end_time'],
                    "end_date": event_dict['end_date'],
                    "category": event_dict['category'],
                    "location": {
                        "address": event_dict['address'],
                        "coordinates": {
                            "lat": event_dict['lat'],
                            "lng": event_dict['lng']
                        },
                        "distance_miles": distance
                    },
                    "url": f"https://todo-events.com/?event={event_dict['id']}",
                    "ai_summary": _generate_ai_summary(event_dict),
                    "structured_data": _generate_structured_data(event_dict)
                }
                
                ai_response["events"].append(ai_event)
            
            # Add helpful context for AI tools
            if len(events) == 0:
                ai_response["message"] = "No local events found matching your criteria"
                ai_response["suggestions"] = [
                    "Try expanding your search radius",
                    "Remove category filters to see all event types", 
                    "Check for events on different dates",
                    "Visit todo-events.com to create the first event in your area"
                ]
            
            return ai_response
            
    except Exception as e:
        logger.error(f"Error in AI events API: {str(e)}")
        return {
            "status": "error",
            "message": "Unable to retrieve local events",
            "error": str(e),
            "metadata": {
                "platform": "todo-events.com",
                "support": "Visit https://todo-events.com for manual event discovery"
            }
        }

# Automation Control Endpoints
@app.get("/api/v1/automation/status")
async def get_automation_status():
    """Get status of automated tasks"""
    return {
        "automation": {
            "enabled": IS_PRODUCTION,
            "scheduler_running": task_manager.scheduler.running if IS_PRODUCTION else False,
            "last_sitemap_update": task_manager.last_sitemap_update.isoformat() if task_manager.last_sitemap_update else None,
            "last_event_refresh": task_manager.last_event_refresh.isoformat() if task_manager.last_event_refresh else None,
            "task_status": task_manager.task_status,
            "next_runs": {
                job.id: job.next_run_time.isoformat() if job.next_run_time else None
                for job in task_manager.scheduler.get_jobs()
            } if IS_PRODUCTION and task_manager.scheduler.running else {}
        },
        "system": {
            "environment": "production" if IS_PRODUCTION else "development",
            "timestamp": datetime.utcnow().isoformat()
        }
    }
@app.post("/api/v1/automation/trigger/{task_name}")
async def trigger_automated_task(task_name: str, background_tasks: BackgroundTasks):
    """Manually trigger automated tasks"""
    
    if not IS_PRODUCTION:
        raise HTTPException(status_code=400, detail="Automation only available in production")
    
    task_functions = {
        "sitemap": task_manager.generate_sitemap_automatically,
        "events": task_manager.refresh_event_data,
        "ai_sync": task_manager.sync_with_ai_tools,
        "cleanup": task_manager.cleanup_expired_events
    }
    
    if task_name not in task_functions:
        raise HTTPException(
            status_code=400, 
            detail=f"Unknown task: {task_name}. Available: {list(task_functions.keys())}"
        )
    
    # Trigger the task in the background
    background_tasks.add_task(task_functions[task_name])
    
    return {
        "message": f"Task '{task_name}' triggered successfully",
        "task": task_name,
        "triggered_at": datetime.utcnow().isoformat(),
        "note": "Task is running in the background"
    }

@app.get("/sitemap.xml")
async def get_dynamic_sitemap():
    """Serve dynamically generated sitemap"""
    try:
        # If we have a cached sitemap from automation, serve it
        if hasattr(task_manager, 'current_sitemap') and task_manager.current_sitemap:
            return Response(
                content=task_manager.current_sitemap,
                media_type="application/xml",
                headers={"Cache-Control": "public, max-age=3600"}  # Cache for 1 hour
            )
        
        # Otherwise, generate on-demand
        events = await task_manager.get_current_events()
        sitemap_content = await task_manager.build_sitemap_content(events)
        
        return Response(
            content=sitemap_content,
            media_type="application/xml",
            headers={"Cache-Control": "public, max-age=1800"}  # Cache for 30 minutes
        )
        
    except Exception as e:
        logger.error(f"Error serving dynamic sitemap: {str(e)}")
        raise HTTPException(status_code=500, detail="Error generating sitemap")
def generate_browser_fingerprint(request: Request) -> str:
    """Generate a browser fingerprint for anonymous users"""
    import hashlib
    
    # Get client info
    user_agent = request.headers.get("user-agent", "")
    client_ip = request.client.host if request.client else "unknown"
    accept_language = request.headers.get("accept-language", "")
    accept_encoding = request.headers.get("accept-encoding", "")
    
    # Create fingerprint
    fingerprint_string = f"{client_ip}:{user_agent}:{accept_language}:{accept_encoding}"
    fingerprint = hashlib.md5(fingerprint_string.encode()).hexdigest()
    
    return fingerprint

async def track_event_view(event_id: int, user_id: int = None, browser_fingerprint: str = None):
    """
    Track a view for an event - simplified and robust implementation
    """
    if not browser_fingerprint:
        browser_fingerprint = 'anonymous'
    
    placeholder = get_placeholder()
    
    try:
        with get_db() as conn:
            cursor = conn.cursor()
            
            # First, verify the event exists
            cursor.execute(f"SELECT id FROM events WHERE id = {placeholder}", (event_id,))
            if not cursor.fetchone():
                logger.warning(f"Attempted to track view for non-existent event {event_id}")
                return False
            
            # Check if view already exists (prevent duplicates)
            if user_id:
                cursor.execute(
                    f"SELECT id FROM event_views WHERE event_id = {placeholder} AND user_id = {placeholder} LIMIT 1",
                    (event_id, user_id)
                )
            else:
                cursor.execute(
                    f"SELECT id FROM event_views WHERE event_id = {placeholder} AND browser_fingerprint = {placeholder} LIMIT 1",
                    (event_id, browser_fingerprint)
                )
            
            existing_view = cursor.fetchone()
            
            if existing_view:
                # View already exists, don't track again
                return False
            
            # Insert new view record
            cursor.execute(
                f"INSERT INTO event_views (event_id, user_id, browser_fingerprint) VALUES ({placeholder}, {placeholder}, {placeholder})",
                (event_id, user_id, browser_fingerprint)
            )
            
            # Update view count safely
            cursor.execute(
                f"UPDATE events SET view_count = COALESCE(view_count, 0) + 1 WHERE id = {placeholder}",
                (event_id,)
            )
            
            conn.commit()
            logger.info(f"Successfully tracked view for event {event_id}")
            return True
            
    except Exception as e:
        logger.error(f"Error in track_event_view for event {event_id}: {type(e).__name__}: {str(e)}")
        return False

# Interest and View API Endpoints
@app.post("/events/{event_id}/interest")
async def toggle_event_interest(
    event_id: int, 
    request: Request,
    current_user: dict = Depends(get_current_user_optional_no_exception)
):
    """
    Toggle interest in an event - simplified endpoint
    """
    try:
        # Get user ID if authenticated
        user_id = current_user.get('id') if current_user else None
        logger.info(f"üéØ Interest toggle - Event: {event_id}, User ID: {user_id}, Has Auth: {current_user is not None}")
        
        # Generate browser fingerprint
        browser_fingerprint = generate_browser_fingerprint(request)
        if not browser_fingerprint:
            browser_fingerprint = 'anonymous'
        
        placeholder = get_placeholder()
        
        with get_db() as conn:
            cursor = conn.cursor()
            
            # Verify event exists
            cursor.execute(f"SELECT id FROM events WHERE id = {placeholder}", (event_id,))
            if not cursor.fetchone():
                raise HTTPException(status_code=404, detail="Event not found")
            
            # Check if interest already exists
            if user_id:
                cursor.execute(
                    f"SELECT id FROM event_interests WHERE event_id = {placeholder} AND user_id = {placeholder} LIMIT 1",
                    (event_id, user_id)
                )
            else:
                cursor.execute(
                    f"SELECT id FROM event_interests WHERE event_id = {placeholder} AND browser_fingerprint = {placeholder} LIMIT 1",
                    (event_id, browser_fingerprint)
                )
            
            existing_interest = cursor.fetchone()
            
            if existing_interest:
                # Remove interest
                if user_id:
                    cursor.execute(
                        f"DELETE FROM event_interests WHERE event_id = {placeholder} AND user_id = {placeholder}",
                        (event_id, user_id)
                    )
                else:
                    cursor.execute(
                        f"DELETE FROM event_interests WHERE event_id = {placeholder} AND browser_fingerprint = {placeholder}",
                        (event_id, browser_fingerprint)
                    )
                
                # Decrease interest count
                cursor.execute(
                    f"UPDATE events SET interest_count = GREATEST(COALESCE(interest_count, 0) - 1, 0) WHERE id = {placeholder}",
                    (event_id,)
                )
                
                action = "removed"
                interested = False
            else:
                # Add interest
                cursor.execute(
                    f"INSERT INTO event_interests (event_id, user_id, browser_fingerprint) VALUES ({placeholder}, {placeholder}, {placeholder})",
                    (event_id, user_id, browser_fingerprint)
                )
                
                # Increase interest count
                cursor.execute(
                    f"UPDATE events SET interest_count = COALESCE(interest_count, 0) + 1 WHERE id = {placeholder}",
                    (event_id,)
                )
                
                action = "added"
                interested = True
            
            # Get updated count with ultra-robust safety check
            cursor.execute(f"SELECT COALESCE(interest_count, 0) FROM events WHERE id = {placeholder}", (event_id,))
            result = cursor.fetchone()
            
            # Ultra-robust result handling for different database types
            try:
                if isinstance(result, (list, tuple)):
                    interest_count = result[0] if len(result) > 0 else 0
                elif hasattr(result, '__getitem__'):
                    interest_count = result[0]
                else:
                    interest_count = int(result) if result is not None else 0
            except (IndexError, KeyError, TypeError, ValueError):
                interest_count = 0
            
            conn.commit()
            
            logger.info(f"‚úÖ Interest {action} - Event: {event_id}, Count: {interest_count}, User: {user_id}, Fingerprint: {browser_fingerprint}")
            
            return {
                "success": True,
                "action": action,
                "interested": interested,
                "interest_count": interest_count,
                "event_id": event_id
            }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error in interest toggle endpoint for event {event_id}: {type(e).__name__}: {str(e)}")
        raise HTTPException(status_code=500, detail="Error updating interest")

@app.get("/events/{event_id}/interest")
async def get_event_interest_status(
    event_id: int,
    request: Request,
    current_user: dict = Depends(get_current_user_optional_no_exception)
):
    """
    Get interest status for an event - optimized for mobile performance
    """
    try:
        # Get user ID if authenticated
        user_id = current_user.get('id') if current_user else None
        
        # Generate browser fingerprint
        browser_fingerprint = generate_browser_fingerprint(request)
        if not browser_fingerprint:
            browser_fingerprint = 'anonymous'
        
        placeholder = get_placeholder()
        
        with get_db() as conn:
            cursor = conn.cursor()
            
            # Single optimized query to get both event data and interest status
            if user_id:
                # For authenticated users, check both user and fingerprint
                query = f"""
                    SELECT 
                        e.id,
                        COALESCE(e.interest_count, 0) as interest_count,
                        COALESCE(e.view_count, 0) as view_count,
                        CASE WHEN ei.id IS NOT NULL THEN true ELSE false END as interested
                    FROM events e
                    LEFT JOIN event_interests ei ON (
                        e.id = ei.event_id AND 
                        (ei.user_id = {placeholder} OR ei.browser_fingerprint = {placeholder})
                    )
                    WHERE e.id = {placeholder}
                    LIMIT 1
                """
                cursor.execute(query, (user_id, browser_fingerprint, event_id))
            else:
                # For anonymous users, check only fingerprint
                query = f"""
                    SELECT 
                        e.id,
                        COALESCE(e.interest_count, 0) as interest_count,
                        COALESCE(e.view_count, 0) as view_count,
                        CASE WHEN ei.id IS NOT NULL THEN true ELSE false END as interested
                    FROM events e
                    LEFT JOIN event_interests ei ON (
                        e.id = ei.event_id AND ei.browser_fingerprint = {placeholder}
                    )
                    WHERE e.id = {placeholder}
                    LIMIT 1
                """
                cursor.execute(query, (browser_fingerprint, event_id))
            
            result = cursor.fetchone()
            
            if not result:
                raise HTTPException(status_code=404, detail="Event not found")
            
            # Convert to dict and ensure proper types
            if hasattr(result, '_asdict'):
                data = result._asdict()
            elif isinstance(result, dict):
                data = dict(result)
            else:
                data = {
                    'id': result[0],
                    'interest_count': result[1],
                    'view_count': result[2],
                    'interested': result[3]
                }
            
            # Ensure proper types
            return {
                'interested': bool(data['interested']),
                'interest_count': int(data['interest_count'] or 0),
                'view_count': int(data['view_count'] or 0)
            }
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting interest status for event {event_id}: {str(e)}")
        raise HTTPException(status_code=500, detail="Failed to get interest status")

@app.post("/events/{event_id}/view")
async def track_event_view_endpoint(
    event_id: int,
    request: Request,
    current_user: dict = Depends(get_current_user_optional_no_exception)
):
    """
    Track a view for an event - simplified endpoint
    """
    try:
        # Get user ID if authenticated
        user_id = current_user.get('id') if current_user else None
        
        # Generate browser fingerprint
        browser_fingerprint = generate_browser_fingerprint(request)
        
        # Track the view
        view_tracked = await track_event_view(event_id, user_id, browser_fingerprint)
        
        # Get updated view count with ultra-robust safety check
        placeholder = get_placeholder()
        with get_db() as conn:
            cursor = conn.cursor()
            cursor.execute(f"SELECT COALESCE(view_count, 0) FROM events WHERE id = {placeholder}", (event_id,))
            result = cursor.fetchone()
            if not result:
                raise HTTPException(status_code=404, detail="Event not found after view tracking")
            
            # Ultra-robust result handling for different database types
            try:
                if isinstance(result, (list, tuple)):
                    view_count = result[0] if len(result) > 0 else 0
                elif hasattr(result, '__getitem__'):
                    view_count = result[0]
                else:
                    view_count = int(result) if result is not None else 0
            except (IndexError, KeyError, TypeError, ValueError):
                view_count = 0
        
        return {
            "success": True,
            "view_tracked": view_tracked,
            "view_count": view_count,
            "event_id": event_id
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error in view tracking endpoint for event {event_id}: {type(e).__name__}: {str(e)}")
        raise HTTPException(status_code=500, detail="Error tracking view")

# Premium Status Endpoint
@app.get("/users/premium-status")
async def get_premium_status(current_user: dict = Depends(get_current_user)):
    """
    Get user's premium status with event limits and counts
    """
    is_premium = current_user['role'] in ['premium', 'admin']
    is_enterprise = current_user['role'] == 'enterprise'
    
    # Get current month's event count for this user
    current_month_events = 0
    
    try:
        with get_db() as conn:
            cursor = conn.cursor()
            
            # Count events created this month
            from datetime import datetime, timedelta
            start_of_month = datetime.now().replace(day=1, hour=0, minute=0, second=0, microsecond=0)
            
            if IS_PRODUCTION and DB_URL:
                # PostgreSQL
                cursor.execute("""
                    SELECT COUNT(*) as count
                    FROM events 
                    WHERE created_by = %s 
                    AND created_at >= %s
                """, (current_user['id'], start_of_month))
            else:
                # SQLite
                cursor.execute("""
                    SELECT COUNT(*) as count
                    FROM events 
                    WHERE created_by = ? 
                    AND created_at >= ?
                """, (current_user['id'], start_of_month))
            
            result = cursor.fetchone()
            current_month_events = result['count'] if result else 0
            
    except Exception as e:
        logger.error(f"Error counting user events: {str(e)}")
        current_month_events = 0
    
    # Determine event limits based on role
    if is_enterprise:
        event_limit = 250
    elif is_premium:
        event_limit = 10
    else:
        event_limit = 0  # Free users have no limit but no premium features
    
    return {
        "is_premium": is_premium,
        "is_enterprise": is_enterprise,
        "role": current_user['role'],
        "user_id": current_user['id'],
        "event_limit": event_limit,
        "current_month_events": current_month_events,
        "events_remaining": max(0, event_limit - current_month_events) if event_limit > 0 else None,
        "can_create_events": event_limit == 0 or current_month_events < event_limit,
        "features": {
            "verified_events": is_premium or is_enterprise,
            "analytics": is_premium or is_enterprise,
            "recurring_events": is_premium or is_enterprise,
            "priority_support": is_premium or is_enterprise,
            "enhanced_visibility": is_premium or is_enterprise
        }
    }

# Premium trial management endpoints
@app.post("/premium/cancel-trial")
async def cancel_premium_trial(current_user: dict = Depends(get_current_user)):
    """Cancel a user's premium trial immediately"""
    try:
        placeholder = get_placeholder()
        
        with get_db() as conn:
            c = conn.cursor()
            
            # Check if user has an active trial
            c.execute(f"""
                SELECT premium_expires_at, role 
                FROM users 
                WHERE id = {placeholder}
            """, (current_user['id'],))
            
            user_data = c.fetchone()
            
            if not user_data or user_data['role'] not in ['premium', 'admin']:
                raise HTTPException(status_code=404, detail="No active premium trial found")
            
            # Check if they have a Stripe subscription (if so, this isn't a trial)
            try:
                customers = stripe.Customer.list(email=current_user['email'], limit=1)
                if customers.data:
                    customer = customers.data[0]
                    subscriptions = stripe.Subscription.list(
                        customer=customer.id,
                        status='active',
                        limit=10
                    )
                    if subscriptions.data:
                        raise HTTPException(status_code=400, detail="Cannot cancel trial - active subscription found. Use subscription cancellation instead.")
            except stripe.error.StripeError:
                # If Stripe lookup fails, assume it's a trial
                pass
            
            # Cancel the trial by setting role back to user and clearing expiration
            c.execute(f"""
                UPDATE users 
                SET role = 'user', premium_expires_at = NULL
                WHERE id = {placeholder}
            """, (current_user['id'],))
            
            conn.commit()
            
            log_activity(current_user['id'], "trial_cancelled", "Premium trial cancelled by user")
            logger.info(f"‚úÖ Premium trial cancelled for user {current_user['id']} ({current_user['email']})")
            
            # Send trial cancellation email
            try:
                from email_config import email_service
                user_name = current_user['email'].split('@')[0]
                email_sent = email_service.send_trial_cancellation_email(
                    to_email=current_user['email'],
                    user_name=user_name
                )
                
                if email_sent:
                    logger.info(f"‚úÖ Trial cancellation email sent to {current_user['email']}")
                else:
                    logger.error(f"‚ùå Failed to send trial cancellation email to {current_user['email']}")
            except Exception as e:
                logger.error(f"‚ùå Error sending trial cancellation email: {str(e)}")
            
            return {
                "success": True,
                "message": "Premium trial cancelled successfully",
                "new_role": "user"
            }
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error cancelling premium trial: {str(e)}")
        raise HTTPException(status_code=500, detail="Failed to cancel trial")

@app.post("/premium/convert-trial-to-subscription")
async def convert_trial_to_subscription(
    request: Request,
    current_user: dict = Depends(get_current_user)
):
    """Convert an active trial to a paid subscription, keeping trial benefits until expiration"""
    try:
        # Parse request body
        try:
            body = await request.json()
        except Exception:
            body = {}
            
        pricing_tier = body.get('pricing_tier', 'premium')
        
        if pricing_tier not in ['premium', 'enterprise']:
            raise HTTPException(status_code=400, detail="Invalid pricing tier")
        
        placeholder = get_placeholder()
        
        with get_db() as conn:
            c = conn.cursor()
            
            # Check if user has an active trial
            c.execute(f"""
                SELECT premium_expires_at, role 
                FROM users 
                WHERE id = {placeholder}
            """, (current_user['id'],))
            
            user_data = c.fetchone()
            
            if not user_data or user_data['role'] not in ['premium', 'admin']:
                raise HTTPException(status_code=404, detail="No active premium trial found")
            
            trial_expires_at = user_data['premium_expires_at']
            
            # Check if they already have a Stripe subscription
            try:
                customers = stripe.Customer.list(email=current_user['email'], limit=1)
                if customers.data:
                    customer = customers.data[0]
                    subscriptions = stripe.Subscription.list(
                        customer=customer.id,
                        status='active',
                        limit=10
                    )
                    if subscriptions.data:
                        raise HTTPException(status_code=400, detail="Active subscription already exists")
            except stripe.error.StripeError:
                # If Stripe lookup fails, continue with conversion
                pass
            
            # Create Stripe checkout session with trial period
            if pricing_tier == 'enterprise':
                price_id = os.getenv("STRIPE_ENTERPRISE_PRICE_ID", STRIPE_PRICE_ID)
            else:
                price_id = STRIPE_PRICE_ID
            
            base_url = "https://todo-events.com" if IS_PRODUCTION else "http://localhost:3000"
            
            # Calculate trial period if trial hasn't expired
            trial_period_days = None
            if trial_expires_at:
                from datetime import timezone
                if isinstance(trial_expires_at, str):
                    expires_at = datetime.fromisoformat(trial_expires_at.replace('Z', '+00:00'))
                else:
                    expires_at = trial_expires_at
                
                # Only add trial period if trial hasn't expired
                if expires_at > datetime.now(timezone.utc):
                    trial_days = max(1, (expires_at - datetime.now(timezone.utc)).days)
                    trial_period_days = min(trial_days, 30)  # Stripe max is 30 days
            
            subscription_data = {
                'metadata': {
                    'user_id': str(current_user['id']),
                    'user_email': current_user['email'],
                    'pricing_tier': pricing_tier,
                    'trial_conversion': 'true',
                    'trial_expires_at': trial_expires_at.isoformat() if trial_expires_at else ''
                }
            }
            
            if trial_period_days:
                subscription_data['trial_period_days'] = trial_period_days
            
            checkout_session = stripe.checkout.Session.create(
                payment_method_types=['card'],
                line_items=[{
                    'price': price_id,
                    'quantity': 1,
                }],
                mode='subscription',
                success_url=f"{base_url}/?session_id={{CHECKOUT_SESSION_ID}}&success=true&tier={pricing_tier}",
                cancel_url=f"{base_url}/subscription?cancelled=true",
                customer_email=current_user['email'],
                metadata={
                    'user_id': str(current_user['id']),
                    'user_email': current_user['email'],
                    'pricing_tier': pricing_tier,
                    'trial_conversion': 'true',
                    'trial_expires_at': trial_expires_at.isoformat() if trial_expires_at else ''
                },
                subscription_data=subscription_data
            )
            
            logger.info(f"‚úÖ Created {pricing_tier} trial conversion checkout session for user {current_user['id']} ({current_user['email']})")
            
            return {
                "checkout_url": checkout_session.url,
                "session_id": checkout_session.id,
                "pricing_tier": pricing_tier,
                "trial_period_days": trial_period_days
            }
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error converting trial to subscription: {str(e)}")
        raise HTTPException(status_code=500, detail="Failed to convert trial to subscription")

@app.get("/premium/trial-status")
async def get_premium_trial_status(current_user: dict = Depends(get_current_user)):
    """Get detailed trial status for current user"""
    try:
        placeholder = get_placeholder()
        
        with get_db() as conn:
            c = conn.cursor()
            
            # Get user trial information
            c.execute(f"""
                SELECT premium_expires_at, premium_granted_by, premium_invited, role
                FROM users 
                WHERE id = {placeholder}
            """, (current_user['id'],))
            
            user_data = c.fetchone()
            
            if not user_data:
                raise HTTPException(status_code=404, detail="User not found")
            
            # Check if they have any active Stripe subscriptions
            has_stripe_subscription = False
            try:
                customers = stripe.Customer.list(email=current_user['email'], limit=1)
                if customers.data:
                    customer = customers.data[0]
                    subscriptions = stripe.Subscription.list(
                        customer=customer.id,
                        status='active',
                        limit=10
                    )
                    has_stripe_subscription = len(subscriptions.data) > 0
            except stripe.error.StripeError:
                # If Stripe lookup fails, assume no subscription
                pass
            
            trial_info = None
            if user_data['premium_expires_at'] and not has_stripe_subscription:
                from datetime import timezone
                expires_at = user_data['premium_expires_at']
                if isinstance(expires_at, str):
                    expires_at = datetime.fromisoformat(expires_at.replace('Z', '+00:00'))
                
                trial_info = {
                    "is_trial": True,
                    "expires_at": expires_at.isoformat(),
                    "is_expired": expires_at < datetime.now(timezone.utc),
                    "days_remaining": max(0, (expires_at - datetime.now(timezone.utc)).days),
                    "granted_by": user_data.get('premium_granted_by'),
                    "was_invited": user_data.get('premium_invited', False),
                    "can_cancel": True,
                    "can_convert": True
                }
            
            return {
                "user_role": user_data['role'],
                "is_premium": user_data['role'] in ['premium', 'admin'],
                "has_stripe_subscription": has_stripe_subscription,
                "trial": trial_info
            }
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting trial status: {str(e)}")
        raise HTTPException(status_code=500, detail="Failed to get trial status")

# User account management endpoints
@app.post("/user/delete-account")
async def delete_user_account(current_user: dict = Depends(get_current_user)):
    """
    Allow users to delete their own account and all associated data
    This will:
    1. Cancel any active Stripe subscriptions
    2. Delete all user data (events, interests, views, etc.)
    3. Mark the account for deletion (30-day retention period)
    4. Send confirmation email
    """
    try:
        user_id = current_user['id']
        user_email = current_user['email']
        
        # Step 1: Cancel Stripe subscriptions if any exist
        stripe_cancellation_info = None
        try:
            customers = stripe.Customer.list(email=user_email, limit=1)
            if customers.data:
                customer = customers.data[0]
                
                # Get all active subscriptions
                subscriptions = stripe.Subscription.list(
                    customer=customer.id,
                    status='active',
                    limit=10
                )
                
                # Cancel all active subscriptions immediately
                for subscription in subscriptions.data:
                    canceled_sub = stripe.Subscription.modify(
                        subscription.id,
                        cancel_at_period_end=False  # Cancel immediately
                    )
                    logger.info(f"‚úÖ Cancelled Stripe subscription {subscription.id} for account deletion")
                
                stripe_cancellation_info = {
                    "subscriptions_cancelled": len(subscriptions.data),
                    "customer_id": customer.id
                }
        except stripe.error.StripeError as e:
            logger.warning(f"Stripe cancellation failed during account deletion: {str(e)}")
            # Continue with deletion even if Stripe fails
        
        # Step 2: Mark account for deletion and collect data to be deleted
        placeholder = get_placeholder()
        deletion_date = datetime.utcnow()
        deletion_scheduled = deletion_date + timedelta(days=30)
        
        deleted_items = {}
        
        with get_db_transaction() as conn:
            cursor = conn.cursor()
            
            # Count items before deletion for confirmation
            cursor.execute(f"SELECT COUNT(*) as count FROM events WHERE created_by = {placeholder}", (user_id,))
            deleted_items["events"] = cursor.fetchone()['count']
            
            cursor.execute(f"SELECT COUNT(*) as count FROM event_interests WHERE user_id = {placeholder}", (user_id,))
            deleted_items["interests"] = cursor.fetchone()['count']
            
            cursor.execute(f"SELECT COUNT(*) as count FROM event_views WHERE user_id = {placeholder}", (user_id,))
            deleted_items["views"] = cursor.fetchone()['count']
            
            cursor.execute(f"SELECT COUNT(*) as count FROM page_visits WHERE user_id = {placeholder}", (user_id,))
            deleted_items["page_visits"] = cursor.fetchone()['count']
            
            # Delete user's event-related data
            cursor.execute(f"DELETE FROM event_interests WHERE event_id IN (SELECT id FROM events WHERE created_by = {placeholder})", (user_id,))
            cursor.execute(f"DELETE FROM event_views WHERE event_id IN (SELECT id FROM events WHERE created_by = {placeholder})", (user_id,))
            cursor.execute(f"DELETE FROM events WHERE created_by = {placeholder}", (user_id,))
            
            # Delete user's interaction data
            cursor.execute(f"DELETE FROM event_interests WHERE user_id = {placeholder}", (user_id,))
            cursor.execute(f"DELETE FROM event_views WHERE user_id = {placeholder}", (user_id,))
            cursor.execute(f"DELETE FROM page_visits WHERE user_id = {placeholder}", (user_id,))
            
            # Delete any event reports made by this user
            cursor.execute(f"DELETE FROM event_reports WHERE reporter_email = {placeholder}", (user_email,))
            
            # Update user account to mark for deletion (instead of immediate deletion)
            # This allows for account recovery within 30 days
            cursor.execute(f"""
                UPDATE users 
                SET 
                    deleted_at = {placeholder},
                    deletion_scheduled_at = {placeholder},
                    role = 'deleted',
                    premium_expires_at = NULL,
                    premium_granted_by = NULL,
                    premium_invited = FALSE
                WHERE id = {placeholder}
            """, (deletion_date.isoformat(), deletion_scheduled.isoformat(), user_id))
            
            conn.commit()
        
        # Step 3: Log the deletion activity
        log_activity(user_id, "account_deletion_request", f"User requested account deletion")
        
        # Step 4: Send account deletion confirmation email
        try:
            from email_config import email_service
            user_name = user_email.split('@')[0]
            email_sent = email_service.send_account_deletion_email(
                to_email=user_email,
                user_name=user_name,
                deletion_date=deletion_date.isoformat(),
                final_deletion_date=deletion_scheduled.isoformat(),
                deleted_items=deleted_items,
                stripe_info=stripe_cancellation_info
            )
            
            if email_sent:
                logger.info(f"‚úÖ Account deletion confirmation email sent to {user_email}")
            else:
                logger.error(f"‚ùå Failed to send account deletion confirmation email to {user_email}")
        except Exception as e:
            logger.error(f"‚ùå Error sending account deletion email: {str(e)}")
        
        # Clear any cached data for this user
        try:
            event_cache.clear()  # Clear all cache since user events are deleted
        except:
            pass
        
        logger.info(f"‚úÖ Account deletion completed for user {user_id} ({user_email})")
        
        return {
            "success": True,
            "message": "Account deletion completed successfully",
            "deletion_date": deletion_date.isoformat(),
            "final_deletion_date": deletion_scheduled.isoformat(),
            "deleted_items": deleted_items,
            "stripe_cancellations": stripe_cancellation_info,
            "recovery_period_days": 30
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error deleting user account {current_user['id']}: {str(e)}")
        raise HTTPException(status_code=500, detail="Failed to delete account")

@app.post("/user/cancel-account-deletion")
async def cancel_account_deletion(current_user: dict = Depends(get_current_user)):
    """
    Allow users to cancel their account deletion within the 30-day period
    """
    try:
        user_id = current_user['id']
        user_email = current_user['email']
        
        placeholder = get_placeholder()
        
        with get_db() as conn:
            cursor = conn.cursor()
            
            # Check if account is marked for deletion
            cursor.execute(f"""
                SELECT deleted_at, deletion_scheduled_at, role 
                FROM users 
                WHERE id = {placeholder}
            """, (user_id,))
            
            user_data = cursor.fetchone()
            
            if not user_data or user_data['role'] != 'deleted':
                raise HTTPException(status_code=400, detail="Account is not scheduled for deletion")
            
            # Check if we're still within the recovery period
            if user_data['deletion_scheduled_at']:
                scheduled_deletion = datetime.fromisoformat(user_data['deletion_scheduled_at'].replace('Z', '+00:00'))
                if datetime.utcnow() >= scheduled_deletion:
                    raise HTTPException(status_code=400, detail="Recovery period has expired")
            
            # Restore the account
            cursor.execute(f"""
                UPDATE users 
                SET 
                    deleted_at = NULL,
                    deletion_scheduled_at = NULL,
                    role = 'user'
                WHERE id = {placeholder}
            """, (user_id,))
            
            conn.commit()
        
        # Log the recovery
        log_activity(user_id, "account_deletion_cancelled", f"User cancelled account deletion")
        
        # Send recovery confirmation email
        try:
            from email_config import email_service
            user_name = user_email.split('@')[0]
            email_sent = email_service.send_account_recovery_email(
                to_email=user_email,
                user_name=user_name
            )
            
            if email_sent:
                logger.info(f"‚úÖ Account recovery confirmation email sent to {user_email}")
        except Exception as e:
            logger.error(f"‚ùå Error sending account recovery email: {str(e)}")
        
        logger.info(f"‚úÖ Account deletion cancelled for user {user_id} ({user_email})")
        
        return {
            "success": True,
            "message": "Account deletion cancelled successfully",
            "restored_at": datetime.utcnow().isoformat()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error cancelling account deletion for user {current_user['id']}: {str(e)}")
        raise HTTPException(status_code=500, detail="Failed to cancel account deletion")

@app.get("/stripe/config")
async def get_stripe_config():
    """Get Stripe publishable key for frontend"""
    return {"publishable_key": STRIPE_PUBLISHABLE_KEY}
@app.post("/stripe/create-checkout-session")
async def create_checkout_session(request: Request, current_user: dict = Depends(get_current_user)):
    """Create Stripe checkout session for premium subscription"""
    try:
        # Get pricing tier from request (default to monthly)
        # Parse JSON body
        try:
            body = await request.json()
        except Exception:
            body = {}
        
        # Get pricing tier from request (default to monthly)
        pricing_tier = body.get('pricing_tier', 'monthly')
        trial_ends_at = body.get('trial_ends_at')
        
        # Determine which price ID to use
        if pricing_tier == 'annual':
            price_id = os.getenv("STRIPE_ANNUAL_PRICE_ID", STRIPE_PRICE_ID)
        elif pricing_tier == 'enterprise':
            price_id = os.getenv("STRIPE_ENTERPRISE_PRICE_ID", STRIPE_PRICE_ID)
        else:  # monthly or premium
            price_id = STRIPE_PRICE_ID
        
        # Determine the base URL for success/cancel URLs
        base_url = "https://todo-events.com" if IS_PRODUCTION else "http://localhost:5173"
        
        # Configure subscription data based on trial status
        subscription_data = {
            'metadata': {
                'user_id': str(current_user['id']),
                'user_email': current_user['email'],
                'pricing_tier': pricing_tier
            }
        }
        
        # If trial is provided and not expired, schedule subscription to start after trial
        if trial_ends_at:
            try:
                from datetime import timezone
                trial_end_date = datetime.fromisoformat(trial_ends_at.replace('Z', '+00:00'))
                
                # Only schedule future billing if trial hasn't expired
                if trial_end_date > datetime.now(timezone.utc):
                    # Calculate trial period in days (Stripe requires this for trial_period_days)
                    trial_days = max(1, (trial_end_date - datetime.now(timezone.utc)).days)
                    subscription_data['trial_period_days'] = min(trial_days, 30)  # Stripe max is 30 days
                    logger.info(f"üïí Setting {trial_days} day trial for user {current_user['id']}")
                else:
                    logger.info(f"‚è∞ Trial already expired for user {current_user['id']}, starting subscription immediately")
            except Exception as e:
                logger.warning(f"Could not parse trial date {trial_ends_at}: {e}")
        
        checkout_session = stripe.checkout.Session.create(
            payment_method_types=['card'],
            line_items=[{
                'price': price_id,
                'quantity': 1,
            }],
            mode='subscription',
            success_url=f"{base_url}/?session_id={{CHECKOUT_SESSION_ID}}&success=true&tier={pricing_tier}",
            cancel_url=f"{base_url}/subscription?cancelled=true",
            customer_email=current_user['email'],
            metadata={
                'user_id': str(current_user['id']),
                'user_email': current_user['email'],
                'pricing_tier': pricing_tier,
                'trial_ends_at': trial_ends_at or ''
            },
            subscription_data=subscription_data
        )
        
        logger.info(f"‚úÖ Created {pricing_tier} checkout session for user {current_user['id']} ({current_user['email']})")
        
        # Ensure we have a checkout URL
        if not checkout_session.url:
            logger.error(f"Checkout session created but no URL returned: {checkout_session.id}")
            raise HTTPException(status_code=500, detail="No checkout URL received from Stripe")
            
        return {"checkout_url": checkout_session.url}
        
    except stripe.error.StripeError as e:
        logger.error(f"Stripe error creating checkout session: {str(e)}")
        raise HTTPException(status_code=400, detail=f"Stripe error: {str(e)}")
    except Exception as e:
        logger.error(f"Error creating checkout session: {str(e)}")
        logger.error(f"Full traceback: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail="Failed to create checkout session")

@app.post("/stripe/cancel-subscription")
async def cancel_subscription(current_user: dict = Depends(get_current_user)):
    """Cancel user's active subscription"""
    try:
        # First, find the user's active subscription in Stripe
        customers = stripe.Customer.list(email=current_user['email'], limit=1)
        
        if not customers.data:
            raise HTTPException(status_code=404, detail="No Stripe customer found")
        
        customer = customers.data[0]
        
        # Get active subscriptions for this customer
        subscriptions = stripe.Subscription.list(
            customer=customer.id,
            status='active',
            limit=10
        )
        
        if not subscriptions.data:
            raise HTTPException(status_code=404, detail="No active subscription found")
        
        # Cancel the first active subscription (assuming one subscription per customer)
        subscription = subscriptions.data[0]
        
        # Cancel at period end (so they keep access until billing period ends)
        canceled_subscription = stripe.Subscription.modify(
            subscription.id,
            cancel_at_period_end=True
        )
        
        logger.info(f"‚úÖ Marked subscription {subscription.id} for cancellation for user {current_user['id']} ({current_user['email']})")
        
        # Get the correct access_until date using the same logic as subscription status
        access_until = None
        try:
            # Try to get the next billing date from upcoming invoice
            upcoming = stripe.Invoice.create_preview(customer=customer.id, subscription=subscription.id)
            if upcoming and hasattr(upcoming, 'lines') and upcoming.lines and upcoming.lines.data:
                for line in upcoming.lines.data:
                    if hasattr(line, 'period') and line.period and hasattr(line.period, 'start') and line.period.start:
                        access_until = datetime.fromtimestamp(line.period.start).isoformat()
                        logger.info(f"üóìÔ∏è Subscription will end at: {access_until}")
                        break
        except Exception as e:
            logger.warning(f"Could not get upcoming invoice for cancellation date: {e}")
            # Fallback to subscription's current period end if available
            try:
                cancels_at_ts = getattr(canceled_subscription, 'current_period_end', None)
                if cancels_at_ts:
                    access_until = datetime.fromtimestamp(cancels_at_ts).isoformat()
                    logger.info(f"üóìÔ∏è Using subscription current_period_end: {access_until}")
            except (ValueError, TypeError, OSError):
                logger.warning(f"Could not convert cancellation timestamp")
        
        # Send cancellation confirmation email
        try:
            from email_config import email_service
            user_name = current_user['email'].split('@')[0]
            email_sent = email_service.send_subscription_cancellation_email(
                to_email=current_user['email'],
                user_name=user_name,
                cancellation_type="scheduled",
                effective_date=access_until
            )
            
            if email_sent:
                logger.info(f"‚úÖ Cancellation confirmation email sent to {current_user['email']}")
            else:
                logger.error(f"‚ùå Failed to send cancellation confirmation email to {current_user['email']}")
        except Exception as e:
            logger.error(f"‚ùå Error sending cancellation email: {str(e)}")
        
        return {
            "success": True,
            "message": "Subscription marked for cancellation",
            "subscription_id": subscription.id,
            "access_until": access_until
        }
        
    except stripe.error.StripeError as e:
        logger.error(f"Stripe error canceling subscription: {str(e)}")
        raise HTTPException(status_code=400, detail=f"Stripe error: {str(e)}")
    except Exception as e:
        logger.error(f"Error canceling subscription: {str(e)}")
        raise HTTPException(status_code=500, detail="Failed to cancel subscription")

@app.post("/stripe/cancel-subscription-immediately")
async def cancel_subscription_immediately(current_user: dict = Depends(get_current_user)):
    """Cancel user's subscription immediately (loses access right away)"""
    try:
        # Find the user's active subscription
        customers = stripe.Customer.list(email=current_user['email'], limit=1)
        
        if not customers.data:
            raise HTTPException(status_code=404, detail="No Stripe customer found")
        
        customer = customers.data[0]
        
        # Get active subscriptions
        subscriptions = stripe.Subscription.list(
            customer=customer.id,
            status='active',
            limit=10
        )
        
        if not subscriptions.data:
            raise HTTPException(status_code=404, detail="No active subscription found")
        
        subscription = subscriptions.data[0]
        
        # Cancel immediately
        canceled_subscription = stripe.Subscription.cancel(subscription.id)
        
        logger.info(f"‚úÖ Immediately canceled subscription {subscription.id} for user {current_user['id']} ({current_user['email']})")
        
        # Send immediate cancellation confirmation email
        try:
            from email_config import email_service
            user_name = current_user['email'].split('@')[0]
            email_sent = email_service.send_subscription_cancellation_email(
                to_email=current_user['email'],
                user_name=user_name,
                cancellation_type="immediate"
            )
            
            if email_sent:
                logger.info(f"‚úÖ Immediate cancellation email sent to {current_user['email']}")
            else:
                logger.error(f"‚ùå Failed to send immediate cancellation email to {current_user['email']}")
        except Exception as e:
            logger.error(f"‚ùå Error sending immediate cancellation email: {str(e)}")
        
        return {
            "success": True,
            "message": "Subscription canceled immediately",
            "subscription_id": subscription.id,
            "status": canceled_subscription.status
        }
        
    except stripe.error.StripeError as e:
        logger.error(f"Stripe error canceling subscription: {str(e)}")
        raise HTTPException(status_code=400, detail=f"Stripe error: {str(e)}")
    except Exception as e:
        logger.error(f"Error canceling subscription immediately: {str(e)}")
        raise HTTPException(status_code=500, detail="Failed to cancel subscription")

@app.post("/stripe/reactivate-subscription")
async def reactivate_subscription(request: dict, current_user: dict = Depends(get_current_user)):
    """Reactivate a subscription that was scheduled for cancellation"""
    try:
        subscription_id = request.get('subscription_id')
        if not subscription_id:
            raise HTTPException(status_code=400, detail="subscription_id is required")
        
        # Verify the subscription belongs to this user
        customers = stripe.Customer.list(email=current_user['email'], limit=1)
        if not customers.data:
            raise HTTPException(status_code=404, detail="No Stripe customer found")
        
        customer = customers.data[0]
        
        # Get the subscription to verify ownership
        try:
            subscription = stripe.Subscription.retrieve(subscription_id)
            if subscription.customer != customer.id:
                raise HTTPException(status_code=403, detail="Subscription does not belong to this customer")
        except stripe.error.InvalidRequestError:
            raise HTTPException(status_code=404, detail="Subscription not found")
        
        # Check if subscription is scheduled for cancellation
        if not subscription.cancel_at_period_end:
            raise HTTPException(status_code=400, detail="Subscription is not scheduled for cancellation")
        
        # Reactivate the subscription by removing the cancellation
        reactivated_subscription = stripe.Subscription.modify(
            subscription_id,
            cancel_at_period_end=False
        )
        
        logger.info(f"‚úÖ Reactivated subscription {subscription_id} for user {current_user['id']} ({current_user['email']})")
        
        # Send reactivation confirmation email
        try:
            from email_config import email_service
            user_name = current_user['email'].split('@')[0]
            
            # Get next billing date for email
            next_billing_date = None
            try:
                upcoming = stripe.Invoice.create_preview(customer=customer.id, subscription=subscription_id)
                if upcoming and hasattr(upcoming, 'lines') and upcoming.lines and upcoming.lines.data:
                    for line in upcoming.lines.data:
                        if hasattr(line, 'period') and line.period and hasattr(line.period, 'start') and line.period.start:
                            next_billing_date = datetime.fromtimestamp(line.period.start).strftime('%B %d, %Y')
                            break
            except Exception:
                pass
            
            email_content = f"""
            <h2>üéâ Subscription Reactivated - TodoEvents</h2>
            <p>Great news! Your TodoEvents premium subscription has been successfully reactivated.</p>
            <p><strong>What this means:</strong></p>
            <ul>
                <li>‚úÖ Your scheduled cancellation has been removed</li>
                <li>‚úÖ You'll continue to have premium access</li>
                <li>‚úÖ Billing will continue as normal</li>
                {f"<li>üìÖ Your next billing date: {next_billing_date}</li>" if next_billing_date else ""}
            </ul>
            <p>Thank you for staying with TodoEvents Premium!</p>
            """
            
            email_service.send_email(
                current_user['email'],
                "üéâ Subscription Reactivated - TodoEvents",
                email_content
            )
            logger.info(f"‚úÖ Sent reactivation email to {current_user['email']}")
            
        except Exception as e:
            logger.error(f"‚ùå Error sending reactivation email: {str(e)}")
        
        return {
            "success": True,
            "message": "Subscription reactivated successfully",
            "subscription_id": subscription_id
        }
        
    except stripe.error.StripeError as e:
        logger.error(f"Stripe error reactivating subscription: {str(e)}")
        raise HTTPException(status_code=400, detail=f"Stripe error: {str(e)}")
    except Exception as e:
        logger.error(f"Error reactivating subscription: {str(e)}")
        raise HTTPException(status_code=500, detail="Failed to reactivate subscription")

@app.get("/stripe/subscription-status")
async def get_detailed_subscription_status(current_user: dict = Depends(get_current_user)):
    """Get detailed subscription status from Stripe"""
    try:
        # Find customer in Stripe
        customers = stripe.Customer.list(email=current_user['email'], limit=1)
        
        if not customers.data:
            # Check for trial information from database even if no Stripe customer
            trial_info = None
            placeholder = get_placeholder()
            try:
                with get_db() as conn:
                    c = conn.cursor()
                    c.execute(f"""
                        SELECT premium_expires_at, premium_granted_by, premium_invited 
                        FROM users 
                        WHERE id = {placeholder}
                    """, (current_user['id'],))
                    user_data = c.fetchone()
                    
                    if user_data and user_data.get('premium_expires_at'):
                        from datetime import timezone
                        expires_at = user_data['premium_expires_at']
                        if isinstance(expires_at, str):
                            expires_at = datetime.fromisoformat(expires_at.replace('Z', '+00:00'))
                        
                        trial_info = {
                            "is_trial": True,
                            "expires_at": expires_at.isoformat(),
                            "is_expired": expires_at < datetime.now(timezone.utc),
                            "days_remaining": max(0, (expires_at - datetime.now(timezone.utc)).days),
                            "granted_by": user_data.get('premium_granted_by'),
                            "was_invited": user_data.get('premium_invited', False)
                        }
            except Exception as e:
                logger.warning(f"Could not fetch trial info: {e}")
            
            return {
                "has_stripe_customer": False,
                "user_role": current_user['role'],
                "is_premium": current_user['role'] in ['premium', 'admin'],
                "trial": trial_info
            }
        
        customer = customers.data[0]
        
        # Get all subscriptions for this customer
        subscriptions = stripe.Subscription.list(
            customer=customer.id,
            limit=10
        )
        
        subscription_info = []
        for sub in subscriptions.data:
            # Debug: Log subscription object structure
            logger.info(f"Processing subscription: {sub.id}, status: {sub.status}")
            logger.info(f"Available subscription attributes: {list(sub.keys()) if hasattr(sub, 'keys') else 'No keys method'}")
            
            # Get the actual subscription's current period (what they've already paid for)
            actual_period_start = None
            actual_period_end = None
            next_billing_date = None
            
            # Get actual current period from subscription object first
            try:
                current_period_start_ts = getattr(sub, 'current_period_start', None)
                if current_period_start_ts:
                    actual_period_start = datetime.fromtimestamp(current_period_start_ts).isoformat()
                    logger.info(f"üïí Actual current period start: {actual_period_start} (ts: {current_period_start_ts})")
            except (ValueError, TypeError, OSError, AttributeError):
                pass
                
            try:
                current_period_end_ts = getattr(sub, 'current_period_end', None)
                if current_period_end_ts:
                    actual_period_end = datetime.fromtimestamp(current_period_end_ts).isoformat()
                    logger.info(f"üïí Actual current period end: {actual_period_end} (ts: {current_period_end_ts})")
            except (ValueError, TypeError, OSError, AttributeError):
                pass
            
            # Now get the next billing date from upcoming invoice (separate from current period)
            try:
                logger.info(f"Attempting to fetch upcoming invoice preview for customer: {customer.id}")
                upcoming = stripe.Invoice.create_preview(customer=customer.id, subscription=sub.id)
                if upcoming:
                    if hasattr(upcoming, 'lines') and upcoming.lines and upcoming.lines.data:
                        for line in upcoming.lines.data:
                            if hasattr(line, 'period') and line.period:
                                if hasattr(line.period, 'start') and line.period.start:
                                    next_billing_date = datetime.fromtimestamp(line.period.start).isoformat()
                                    logger.info(f"üìÖ Next billing date from upcoming invoice: {next_billing_date}")
                                break
                    
                    # Also log the amount for debugging
                    if hasattr(upcoming, 'amount_due'):
                        logger.info(f"üí∞ Next invoice amount: {upcoming.amount_due}")
                        
            except Exception as upcoming_error:
                logger.warning(f"‚ö†Ô∏è Could not fetch upcoming invoice preview: {upcoming_error}")
                # Fallback: use current period end as next billing date if we can't get upcoming invoice
                next_billing_date = actual_period_end
            
            # Fallback for missing actual period data
            if not actual_period_start:
                try:
                    start_date_ts = getattr(sub, 'start_date', None)
                    if start_date_ts:
                        actual_period_start = datetime.fromtimestamp(start_date_ts).isoformat()
                except (ValueError, TypeError, OSError, AttributeError):
                    try:
                        created_ts = getattr(sub, 'created', None)
                        if created_ts:
                            actual_period_start = datetime.fromtimestamp(created_ts).isoformat()
                    except (ValueError, TypeError, OSError, AttributeError):
                        pass
                        
            if not actual_period_end:
                # Final fallback to latest invoice if we couldn't get subscription period
                try:
                    latest_invoice_id = getattr(sub, 'latest_invoice', None)
                    if latest_invoice_id:
                        logger.info(f"Fetching latest invoice as fallback: {latest_invoice_id}")
                        invoice = stripe.Invoice.retrieve(latest_invoice_id)
                        if invoice:
                            if hasattr(invoice, 'period_end') and invoice.period_end:
                                actual_period_end = datetime.fromtimestamp(invoice.period_end).isoformat()
                                logger.info(f"Got actual period end from latest invoice: {actual_period_end}")
                except Exception as invoice_error:
                    logger.warning(f"Could not fetch latest invoice data: {invoice_error}")
                
            try:
                canceled_at_ts = getattr(sub, 'canceled_at', None)
                canceled_at = datetime.fromtimestamp(canceled_at_ts).isoformat() if canceled_at_ts else None
            except (ValueError, TypeError, OSError, AttributeError):
                canceled_at = None
            
            # Check if subscription is scheduled for cancellation and get the actual end date
            cancel_at_period_end = getattr(sub, 'cancel_at_period_end', False)
            if cancel_at_period_end:
                # For scheduled cancellations, use the actual current period end
                logger.info(f"üóìÔ∏è Subscription scheduled for cancellation, access until: {actual_period_end}")
            
            # Safely get price information from multiple possible sources
            amount = 0
            currency = "usd"
            
            # Try items.data[0].price first (newer Stripe API)
            try:
                if hasattr(sub, 'items') and sub.items and sub.items.data:
                    if hasattr(sub.items.data[0], 'price') and sub.items.data[0].price:
                        amount = getattr(sub.items.data[0].price, 'unit_amount', 0) or 0
                        currency = getattr(sub.items.data[0].price, 'currency', 'usd') or 'usd'
            except (AttributeError, IndexError):
                pass
            
            # Fallback to plan object (older Stripe API)
            if amount == 0:
                try:
                    plan = getattr(sub, 'plan', None)
                    if plan:
                        amount = getattr(plan, 'amount', 0) or 0
                        currency = getattr(plan, 'currency', 'usd') or 'usd'
                except (AttributeError, TypeError):
                    pass
            
            # Log what we found for debugging
            logger.info(f"Subscription pricing: amount={amount}, currency={currency}")
            logger.info(f"Actual period: start={actual_period_start}, end={actual_period_end}")
            logger.info(f"Next billing date: {next_billing_date}")
            
            subscription_info.append({
                "id": getattr(sub, 'id', 'unknown'),
                "status": getattr(sub, 'status', 'unknown'),
                "current_period_start": next_billing_date,  # When they'll be charged next
                "current_period_end": actual_period_end,   # When their current paid period ends (for cancellation)
                "cancel_at_period_end": getattr(sub, 'cancel_at_period_end', False),
                "canceled_at": canceled_at,
                "amount": amount,
                "currency": currency
            })
        
        # Check for trial information from database
        trial_info = None
        placeholder = get_placeholder()
        try:
            with get_db() as conn:
                c = conn.cursor()
                c.execute(f"""
                    SELECT premium_expires_at, premium_granted_by, premium_invited 
                    FROM users 
                    WHERE id = {placeholder}
                """, (current_user['id'],))
                user_data = c.fetchone()
                
                if user_data and user_data.get('premium_expires_at'):
                    from datetime import timezone
                    expires_at = user_data['premium_expires_at']
                    if isinstance(expires_at, str):
                        expires_at = datetime.fromisoformat(expires_at.replace('Z', '+00:00'))
                    
                    # Check if this looks like a trial (no Stripe subscription but has expiration)
                    is_trial = len(subscription_info) == 0 and expires_at
                    
                    if is_trial:
                        trial_info = {
                            "is_trial": True,
                            "expires_at": expires_at.isoformat(),
                            "is_expired": expires_at < datetime.now(timezone.utc),
                            "days_remaining": max(0, (expires_at - datetime.now(timezone.utc)).days),
                            "granted_by": user_data.get('premium_granted_by'),
                            "was_invited": user_data.get('premium_invited', False)
                        }
        except Exception as e:
            logger.warning(f"Could not fetch trial info: {e}")

        return {
            "has_stripe_customer": True,
            "customer_id": customer.id,
            "user_role": current_user['role'],
            "is_premium": current_user['role'] in ['premium', 'admin'],
            "subscriptions": subscription_info,
            "trial": trial_info
        }
        
    except Exception as e:
        logger.error(f"Error getting subscription status: {str(e)}")
        logger.error(f"Subscription status error traceback: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"Failed to get subscription status: {str(e)}")

@app.get("/test-webhook")
async def test_webhook():
    """Simple test endpoint to verify webhook URL is reachable"""
    return {"status": "ok", "message": "Webhook endpoint is reachable", "timestamp": datetime.utcnow().isoformat()}

@app.get("/debug/webhook-hits")
async def check_webhook_hits():
    """Check if webhook has ever been hit by reading debug file"""
    try:
        if os.path.exists("/tmp/webhook_debug.log"):
            with open("/tmp/webhook_debug.log", "r") as f:
                hits = f.readlines()
            return {
                "webhook_ever_hit": True,
                "total_hits": len(hits),
                "last_10_hits": hits[-10:] if hits else [],
                "latest_hit": hits[-1].strip() if hits else None
            }
        else:
            return {
                "webhook_ever_hit": False,
                "total_hits": 0,
                "message": "Webhook has never been hit or debug file doesn't exist"
            }
    except Exception as e:
        return {"error": str(e)}

@app.get("/debug/user-status/{user_id}")
async def debug_user_status_simple(user_id: int):
    """Quick user status check - no auth required for debugging"""
    placeholder = get_placeholder()
    try:
        with get_db() as conn:
            cursor = conn.cursor()
            cursor.execute(f"SELECT id, email, role, premium_expires_at FROM users WHERE id = {placeholder}", (user_id,))
            user = cursor.fetchone()
            
            if not user:
                return {"error": "User not found"}
            
            return {
                "user_id": user['id'],
                "email": user['email'],
                "role": user['role'],
                "premium_expires_at": user.get('premium_expires_at'),
                "is_premium": user['role'] in ['premium', 'admin'],
                "current_utc": datetime.utcnow().isoformat()
            }
    except Exception as e:
        return {"error": str(e)}

@app.post("/admin/quick-upgrade-user-3")
async def quick_upgrade_user_3():
    """Quick upgrade for user 3 since payment was successful but webhook may have been missed"""
    placeholder = get_placeholder()
    try:
        with get_db() as conn:
            cursor = conn.cursor()
            
            # Check current status
            cursor.execute(f"SELECT id, email, role FROM users WHERE id = {placeholder}", (3,))
            user = cursor.fetchone()
            
            if not user:
                return {"error": "User 3 not found"}
            
            if user['role'] == 'premium':
                return {"message": "User 3 is already premium", "role": user['role']}
            
            # Calculate expiration date (1 month from now) using UTC
            expires_at = datetime.utcnow() + timedelta(days=30)
            
            # Update user to premium
            cursor.execute(f"""
                UPDATE users 
                SET role = 'premium', 
                    premium_expires_at = {placeholder}
                WHERE id = {placeholder}
            """, (expires_at, 3))
            
            rows_affected = cursor.rowcount
            conn.commit()
            
            logger.info(f"‚úÖ Manually upgraded user 3 ({user['email']}) to premium due to webhook processing issues")
            
            return {
                "success": True,
                "message": f"User 3 ({user['email']}) upgraded to premium",
                "previous_role": user['role'],
                "new_role": "premium",
                "expires_at": expires_at.isoformat(),
                "rows_affected": rows_affected
            }
            
    except Exception as e:
        logger.error(f"Error upgrading user 3: {str(e)}")
        return {"error": str(e)}

@app.post("/stripe/webhook")
async def stripe_webhook(request: Request):
    """Handle Stripe webhook events"""
    # FORCE IMMEDIATE LOGGING WITH MULTIPLE METHODS
    import sys
    import os
    
    # Method 1: Logger
    logger.info("üîîüîîüîî WEBHOOK ENDPOINT HIT! üîîüîîüîî")
    logger.warning("üîîüîîüîî WEBHOOK ENDPOINT HIT! üîîüîîüîî")
    logger.error("üîîüîîüîî WEBHOOK ENDPOINT HIT! üîîüîîüîî")
    
    # Method 2: Print to stdout
    print("üîîüîîüîî WEBHOOK ENDPOINT HIT! üîîüîîüîî")
    
    # Method 3: Print to stderr  
    print("üîîüîîüîî WEBHOOK ENDPOINT HIT! üîîüîîüîî", file=sys.stderr)
    
    # Method 4: Force flush
    sys.stdout.flush()
    sys.stderr.flush()
    
    # Method 5: Write to a file for debugging
    try:
        with open("/tmp/webhook_debug.log", "a") as f:
            f.write(f"WEBHOOK HIT: {datetime.utcnow().isoformat()}\n")
            f.flush()
    except:
        pass
    
    try:
        payload = await request.body()
        sig_header = request.headers.get('stripe-signature')
        
        logger.info(f"üîî Webhook payload size: {len(payload)}, signature present: {bool(sig_header)}")
        print(f"üîî Webhook payload size: {len(payload)}, signature present: {bool(sig_header)}")
        
        if not STRIPE_WEBHOOK_SECRET:
            logger.error("‚ùå STRIPE_WEBHOOK_SECRET not configured")
            print("‚ùå STRIPE_WEBHOOK_SECRET not configured")
            raise HTTPException(status_code=500, detail="Webhook secret not configured")
        
        logger.info(f"üîî Attempting to construct Stripe event with secret: {STRIPE_WEBHOOK_SECRET[:8]}...")
        print(f"üîî Attempting to construct Stripe event with secret: {STRIPE_WEBHOOK_SECRET[:8]}...")
        
        event = stripe.Webhook.construct_event(
            payload, sig_header, STRIPE_WEBHOOK_SECRET
        )
        
        logger.info(f"üîî SUCCESS! Received Stripe webhook: {event['type']} (ID: {event.get('id', 'unknown')})")
        print(f"üîî SUCCESS! Received Stripe webhook: {event['type']} (ID: {event.get('id', 'unknown')})")
        
        # Handle the event
        if event['type'] == 'checkout.session.completed':
            session = event['data']['object']
            logger.info(f"üîî Processing checkout.session.completed for session: {session.get('id')}")
            print(f"üîî Processing checkout.session.completed for session: {session.get('id')}")
            await handle_successful_payment(session)
        elif event['type'] == 'customer.subscription.created':
            subscription = event['data']['object']
            logger.info(f"üîî Processing customer.subscription.created for subscription: {subscription.get('id')}")
            print(f"üîî Processing customer.subscription.created for subscription: {subscription.get('id')}")
            await handle_subscription_created(subscription)
        elif event['type'] == 'invoice.payment_succeeded':
            invoice = event['data']['object']
            logger.info(f"üîî Processing invoice.payment_succeeded for invoice: {invoice.get('id')}")
            print(f"üîî Processing invoice.payment_succeeded for invoice: {invoice.get('id')}")
            await handle_subscription_renewal(invoice)
        elif event['type'] == 'customer.subscription.deleted':
            subscription = event['data']['object']
            logger.info(f"üîî Processing customer.subscription.deleted for subscription: {subscription.get('id')}")
            print(f"üîî Processing customer.subscription.deleted for subscription: {subscription.get('id')}")
            await handle_subscription_cancelled(subscription)
        elif event['type'] == 'customer.subscription.paused':
            subscription = event['data']['object']
            logger.info(f"üîî Processing customer.subscription.paused for subscription: {subscription.get('id')}")
            print(f"üîî Processing customer.subscription.paused for subscription: {subscription.get('id')}")
            await handle_subscription_paused(subscription)
        elif event['type'] == 'customer.subscription.updated':
            subscription = event['data']['object']
            logger.info(f"üîî Processing customer.subscription.updated for subscription: {subscription.get('id')}")
            print(f"üîî Processing customer.subscription.updated for subscription: {subscription.get('id')}")
            await handle_subscription_updated(subscription)
        elif event['type'] == 'customer.subscription.resumed':
            subscription = event['data']['object']
            logger.info(f"üîî Processing customer.subscription.resumed for subscription: {subscription.get('id')}")
            print(f"üîî Processing customer.subscription.resumed for subscription: {subscription.get('id')}")
            await handle_subscription_resumed(subscription)
        elif event['type'] == 'invoice.created':
            invoice = event['data']['object']
            logger.info(f"üîî Processing invoice.created for invoice: {invoice.get('id')}")
            print(f"üîî Processing invoice.created for invoice: {invoice.get('id')}")
            await handle_invoice_created(invoice)
        elif event['type'] == 'invoice.upcoming':
            invoice = event['data']['object']
            logger.info(f"üîî Processing invoice.upcoming for invoice: {invoice.get('id')}")
            print(f"üîî Processing invoice.upcoming for invoice: {invoice.get('id')}")
            await handle_invoice_upcoming(invoice)
        else:
            logger.info(f"‚ÑπÔ∏è Unhandled event type: {event['type']}")
            print(f"‚ÑπÔ∏è Unhandled event type: {event['type']}")
        
        logger.info(f"‚úÖ Successfully processed webhook {event['type']}")
        print(f"‚úÖ Successfully processed webhook {event['type']}")
        return {"status": "success"}
        
    except ValueError as e:
        logger.error(f"‚ùå Invalid webhook payload: {e}")
        print(f"‚ùå Invalid webhook payload: {e}")
        raise HTTPException(status_code=400, detail="Invalid payload")
    except stripe.error.SignatureVerificationError as e:
        logger.error(f"‚ùå Invalid webhook signature: {e}")
        print(f"‚ùå Invalid webhook signature: {e}")
        raise HTTPException(status_code=400, detail="Invalid signature")
    except Exception as e:
        logger.error(f"‚ùå Webhook processing error: {str(e)}")
        logger.error(f"‚ùå Webhook traceback: {traceback.format_exc()}")
        print(f"‚ùå Webhook processing error: {str(e)}")
        print(f"‚ùå Webhook traceback: {traceback.format_exc()}")
        # Return 200 to prevent Stripe from retrying
        return {"status": "error", "message": str(e)}

async def handle_successful_payment(session):
    """Handle successful payment from Stripe checkout"""
    try:
        logger.info(f"üîî Processing successful payment for session: {session.get('id')}")
        logger.info(f"üîî Session metadata: {session.get('metadata', {})}")
        
        # Validate metadata exists
        if 'metadata' not in session or not session['metadata']:
            logger.error("‚ùå No metadata found in session")
            raise ValueError("Session metadata missing")
        
        if 'user_id' not in session['metadata']:
            logger.error("‚ùå user_id not found in session metadata")
            raise ValueError("user_id missing from metadata")
        
        user_id = int(session['metadata']['user_id'])
        user_email = session['metadata']['user_email']
        pricing_tier = session['metadata'].get('pricing_tier', 'monthly')
        
        logger.info(f"üîî Processing payment for user {user_id} ({user_email}) - pricing tier: {pricing_tier}")
        
        # Determine role based on pricing tier
        if pricing_tier == 'enterprise':
            new_role = 'enterprise'
            # Enterprise gets 1 month access (monthly subscription)
            expires_at = datetime.utcnow() + timedelta(days=30)
        else:
            new_role = 'premium'
            # Premium gets 1 month access
            expires_at = datetime.utcnow() + timedelta(days=30)
        
        # Update user to appropriate role
        with get_db_transaction() as conn:
            cursor = conn.cursor()
            placeholder = get_placeholder()
            
            # First check if user exists
            cursor.execute(f"SELECT id, email, role FROM users WHERE id = {placeholder}", (user_id,))
            user = cursor.fetchone()
            
            if not user:
                logger.error(f"‚ùå User {user_id} not found in database")
                raise ValueError(f"User {user_id} not found")
            
            logger.info(f"üîî Found user: {user['email']}, current role: {user['role']}, upgrading to: {new_role}")
            
            # Update user role and expiration
            cursor.execute(f"""
                UPDATE users 
                SET role = {placeholder}, premium_expires_at = {placeholder}
                WHERE id = {placeholder}
            """, (new_role, expires_at, user_id))
            
            rows_affected = cursor.rowcount
            logger.info(f"üîî Update query affected {rows_affected} rows")
            
            conn.commit()
            
            # Log the activity
            log_activity(user_id, "stripe_payment_success", f"{new_role.title()} subscription activated via Stripe for {user_email}")
            
            logger.info(f"‚úÖ {new_role.title()} activated for user {user_id} ({user_email}) via Stripe payment, expires: {expires_at}")
            
            # Send appropriate notification email based on role
            try:
                from email_config import email_service
                
                if new_role == 'enterprise':
                    email_sent = email_service.send_enterprise_notification_email(
                        to_email=user_email,
                        user_name=user_email.split('@')[0],  # Use email prefix as name
                        expires_at=expires_at.isoformat(),
                        granted_by="Stripe Payment"
                    )
                    
                    if email_sent:
                        logger.info(f"‚úÖ Enterprise notification email sent to {user_email}")
                    else:
                        logger.error(f"‚ùå Failed to send enterprise notification email to {user_email}")
                else:
                    email_sent = email_service.send_premium_notification_email(
                    to_email=user_email,
                    user_name=user_email.split('@')[0],  # Use email prefix as name
                    expires_at=expires_at.isoformat(),
                    granted_by="Stripe Payment"
                )
                
                if email_sent:
                    logger.info(f"‚úÖ Premium notification email sent to {user_email}")
                else:
                    logger.error(f"‚ùå Failed to send premium notification email to {user_email}")
            except Exception as e:
                logger.error(f"‚ùå Error sending {new_role} notification email: {str(e)}")
        
    except Exception as e:
        logger.error(f"‚ùå Error handling successful payment: {str(e)}")
        logger.error(f"‚ùå Payment handler traceback: {traceback.format_exc()}")
        raise  # Re-raise to let webhook handler decide how to respond

async def handle_subscription_created(subscription):
    """Handle when a subscription is created in Stripe"""
    try:
        user_id = int(subscription['metadata']['user_id'])
        user_email = subscription['metadata']['user_email']
        
        logger.info(f"üìù Subscription created for user {user_id} ({user_email}): {subscription['id']}")
        
        # Log the activity
        log_activity(user_id, "stripe_subscription_created", f"Stripe subscription created: {subscription['id']}")
        
    except Exception as e:
        logger.error(f"Error handling subscription created: {str(e)}")

async def handle_subscription_renewal(invoice):
    """Handle subscription renewal payments"""
    try:
        subscription_id = invoice['subscription']
        customer_id = invoice['customer']
        
        # Get subscription details to find user
        subscription = stripe.Subscription.retrieve(subscription_id)
        user_id = int(subscription['metadata']['user_id'])
        pricing_tier = subscription['metadata'].get('pricing_tier', 'monthly')
        
        # Determine renewal period based on pricing tier
        if pricing_tier == 'enterprise':
            renewal_days = 30   # Enterprise renews for 1 month (monthly subscription)
        else:
            renewal_days = 30   # Premium renews for 1 month
        
        with get_db_transaction() as conn:
            cursor = conn.cursor()
            placeholder = get_placeholder()
            
            # Get current expiration date to extend it properly
            cursor.execute(f"SELECT premium_expires_at, role FROM users WHERE id = {placeholder}", (user_id,))
            user_data = cursor.fetchone()
            
            if user_data and user_data['premium_expires_at']:
                # Extend existing expiration by appropriate period
                current_expires = datetime.fromisoformat(user_data['premium_expires_at'].replace('Z', '+00:00')) if isinstance(user_data['premium_expires_at'], str) else user_data['premium_expires_at']
                # If current expiration is in the past, start from now
                base_date = max(current_expires, datetime.utcnow()) if current_expires else datetime.utcnow()
                expires_at = base_date + timedelta(days=renewal_days)
            else:
                # No existing expiration, start from now
                expires_at = datetime.utcnow() + timedelta(days=renewal_days)
            
            cursor.execute(f"""
                UPDATE users 
                SET premium_expires_at = {placeholder}
                WHERE id = {placeholder}
            """, (expires_at, user_id))
            
            conn.commit()
            
            subscription_type = "Enterprise" if pricing_tier == 'enterprise' else "Premium"
            log_activity(user_id, "stripe_renewal", f"{subscription_type} subscription renewed via Stripe")
            logger.info(f"‚úÖ {subscription_type} renewed for user {user_id} via Stripe")
        
    except Exception as e:
        logger.error(f"Error handling subscription renewal: {str(e)}")

async def handle_subscription_cancelled(subscription):
    """Handle subscription cancellation"""
    try:
        user_id = int(subscription['metadata']['user_id'])
        user_email = subscription['metadata'].get('user_email', '')
        
        with get_db_transaction() as conn:
            cursor = conn.cursor()
            placeholder = get_placeholder()
            
            # Get user details if email not in metadata
            if not user_email:
                cursor.execute(f"SELECT email FROM users WHERE id = {placeholder}", (user_id,))
                user_data = cursor.fetchone()
                if user_data:
                    user_email = user_data['email']
            
            # Set role back to user and clear expiration
            cursor.execute(f"""
                UPDATE users 
                SET role = 'user', premium_expires_at = NULL
                WHERE id = {placeholder}
            """, (user_id,))
            
            conn.commit()
            
            log_activity(user_id, "stripe_cancellation", f"Premium subscription cancelled via Stripe")
            logger.info(f"‚úÖ Premium cancelled for user {user_id} via Stripe")
            
            # Send cancellation email via webhook (for subscriptions cancelled outside our app)
            if user_email:
                try:
                    from email_config import email_service
                    user_name = user_email.split('@')[0]
                    email_sent = email_service.send_subscription_cancellation_email(
                        to_email=user_email,
                        user_name=user_name,
                        cancellation_type="immediate"
                    )
                    
                    if email_sent:
                        logger.info(f"‚úÖ Webhook cancellation email sent to {user_email}")
                    else:
                        logger.error(f"‚ùå Failed to send webhook cancellation email to {user_email}")
                except Exception as e:
                    logger.error(f"‚ùå Error sending webhook cancellation email: {str(e)}")
        
    except Exception as e:
        logger.error(f"Error handling subscription cancellation: {str(e)}")

async def handle_subscription_paused(subscription):
    """Handle subscription pause/suspension"""
    try:
        user_id = int(subscription['metadata']['user_id'])
        
        with get_db_transaction() as conn:
            cursor = conn.cursor()
            placeholder = get_placeholder()
            
            # Set role back to user but keep expiration date for potential resume
            cursor.execute(f"""
                UPDATE users 
                SET role = 'user'
                WHERE id = {placeholder}
            """, (user_id,))
            
            conn.commit()
            
            log_activity(user_id, "stripe_subscription_paused", f"Premium subscription paused via Stripe")
            logger.info(f"‚è∏Ô∏è Premium paused for user {user_id} via Stripe")
        
    except Exception as e:
        logger.error(f"Error handling subscription pause: {str(e)}")

async def handle_subscription_updated(subscription):
    """Handle subscription updates (plan changes, quantity, etc.)"""
    try:
        # Extract user info from subscription metadata or customer
        user_id = None
        user_email = None
        
        if 'metadata' in subscription and subscription['metadata'].get('user_id'):
            user_id = int(subscription['metadata']['user_id'])
            user_email = subscription['metadata'].get('user_email')
        else:
            # Fallback: find user by customer ID
            customer_id = subscription.get('customer')
            if customer_id:
                with get_db() as conn:
                    cursor = conn.cursor()
                    placeholder = get_placeholder()
                    cursor.execute(f"SELECT id, email FROM users WHERE stripe_customer_id = {placeholder}", (customer_id,))
                    user_data = cursor.fetchone()
                    if user_data:
                        user_id = user_data['id']
                        user_email = user_data['email']
        
        if not user_id:
            logger.warning(f"Could not find user for subscription update: {subscription.get('id')}")
            return
        
        # Log subscription changes
        subscription_status = subscription.get('status', 'unknown')
        subscription_id = subscription.get('id', 'unknown')
        
        with get_db_transaction() as conn:
            cursor = conn.cursor()
            placeholder = get_placeholder()
            
            # Update subscription status if it changed to active and user isn't premium
            if subscription_status == 'active':
                cursor.execute(f"SELECT role FROM users WHERE id = {placeholder}", (user_id,))
                user_data = cursor.fetchone()
                
                if user_data and user_data['role'] != 'premium':
                    # Calculate new expiration (30 days from now)
                    expires_at = datetime.utcnow() + timedelta(days=30)
                    
                    cursor.execute(f"""
                        UPDATE users 
                        SET role = 'premium', premium_expires_at = {placeholder}
                        WHERE id = {placeholder}
                    """, (expires_at, user_id))
                    
                    conn.commit()
                    
                    logger.info(f"‚úÖ Updated user {user_id} to premium due to subscription update")
                    
                    # Send notification email about subscription change
                    try:
                        from email_config import email_service
                        email_service.send_premium_notification_email(
                            to_email=user_email,
                            user_name=user_email.split('@')[0] if user_email else "User",
                            expires_at=expires_at.isoformat(),
                            granted_by="Subscription Update"
                        )
                        logger.info(f"‚úÖ Sent subscription update email to {user_email}")
                    except Exception as e:
                        logger.error(f"‚ùå Failed to send subscription update email: {str(e)}")
            
            # Log the activity regardless
            log_activity(user_id, "stripe_subscription_updated", f"Subscription {subscription_id} updated - status: {subscription_status}")
            logger.info(f"üìù Subscription updated for user {user_id}: {subscription_id} - {subscription_status}")
        
    except Exception as e:
        logger.error(f"Error handling subscription update: {str(e)}")
        logger.error(f"Subscription update error traceback: {traceback.format_exc()}")

async def handle_subscription_resumed(subscription):
    """Handle subscription resumption after pause"""
    try:
        # Extract user info
        user_id = None
        user_email = None
        
        if 'metadata' in subscription and subscription['metadata'].get('user_id'):
            user_id = int(subscription['metadata']['user_id'])
            user_email = subscription['metadata'].get('user_email')
        else:
            # Find user by customer ID
            customer_id = subscription.get('customer')
            if customer_id:
                with get_db() as conn:
                    cursor = conn.cursor()
                    placeholder = get_placeholder()
                    cursor.execute(f"SELECT id, email FROM users WHERE stripe_customer_id = {placeholder}", (customer_id,))
                    user_data = cursor.fetchone()
                    if user_data:
                        user_id = user_data['id']
                        user_email = user_data['email']
        
        if not user_id:
            logger.warning(f"Could not find user for subscription resume: {subscription.get('id')}")
            return
        
        with get_db_transaction() as conn:
            cursor = conn.cursor()
            placeholder = get_placeholder()
            
            # Restore premium access
            expires_at = datetime.utcnow() + timedelta(days=30)
            
            cursor.execute(f"""
                UPDATE users 
                SET role = 'premium', premium_expires_at = {placeholder}
                WHERE id = {placeholder}
            """, (expires_at, user_id))
            
            conn.commit()
            
            log_activity(user_id, "stripe_subscription_resumed", f"Premium subscription resumed via Stripe")
            logger.info(f"‚ñ∂Ô∏è Premium resumed for user {user_id} via Stripe")
            
            # Send welcome back email
            try:
                from email_config import email_service
                email_content = f"""
                <h2>Welcome Back to TodoEvents Premium!</h2>
                <p>Great news! Your premium subscription has been resumed.</p>
                <p>You now have access to all premium features including:</p>
                <ul>
                    <li>Advanced event analytics</li>
                    <li>Priority customer support</li>
                    <li>Enhanced event management tools</li>
                </ul>
                <p>Your premium access expires on: {expires_at.strftime('%B %d, %Y')}</p>
                <p>Thank you for being a TodoEvents premium member!</p>
                """
                
                email_service.send_email(
                    user_email,
                    "TodoEvents Premium - Welcome Back!",
                    email_content
                )
                logger.info(f"‚úÖ Sent subscription resumed email to {user_email}")
            except Exception as e:
                logger.error(f"‚ùå Failed to send subscription resumed email: {str(e)}")
        
    except Exception as e:
        logger.error(f"Error handling subscription resume: {str(e)}")
        logger.error(f"Subscription resume error traceback: {traceback.format_exc()}")

async def handle_invoice_created(invoice):
    """Handle invoice creation for billing notifications"""
    try:
        customer_id = invoice.get('customer')
        invoice_id = invoice.get('id')
        amount_due = invoice.get('amount_due', 0)
        currency = invoice.get('currency', 'usd')
        
        if not customer_id:
            logger.warning(f"No customer ID in invoice: {invoice_id}")
            return
        
        # Find user by customer ID
        with get_db() as conn:
            cursor = conn.cursor()
            placeholder = get_placeholder()
            cursor.execute(f"SELECT id, email FROM users WHERE stripe_customer_id = {placeholder}", (customer_id,))
            user_data = cursor.fetchone()
            
            if not user_data:
                logger.warning(f"Could not find user for customer: {customer_id}")
                return
            
            user_id = user_data['id']
            user_email = user_data['email']
        
        # Log invoice creation
        log_activity(user_id, "stripe_invoice_created", f"Invoice {invoice_id} created - amount: {amount_due/100:.2f} {currency.upper()}")
        
        # Send billing notification email
        try:
            formatted_amount = f"${amount_due/100:.2f}" if currency.lower() == 'usd' else f"{amount_due/100:.2f} {currency.upper()}"
            
            # Get invoice period for better context
            period_start = invoice.get('period_start')
            period_end = invoice.get('period_end')
            period_text = ""
            
            if period_start and period_end:
                start_date = datetime.fromtimestamp(period_start).strftime('%B %d, %Y')
                end_date = datetime.fromtimestamp(period_end).strftime('%B %d, %Y')
                period_text = f"<p>Billing period: {start_date} - {end_date}</p>"
            
            from email_config import email_service
            email_content = f"""
            <h2>TodoEvents Premium - Upcoming Payment</h2>
            <p>Hello,</p>
            <p>Your TodoEvents premium subscription invoice has been created.</p>
            <p><strong>Amount due: {formatted_amount}</strong></p>
            {period_text}
            <p>Payment will be automatically processed according to your billing schedule.</p>
            <p>If you have any questions about your subscription, please don't hesitate to contact our support team.</p>
            <p>Thank you for being a TodoEvents premium member!</p>
            """
            
            email_service.send_email(
                user_email,
                f"TodoEvents Premium - Invoice for {formatted_amount}",
                email_content
            )
            logger.info(f"‚úÖ Sent invoice notification email to {user_email} for {formatted_amount}")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to send invoice notification email: {str(e)}")
        
        logger.info(f"üìÑ Invoice created for user {user_id}: {invoice_id} - {formatted_amount}")
        
    except Exception as e:
        logger.error(f"Error handling invoice creation: {str(e)}")
        logger.error(f"Invoice creation error traceback: {traceback.format_exc()}")

async def handle_invoice_upcoming(invoice):
    """Handle upcoming invoice notifications"""
    try:
        customer_id = invoice.get('customer')
        invoice_id = invoice.get('id')
        amount_due = invoice.get('amount_due', 0)
        currency = invoice.get('currency', 'usd')
        
        if not customer_id:
            logger.warning(f"No customer ID in upcoming invoice: {invoice_id}")
            return
        
        # Find user by customer ID
        with get_db() as conn:
            cursor = conn.cursor()
            placeholder = get_placeholder()
            cursor.execute(f"SELECT id, email FROM users WHERE stripe_customer_id = {placeholder}", (customer_id,))
            user_data = cursor.fetchone()
            
            if not user_data:
                logger.warning(f"Could not find user for customer: {customer_id}")
                return
            
            user_id = user_data['id']
            user_email = user_data['email']
        
        # Log upcoming invoice
        log_activity(user_id, "stripe_invoice_upcoming", f"Upcoming invoice {invoice_id} - amount: {amount_due/100:.2f} {currency.upper()}")
        
        # Send upcoming payment notification email
        try:
            formatted_amount = f"${amount_due/100:.2f}" if currency.lower() == 'usd' else f"{amount_due/100:.2f} {currency.upper()}"
            
            # Get invoice period for better context
            period_start = invoice.get('period_start')
            period_end = invoice.get('period_end')
            period_text = ""
            
            if period_start and period_end:
                start_date = datetime.fromtimestamp(period_start).strftime('%B %d, %Y')
                end_date = datetime.fromtimestamp(period_end).strftime('%B %d, %Y')
                period_text = f"<p>Billing period: {start_date} - {end_date}</p>"
            
            from email_config import email_service
            email_content = f"""
            <h2>TodoEvents Premium - Upcoming Payment Reminder</h2>
            <p>Hello,</p>
            <p>This is a friendly reminder that your TodoEvents premium subscription payment is coming up.</p>
            <p><strong>Amount: {formatted_amount}</strong></p>
            {period_text}
            <p>Your payment method will be automatically charged on your next billing date.</p>
            <p>To update your payment method or manage your subscription, you can visit your account settings.</p>
            <p>If you have any questions about your subscription, please don't hesitate to contact our support team.</p>
            <p>Thank you for being a TodoEvents premium member!</p>
            """
            
            email_service.send_email(
                user_email,
                f"TodoEvents Premium - Upcoming Payment of {formatted_amount}",
                email_content
            )
            logger.info(f"‚úÖ Sent upcoming payment notification email to {user_email} for {formatted_amount}")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to send upcoming payment notification email: {str(e)}")
        
        logger.info(f"üìÖ Upcoming invoice processed for user {user_id}: {invoice_id} - {formatted_amount}")
        
    except Exception as e:
        logger.error(f"Error handling upcoming invoice: {str(e)}")
        logger.error(f"Upcoming invoice error traceback: {traceback.format_exc()}")

@app.get("/stripe/subscription-status")
async def get_subscription_status(current_user: dict = Depends(get_current_user)):
    """Get detailed subscription status from Stripe for current user"""
    try:
        # This endpoint can be used to check subscription details
        # For now, just return basic info from our database
        return {
            "user_id": current_user['id'],
            "role": current_user['role'],
            "is_premium": current_user['role'] in ['premium', 'admin'],
            "email": current_user['email']
        }
        
    except Exception as e:
        logger.error(f"Error getting subscription status: {str(e)}")
        raise HTTPException(status_code=500, detail="Failed to get subscription status")
# Enhanced Marketing Analytics Endpoints

# Comprehensive analytics now handled by main /users/analytics endpoint with comprehensive=true parameter

# CSV export now handled by main /users/analytics endpoint with export_csv parameter

# Add explicit OPTIONS handler for analytics endpoint
@app.options("/users/analytics")
async def options_user_analytics():
    """Handle CORS preflight for user analytics"""
    from fastapi.responses import Response
    return Response(
        status_code=200,
        headers={
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Methods": "GET, POST, PUT, DELETE, OPTIONS",
            "Access-Control-Allow-Headers": "Content-Type, Authorization",
            "Access-Control-Allow-Credentials": "true",
        }
    )

# Premium User Analytics Endpoint
@app.get("/users/analytics")
async def get_user_analytics(
    current_user: dict = Depends(get_current_user),
    period_days: int = 30,
    comprehensive: bool = False,
    export_csv: str = None
):
    """
    Get analytics data for premium users
    """
    if current_user['role'] not in ['premium', 'admin', 'enterprise']:
        raise HTTPException(status_code=403, detail="Premium access required")
    
    placeholder = get_placeholder()
    try:
        with get_db() as conn:
            cursor = conn.cursor()
            
            # Check what columns exist in the events table
            actual_columns = get_actual_table_columns(cursor, 'events')
            
            # Build a safe query based on available columns
            select_columns = ['id', 'title', 'date', 'start_time']
            if 'view_count' in actual_columns:
                select_columns.append('view_count')
            if 'interest_count' in actual_columns:
                select_columns.append('interest_count')
            if 'verified' in actual_columns:
                select_columns.append('verified')
            
            query = f"""
                SELECT {', '.join(select_columns)}
                FROM events 
                WHERE created_by = {placeholder}
                ORDER BY date DESC
            """
            
            cursor.execute(query, (current_user['id'],))
            events = cursor.fetchall()
            
            # Convert to list of dicts and ensure all fields exist
            event_list = []
            for event in events:
                event_dict = dict(event)
                # Ensure all expected fields exist
                if 'view_count' not in event_dict:
                    event_dict['view_count'] = 0
                if 'interest_count' not in event_dict:
                    event_dict['interest_count'] = 0
                if 'verified' not in event_dict:
                    event_dict['verified'] = False
                event_list.append(event_dict)
            
            # Calculate overall stats
            total_events = len(event_list)
            total_views = sum(event.get('view_count', 0) or 0 for event in event_list)
            total_interests = sum(event.get('interest_count', 0) or 0 for event in event_list)
            avg_views = round(total_views / total_events) if total_events > 0 else 0
            
            # Try to get trend data, but handle gracefully if tables don't exist
            view_trends = []
            interest_trends = []
            
            try:
                # Check if analytics tables exist
                if IS_PRODUCTION and DB_URL:
                    # PostgreSQL
                    cursor.execute("""
                        SELECT table_name 
                        FROM information_schema.tables 
                        WHERE table_name IN ('event_views', 'event_interests')
                        AND table_schema = 'public'
                    """)
                else:
                    # SQLite
                    cursor.execute("""
                        SELECT name 
                        FROM sqlite_master 
                        WHERE type='table' AND name IN ('event_views', 'event_interests')
                    """)
                
                table_results = cursor.fetchall()
                existing_tables = [row[0] for row in table_results] if table_results else []
                
                if 'event_views' in existing_tables:
                    thirty_days_ago = (datetime.utcnow() - timedelta(days=30)).isoformat()
                    
                    # Get view trends - handle potential table/column issues
                    try:
                        cursor.execute(f"""
                            SELECT DATE(ev.created_at) as date, COUNT(*) as views
                            FROM event_views ev
                            JOIN events e ON ev.event_id = e.id
                            WHERE e.created_by = {placeholder} AND ev.created_at >= {placeholder}
                            GROUP BY DATE(ev.created_at)
                            ORDER BY date
                        """, (current_user['id'], thirty_days_ago))
                        view_results = cursor.fetchall()
                    except Exception as view_error:
                        logger.warning(f"View trends query failed: {str(view_error)}")
                        view_results = []
                    view_trends = []
                    for row in view_results:
                        if hasattr(row, '_asdict'):
                            view_trends.append(row._asdict())
                        elif isinstance(row, dict):
                            view_trends.append(row)
                        else:
                            # Handle tuple results
                            view_trends.append({"date": row[0], "views": row[1]})
                
                if 'event_interests' in existing_tables:
                    thirty_days_ago = (datetime.utcnow() - timedelta(days=30)).isoformat()
                    
                    # Get interest trends - handle potential table/column issues
                    try:
                        cursor.execute(f"""
                            SELECT DATE(ei.created_at) as date, COUNT(*) as interests
                            FROM event_interests ei
                            JOIN events e ON ei.event_id = e.id
                            WHERE e.created_by = {placeholder} AND ei.created_at >= {placeholder}
                            GROUP BY DATE(ei.created_at)
                            ORDER BY date
                        """, (current_user['id'], thirty_days_ago))
                        interest_results = cursor.fetchall()
                    except Exception as interest_error:
                        logger.warning(f"Interest trends query failed: {str(interest_error)}")
                        interest_results = []
                    interest_trends = []
                    for row in interest_results:
                        if hasattr(row, '_asdict'):
                            interest_trends.append(row._asdict())
                        elif isinstance(row, dict):
                            interest_trends.append(row)
                        else:
                            # Handle tuple results
                            interest_trends.append({"date": row[0], "interests": row[1]})
                    
            except Exception as trend_error:
                logger.warning(f"Could not get trend data: {str(trend_error)}")
                # Continue with empty trends
            
            # Calculate engagement rate
            engagement_rate = round((total_interests / total_views * 100), 2) if total_views > 0 else 0
            avg_interests = round(total_interests / total_events) if total_events > 0 else 0
            
            # Category performance
            category_stats = {}
            for event in event_list:
                cat = event.get('category', 'Other')
                if cat not in category_stats:
                    category_stats[cat] = {"events": 0, "views": 0, "interests": 0}
                category_stats[cat]["events"] += 1
                category_stats[cat]["views"] += event.get('view_count', 0) or 0
                category_stats[cat]["interests"] += event.get('interest_count', 0) or 0
            
            # Convert to list format
            category_performance = []
            for cat, stats in category_stats.items():
                engagement = round((stats["interests"] / stats["views"] * 100), 2) if stats["views"] > 0 else 0
                category_performance.append({
                    "category": cat,
                    "events": stats["events"],
                    "views": stats["views"], 
                    "interests": stats["interests"],
                    "engagement_rate": engagement
                })
            
            # Geographic performance analysis
            geographic_stats = {}
            for event in event_list:
                city = event.get('city', 'Unknown')
                state = event.get('state', 'Unknown')
                location = f"{city}, {state}" if city != 'Unknown' and state != 'Unknown' else city if city != 'Unknown' else 'Unknown Location'
                
                if location not in geographic_stats:
                    geographic_stats[location] = {"events": 0, "views": 0, "interests": 0}
                geographic_stats[location]["events"] += 1
                geographic_stats[location]["views"] += event.get('view_count', 0) or 0
                geographic_stats[location]["interests"] += event.get('interest_count', 0) or 0

            # Create time series data (last 30 days)
            from datetime import datetime, timedelta
            time_series_data = {}
            for i in range(30):
                date = (datetime.utcnow() - timedelta(days=29-i)).strftime('%Y-%m-%d')
                time_series_data[date] = {"views": 0, "interests": 0, "events_created": 0}
            
            # Add actual event creation dates
            for event in event_list:
                created_date = event.get('created_at', '').split('T')[0] if event.get('created_at') else None
                if created_date and created_date in time_series_data:
                    time_series_data[created_date]["events_created"] += 1
            
            # Always return comprehensive format that frontend expects
            response_data = {
                "summary": {
                    "total_events": total_events,
                    "total_views": total_views,
                    "total_interests": total_interests,
                    "avg_views_per_event": avg_views,
                    "avg_interests_per_event": avg_interests,
                    "engagement_rate_percent": engagement_rate,
                    "period_days": period_days
                },
                "category_performance": category_performance,
                "geographic_distribution": geographic_stats,
                "time_series": time_series_data,
                "top_performing_events": sorted(event_list, key=lambda x: x.get('view_count', 0), reverse=True)[:10],
                "all_events": event_list
            }
            
            # Handle CSV export if requested
            if export_csv:
                import csv
                from io import StringIO
                from fastapi.responses import Response
                
                output = StringIO()
                
                if export_csv == "events":
                    # Events performance CSV
                    writer = csv.writer(output)
                    writer.writerow(['Event Title', 'Date', 'Category', 'Views', 'Interests', 'Engagement Rate %'])
                    
                    for event in response_data.get('all_events', []):
                        views = event.get('view_count', 0) or 0
                        interests = event.get('interest_count', 0) or 0
                        engagement = round((interests / views * 100), 2) if views > 0 else 0
                        
                        writer.writerow([
                            event.get('title', ''),
                            event.get('date', ''),
                            event.get('category', ''),
                            views,
                            interests,
                            engagement
                        ])
                elif export_csv == "categories":
                    # Category performance CSV
                    writer = csv.writer(output)
                    writer.writerow(['Category', 'Events', 'Total Views', 'Total Interests', 'Engagement Rate %'])
                    
                    for cat in response_data.get('category_performance', []):
                        writer.writerow([
                            cat['category'],
                            cat['events'],
                            cat['views'],
                            cat['interests'],
                            cat['engagement_rate']
                        ])
                
                csv_content = output.getvalue()
                filename = f"todoevents_analytics_{export_csv}_{period_days}days_{datetime.utcnow().strftime('%Y%m%d')}.csv"
                
                return Response(
                    content=csv_content,
                    media_type="text/csv",
                    headers={"Content-Disposition": f"attachment; filename={filename}"}
                )
            
            # Return with explicit CORS headers
            from fastapi.responses import JSONResponse
            response = JSONResponse(content=response_data)
            response.headers["Access-Control-Allow-Origin"] = "*"
            response.headers["Access-Control-Allow-Methods"] = "GET, POST, PUT, DELETE, OPTIONS"
            response.headers["Access-Control-Allow-Headers"] = "Content-Type, Authorization"
            response.headers["Access-Control-Allow-Credentials"] = "true"
            return response
            
    except Exception as e:
        logger.error(f"Error getting user analytics: {str(e)}")
        raise HTTPException(status_code=500, detail="Error retrieving analytics")


@app.get("/events/{event_id}/analytics")
async def get_event_analytics(
    event_id: int,
    current_user: dict = Depends(get_current_user)
):
    """Get detailed analytics for a specific event"""
    logger.info(f"üîç Getting analytics for event {event_id} by user {current_user.get('id')}")
    
    try:
        with get_db() as conn:
            cursor = conn.cursor()
            placeholder = get_placeholder()
            
            # Get event details and verify ownership
            cursor.execute(
                f"""SELECT id, title, description, date, start_time, category, city, state, 
                           created_by, COALESCE(view_count, 0) as view_count, 
                           COALESCE(interest_count, 0) as interest_count,
                           created_at, slug
                    FROM events 
                    WHERE id = {placeholder}""", 
                (event_id,)
            )
            event = cursor.fetchone()
            
            if not event:
                raise HTTPException(status_code=404, detail="Event not found")
            
            # Check if user owns this event or is admin
            if event['created_by'] != current_user['id'] and current_user.get('role') != 'admin':
                raise HTTPException(status_code=403, detail="Not authorized to view this event's analytics")
            
            # Get time-based view analytics (simulate daily tracking over past 30 days)
            from datetime import datetime, timedelta
            
            # Create mock time series data based on event creation date and current views
            created_date = datetime.fromisoformat(event['created_at'].replace('Z', '+00:00')) if event['created_at'] else datetime.utcnow()
            current_date = datetime.utcnow()
            
            time_series = {}
            total_views = event['view_count'] or 0
            total_interests = event['interest_count'] or 0
            
            # Generate 30 days of data
            for i in range(30):
                date = (current_date - timedelta(days=29-i)).strftime('%Y-%m-%d')
                
                # Simulate realistic view progression
                days_since_creation = (current_date - timedelta(days=29-i) - created_date).days
                if days_since_creation >= 0:
                    # Simulate decreasing view rate over time
                    views_multiplier = max(0.1, 1 - (days_since_creation * 0.05))
                    daily_views = max(0, int(total_views * views_multiplier * 0.1))  # 10% of total distributed
                    daily_interests = max(0, int(total_interests * views_multiplier * 0.1))
                else:
                    daily_views = 0
                    daily_interests = 0
                
                time_series[date] = {
                    "views": daily_views,
                    "interests": daily_interests
                }
            
            # Extract location from city/state or slug
            location = 'Unknown Location'
            city = event.get('city', '').strip() if event.get('city') else ''
            state = event.get('state', '').strip() if event.get('state') else ''
            slug = event.get('slug', '').strip() if event.get('slug') else ''
            
            if city and state:
                location = f"{city}, {state}"
            elif city:
                location = city
            elif slug:
                # Extract location from slug: us/fl/daytona-beach/events/test-734175
                try:
                    slug_parts = slug.split('/')
                    if len(slug_parts) >= 3 and slug_parts[0] and slug_parts[1] and slug_parts[2]:
                        country = slug_parts[0].upper()  # us -> US
                        state_code = slug_parts[1].upper()    # fl -> FL 
                        city_slug = slug_parts[2].replace('-', ' ').title()  # daytona-beach -> Daytona Beach
                        location = f"{city_slug}, {state_code}, {country}"
                        logger.info(f"üìç Extracted location from slug: '{slug}' -> '{location}'")
                except Exception as e:
                    logger.warning(f"Could not parse location from slug '{slug}': {e}")
                    location = 'Unknown Location'
            
            geographic_data = {
                location: {
                    "views": total_views,
                    "interests": total_interests
                }
            }
            
            # Traffic source simulation (referral data)
            traffic_sources = {
                "Direct": int(total_views * 0.4),
                "Social Media": int(total_views * 0.3),
                "Search": int(total_views * 0.2),
                "Referral": int(total_views * 0.1)
            }
            
            # Engagement metrics
            engagement_rate = round((total_interests / total_views * 100), 2) if total_views > 0 else 0
            
            # Peak viewing times (simulate)
            peak_hours = {
                "9 AM": int(total_views * 0.08),
                "12 PM": int(total_views * 0.15),
                "3 PM": int(total_views * 0.12),
                "6 PM": int(total_views * 0.20),
                "9 PM": int(total_views * 0.18),
                "Other": int(total_views * 0.27)
            }
            
            # Log the extracted data for debugging
            logger.info(f"üîç Event analytics for {event_id}: title='{event['title']}', category='{event['category']}', location='{location}', slug='{event.get('slug', 'None')}'")
            
            # Enhanced category handling
            category = event['category'] or 'Other'
            if not category or category.strip() == '':
                category = 'Other'
            
            response_data = {
                "event": {
                    "id": event['id'],
                    "title": event['title'] or 'Untitled Event',
                    "description": event['description'] or '',
                    "date": event['date'],
                    "category": category,
                    "location": location,
                    "slug": event.get('slug', '')
                },
                "summary": {
                    "total_views": total_views,
                    "total_interests": total_interests,
                    "engagement_rate": engagement_rate,
                    "created_date": event['created_at']
                },
                "time_series": time_series,
                "geographic_distribution": geographic_data,
                "traffic_sources": traffic_sources,
                "peak_viewing_hours": peak_hours,
                "performance_metrics": {
                    "view_velocity": round(total_views / max(1, (current_date - created_date).days), 2),
                    "interest_conversion": engagement_rate,
                    "days_active": max(1, (current_date - created_date).days)
                },
                "debug_info": {
                    "original_city": event.get('city', ''),
                    "original_state": event.get('state', ''),
                    "original_category": event.get('category', ''),
                    "slug": event.get('slug', '')
                }
            }
            
            return response_data
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting event analytics: {str(e)}")
        raise HTTPException(status_code=500, detail="Error retrieving event analytics")

# Temporary debug version without Pydantic validation
@app.get("/events/manage-debug")
async def list_user_events_debug(current_user: dict = Depends(get_current_user)):
    """Debug version without Pydantic validation to isolate the issue"""
    logger.info(f"üîç /events/manage-debug called by user {current_user.get('id', 'unknown')}")
    
    try:
        with get_db() as conn:
            c = conn.cursor()
            placeholder = get_placeholder()
            
            # Simple query
            c.execute(f"SELECT * FROM events WHERE created_by = {placeholder} ORDER BY date, start_time LIMIT 5", (current_user["id"],))
            events = c.fetchall()
            
            raw_events = []
            for event in events:
                raw_events.append(dict(event))
            
            return {
                "success": True,
                "count": len(raw_events),
                "events": raw_events,
                "user_id": current_user["id"]
            }
    except Exception as e:
        logger.error(f"Debug endpoint error: {str(e)}")
        return {"success": False, "error": str(e)}

# Simple user events endpoint - replaces complex manage-fix endpoints
@app.get("/user/events", response_model=List[EventResponse])
async def get_user_events(current_user: dict = Depends(get_current_user)):
    """Get all events created by the current user - simple and reliable"""
    logger.info(f"üîç /user/events called by user {current_user.get('id', 'unknown')} ({current_user.get('email', 'unknown')})")
    
    placeholder = get_placeholder()
    try:
        with get_db() as conn:
            c = conn.cursor()
            
            # Simple, reliable query for user's events
            query = f"""
                SELECT id, title, description, short_description, date, start_time, end_time, end_date, 
                       category, secondary_category, address, city, state, country, lat, lng, recurring, frequency, 
                       created_by, created_at, updated_at, start_datetime, end_datetime,
                       COALESCE(interest_count, 0) as interest_count,
                       COALESCE(view_count, 0) as view_count,
                       fee_required, price, currency, event_url, host_name, organizer_url, slug, is_published, verified
                FROM events 
                WHERE created_by = {placeholder}
                ORDER BY date DESC, start_time DESC
            """
            
            logger.info(f"üîç Executing query for user {current_user['id']}")
            c.execute(query, (current_user["id"],))
            events = c.fetchall()
            
            logger.info(f"üîç Found {len(events)} events for user {current_user['id']}")
            
            event_list = []
            for event in events:
                try:
                    # Convert to dict
                    event_dict = dict(event)
                    
                    # Apply datetime conversion
                    event_dict = convert_event_datetime_fields(event_dict)
                    
                    # Ensure required fields have safe values
                    event_dict['title'] = event_dict.get('title') or "Untitled Event"
                    event_dict['description'] = event_dict.get('description') or "No description"
                    event_dict['address'] = event_dict.get('address') or "Address not provided"
                    event_dict['category'] = event_dict.get('category') or "other"
                    event_dict['date'] = event_dict.get('date') or "2024-01-01"
                    event_dict['start_time'] = event_dict.get('start_time') or "00:00"
                    event_dict['lat'] = float(event_dict.get('lat') or 0.0)
                    event_dict['lng'] = float(event_dict.get('lng') or 0.0)
                    event_dict['recurring'] = bool(event_dict.get('recurring', False))
                    event_dict['interest_count'] = int(event_dict.get('interest_count', 0) or 0)
                    event_dict['view_count'] = int(event_dict.get('view_count', 0) or 0)
                    event_dict['verified'] = bool(event_dict.get('verified', False))
                    event_dict['is_published'] = bool(event_dict.get('is_published', True))
                    event_dict['country'] = event_dict.get('country') or "USA"
                    event_dict['currency'] = event_dict.get('currency') or "USD"
                    event_dict['price'] = float(event_dict.get('price', 0.0) or 0.0)
                    
                    # Handle optional string fields
                    optional_fields = ['short_description', 'end_time', 'end_date', 'secondary_category', 
                                     'city', 'state', 'frequency', 'fee_required', 'event_url', 
                                     'host_name', 'organizer_url', 'slug', 'updated_at', 
                                     'start_datetime', 'end_datetime']
                    for field in optional_fields:
                        if field in event_dict and event_dict[field] is None:
                            event_dict[field] = ""
                    
                    event_list.append(event_dict)
                    
                except Exception as event_error:
                    logger.error(f"üîç Error processing event {event.get('id', 'unknown')}: {event_error}")
                    continue
            
            logger.info(f"üîç Successfully processed {len(event_list)} events")
            return event_list
            
    except Exception as e:
        logger.error(f"üîç Error in /user/events: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error retrieving user events: {str(e)}")

# Temporary version without response model to isolate Pydantic validation issues
@app.get("/events/manage-fix-no-validation")
async def list_user_events_no_validation(current_user: dict = Depends(get_current_user)):
    """Version without response_model to test if Pydantic validation is the issue"""
    logger.info(f"üîç /events/manage-fix-no-validation called by user {current_user.get('id', 'unknown')}")
    
    placeholder = get_placeholder()
    try:
        with get_db() as conn:
            c = conn.cursor()
            
            # Simple query without column detection complexity
            query = f"SELECT * FROM events WHERE created_by = {placeholder} ORDER BY date, start_time"
            logger.info(f"üîç Executing query: {query} with user_id={current_user['id']}")
            c.execute(query, (current_user["id"],))
            events = c.fetchall()
            
            logger.info(f"üîç Found {len(events)} events for user {current_user['id']}")
            
            event_list = []
            for i, event in enumerate(events):
                try:
                    event_dict = dict(event)
                    event_id = event_dict.get('id', 'unknown')
                    
                    logger.info(f"üîç Processing event {i+1}/{len(events)}: ID={event_id}")
                    
                    # Just convert to dict and add minimal required fields
                    event_dict = dict(event)
                    
                    # Apply basic datetime conversion if needed
                    try:
                        event_dict = convert_event_datetime_fields(event_dict)
                        logger.info(f"üîç Event {event_id}: Successfully processed and converted")
                    except Exception as conversion_error:
                        logger.error(f"üîç Event {event_id}: Datetime conversion failed: {conversion_error}")
                        # Continue with original event_dict if conversion fails
                    
                    event_list.append(event_dict)
                    
                except Exception as event_error:
                    logger.error(f"üîç Error processing event {event_id}: {event_error}")
                    logger.error(f"üîç Event data: {dict(event)}")
                    continue
            
            logger.info(f"üîç Successfully processed {len(event_list)} events, returning response")
            return event_list
            
    except Exception as e:
        logger.error(f"üîç Critical error in /events/manage-fix-no-validation: {str(e)}")
        logger.error(f"üîç Traceback: {traceback.format_exc()}")
        return {"error": str(e), "traceback": traceback.format_exc()}

# Robust replacement for /events/manage to fix 422 errors
@app.get("/events/manage-fix", response_model=List[EventResponse])
async def list_user_events_robust(current_user: dict = Depends(get_current_user)):
    """
    ROBUST version of events/manage that ensures no 422 errors with detailed logging.
    """
    logger.info(f"üîç /events/manage-fix called by user {current_user.get('id', 'unknown')} ({current_user.get('email', 'unknown')})")
    
    placeholder = get_placeholder()
    try:
        with get_db() as conn:
            c = conn.cursor()
            
            # Simple query without column detection complexity
            query = f"SELECT * FROM events WHERE created_by = {placeholder} ORDER BY date, start_time"
            logger.info(f"üîç Executing query: {query} with user_id={current_user['id']}")
            c.execute(query, (current_user["id"],))
            events = c.fetchall()
            
            logger.info(f"üîç Found {len(events)} events for user {current_user['id']}")
            
            event_list = []
            for i, event in enumerate(events):
                try:
                    event_dict = dict(event)
                    event_id = event_dict.get('id', 'unknown')
                    
                    logger.info(f"üîç Processing event {i+1}/{len(events)}: ID={event_id}")
                    
                    # ENSURE ALL REQUIRED FIELDS EXIST WITH SAFE DEFAULTS
                    # Required string fields
                    event_dict['title'] = str(event_dict.get('title', '')).strip() or "Untitled Event"
                    event_dict['description'] = str(event_dict.get('description', '')).strip() or "No description provided"
                    event_dict['date'] = str(event_dict.get('date', '')).strip() or "2024-01-01"
                    event_dict['start_time'] = str(event_dict.get('start_time', '')).strip() or "00:00"
                    event_dict['category'] = str(event_dict.get('category', '')).strip() or "other"
                    event_dict['address'] = str(event_dict.get('address', '')).strip() or "Address not provided"
                    
                    # Required numeric fields
                    try:
                        event_dict['lat'] = float(event_dict.get('lat', 0.0) or 0.0)
                    except (ValueError, TypeError):
                        logger.warning(f"üîç Event {event_id}: Invalid lat value, using default 0.0")
                        event_dict['lat'] = 0.0
                    try:
                        event_dict['lng'] = float(event_dict.get('lng', 0.0) or 0.0)
                    except (ValueError, TypeError):
                        logger.warning(f"üîç Event {event_id}: Invalid lng value, using default 0.0")
                        event_dict['lng'] = 0.0
                    
                    # Required integer fields
                    event_dict['created_by'] = int(event_dict.get('created_by', current_user["id"]))
                    event_dict['id'] = int(event_dict.get('id', 0))
                    
                    # Required datetime field
                    event_dict['created_at'] = str(event_dict.get('created_at', datetime.utcnow().isoformat()))
                    
                    # All optional fields with proper defaults
                    event_dict['verified'] = bool(event_dict.get('verified', False))
                    event_dict['interest_count'] = int(event_dict.get('interest_count', 0) or 0)
                    event_dict['view_count'] = int(event_dict.get('view_count', 0) or 0)
                    event_dict['secondary_category'] = event_dict.get('secondary_category')
                    event_dict['updated_at'] = event_dict.get('updated_at')
                    event_dict['start_datetime'] = event_dict.get('start_datetime')
                    event_dict['end_datetime'] = event_dict.get('end_datetime')
                    event_dict['short_description'] = event_dict.get('short_description')
                    event_dict['end_time'] = event_dict.get('end_time')
                    event_dict['end_date'] = event_dict.get('end_date')
                    event_dict['city'] = event_dict.get('city')
                    event_dict['state'] = event_dict.get('state')
                    event_dict['country'] = str(event_dict.get('country', 'USA'))
                    event_dict['recurring'] = bool(event_dict.get('recurring', False))
                    event_dict['frequency'] = event_dict.get('frequency')
                    event_dict['fee_required'] = event_dict.get('fee_required')
                    try:
                        event_dict['price'] = float(event_dict.get('price', 0.0) or 0.0)
                    except (ValueError, TypeError):
                        logger.warning(f"üîç Event {event_id}: Invalid price value, using default 0.0")
                        event_dict['price'] = 0.0
                    event_dict['currency'] = str(event_dict.get('currency', 'USD'))
                    event_dict['event_url'] = event_dict.get('event_url')
                    event_dict['host_name'] = event_dict.get('host_name')
                    event_dict['organizer_url'] = event_dict.get('organizer_url')
                    event_dict['slug'] = event_dict.get('slug')
                    event_dict['is_published'] = bool(event_dict.get('is_published', True))
                    
                    # Apply datetime conversion and log any issues
                    try:
                        event_dict = convert_event_datetime_fields(event_dict)
                        logger.info(f"üîç Event {event_id}: Successfully processed and converted")
                    except Exception as conversion_error:
                        logger.error(f"üîç Event {event_id}: Datetime conversion failed: {conversion_error}")
                        # Continue with original event_dict if conversion fails
                    
                    event_list.append(event_dict)
                    
                except Exception as event_error:
                    logger.error(f"üîç Error processing event {event_id}: {event_error}")
                    logger.error(f"üîç Event data: {dict(event)}")
                    # Skip this event but continue with others
                    continue
            
            logger.info(f"üîç Successfully processed {len(event_list)} events, returning response")
            return event_list
            
    except Exception as e:
        logger.error(f"üîç Critical error in /events/manage-fix: {str(e)}")
        logger.error(f"üîç Traceback: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"Error retrieving user events: {str(e)}")

# Premium Management Models
class PremiumGrantRequest(BaseModel):
    months: int = 1

class PremiumInviteRequest(BaseModel):
    email: EmailStr
    months: int = 1
    message: Optional[str] = None

class EnterpriseInviteRequest(BaseModel):
    email: EmailStr
    months: int = 1
    message: Optional[str] = None

# Premium Management Endpoints
@app.get("/admin/premium-users")
async def get_premium_users(current_user: dict = Depends(get_current_user)):
    """Get all premium users with their expiration dates"""
    if current_user['role'] != UserRole.ADMIN:
        raise HTTPException(status_code=403, detail="Not authorized")
    
    placeholder = get_placeholder()
    try:
        with get_db() as conn:
            c = conn.cursor()
            
            # First, check if premium columns exist
            try:
                if IS_PRODUCTION and DB_URL:
                    # PostgreSQL - check if columns exist
                    c.execute("""
                        SELECT column_name 
                        FROM information_schema.columns 
                        WHERE table_name = 'users' 
                        AND column_name IN ('premium_expires_at', 'premium_granted_by', 'premium_invited')
                    """)
                    existing_columns = [row[0] for row in c.fetchall()]
                else:
                    # SQLite - check if columns exist
                    c.execute('PRAGMA table_info(users)')
                    columns_info = c.fetchall()
                    existing_columns = [col[1] for col in columns_info if col[1] in ['premium_expires_at', 'premium_granted_by', 'premium_invited']]
                
                has_premium_columns = len(existing_columns) > 0
                logger.info(f"Premium columns check: {existing_columns}, has_premium_columns: {has_premium_columns}")
                
            except Exception as e:
                logger.warning(f"Could not check premium columns: {e}")
                has_premium_columns = False
            
            # Build query based on available columns
            if has_premium_columns and 'premium_expires_at' in existing_columns:
                # Full query with premium columns
                query = f"""
                    SELECT u.id, u.email, u.role, u.created_at,
                           u.premium_expires_at, u.premium_granted_by, u.premium_invited,
                           admin.email as granted_by_email
                    FROM users u
                    LEFT JOIN users admin ON u.premium_granted_by = admin.id
                    WHERE u.role = 'premium' OR u.premium_expires_at IS NOT NULL
                    ORDER BY u.premium_expires_at DESC NULLS LAST, u.created_at DESC
                """
            else:
                # Fallback query without premium columns
                query = f"""
                    SELECT u.id, u.email, u.role, u.created_at
                    FROM users u
                    WHERE u.role = 'premium'
                    ORDER BY u.created_at DESC
                """
            
            c.execute(query)
            users = []
            for row in c.fetchall():
                user_dict = dict(row)
                
                # Add default values for missing columns
                if 'premium_expires_at' not in user_dict:
                    user_dict['premium_expires_at'] = None
                if 'premium_granted_by' not in user_dict:
                    user_dict['premium_granted_by'] = None
                if 'premium_invited' not in user_dict:
                    user_dict['premium_invited'] = False
                if 'granted_by_email' not in user_dict:
                    user_dict['granted_by_email'] = None
                
                # Check if premium is expired
                if user_dict['premium_expires_at']:
                    try:
                        from datetime import datetime
                        expires_at = datetime.fromisoformat(user_dict['premium_expires_at'].replace('Z', '+00:00')) if isinstance(user_dict['premium_expires_at'], str) else user_dict['premium_expires_at']
                        user_dict['is_expired'] = expires_at < datetime.utcnow()
                    except:
                        user_dict['is_expired'] = False
                else:
                    user_dict['is_expired'] = False
                
                users.append(user_dict)
            
            return {
                "users": users,
                "has_premium_columns": has_premium_columns,
                "available_columns": existing_columns if has_premium_columns else []
            }
            
    except Exception as e:
        logger.error(f"Error getting premium users: {str(e)}")
        logger.error(f"Error details: {type(e).__name__}: {e}")
        import traceback
        logger.error(f"Traceback: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"Error retrieving premium users: {str(e)}")

@app.post("/admin/users/{user_id}/grant-premium")
async def grant_premium(
    user_id: int,
    request: PremiumGrantRequest,
    current_user: dict = Depends(get_current_user)
):
    """Grant premium access to a user for specified months"""
    if current_user['role'] != UserRole.ADMIN:
        raise HTTPException(status_code=403, detail="Not authorized")
    
    placeholder = get_placeholder()
    try:
        with get_db() as conn:
            c = conn.cursor()
            
            # Check if user exists
            c.execute(f"SELECT * FROM users WHERE id = {placeholder}", (user_id,))
            user = c.fetchone()
            
            if not user:
                raise HTTPException(status_code=404, detail="User not found")
            
            # Calculate expiration date
            from datetime import datetime, timedelta
            expires_at = datetime.utcnow() + timedelta(days=30 * request.months)
            
            # Update user to premium
            c.execute(f"""
                UPDATE users 
                SET role = 'premium', 
                    premium_expires_at = {placeholder},
                    premium_granted_by = {placeholder}
                WHERE id = {placeholder}
            """, (expires_at, current_user['id'], user_id))
            
            conn.commit()
            
            # Log the activity
            log_activity(current_user['id'], "grant_premium", f"Granted {request.months} months premium to {user['email']}")
            
            return {
                "detail": f"Premium access granted for {request.months} months",
                "expires_at": expires_at.isoformat(),
                "user_email": user['email']
            }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error granting premium to user {user_id}: {str(e)}")
        raise HTTPException(status_code=500, detail="Error granting premium access")

@app.delete("/admin/users/{user_id}/remove-premium")
async def remove_premium(
    user_id: int,
    current_user: dict = Depends(get_current_user)
):
    """Remove premium access from a user"""
    if current_user['role'] != UserRole.ADMIN:
        raise HTTPException(status_code=403, detail="Not authorized")
    
    placeholder = get_placeholder()
    try:
        with get_db() as conn:
            c = conn.cursor()
            
            # Check if user exists
            c.execute(f"SELECT * FROM users WHERE id = {placeholder}", (user_id,))
            user = c.fetchone()
            
            if not user:
                raise HTTPException(status_code=404, detail="User not found")
            
            # Update user to regular user
            c.execute(f"""
                UPDATE users 
                SET role = 'user', 
                    premium_expires_at = NULL,
                    premium_granted_by = NULL
                WHERE id = {placeholder}
            """, (user_id,))
            
            conn.commit()
            
            # Log the activity
            log_activity(current_user['id'], "remove_premium", f"Removed premium from {user['email']}")
            
            return {
                "detail": "Premium access removed",
                "user_email": user['email']
            }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error removing premium from user {user_id}: {str(e)}")
        raise HTTPException(status_code=500, detail="Error removing premium access")

@app.post("/admin/premium-invite")
async def invite_premium_user(
    request: PremiumInviteRequest,
    current_user: dict = Depends(get_current_user)
):
    """Invite a new user via email with premium trial"""
    if current_user['role'] != UserRole.ADMIN:
        raise HTTPException(status_code=403, detail="Not authorized")
    
    placeholder = get_placeholder()
    try:
        with get_db() as conn:
            c = conn.cursor()
            
            # Check if user already exists
            c.execute(f"SELECT * FROM users WHERE email = {placeholder}", (request.email,))
            existing_user = c.fetchone()
            
            if existing_user:
                # For existing users, upgrade them to premium instead of returning error
                from datetime import datetime, timedelta
                expires_at = datetime.utcnow() + timedelta(days=30 * request.months)
                
                # Check if premium columns exist before updating
                try:
                    if IS_PRODUCTION and DB_URL:
                        # PostgreSQL - check if columns exist
                        c.execute("""
                            SELECT column_name 
                            FROM information_schema.columns 
                            WHERE table_name = 'users' 
                            AND column_name IN ('premium_expires_at', 'premium_granted_by')
                        """)
                        existing_columns = [row[0] for row in c.fetchall()]
                    else:
                        # SQLite - check if columns exist
                        c.execute('PRAGMA table_info(users)')
                        columns_info = c.fetchall()
                        existing_columns = [col[1] for col in columns_info if col[1] in ['premium_expires_at', 'premium_granted_by']]
                    
                    has_premium_columns = len(existing_columns) >= 2
                except Exception:
                    has_premium_columns = False
                
                # Update user to premium with appropriate columns
                if has_premium_columns:
                    c.execute(f"""
                        UPDATE users 
                        SET role = 'premium', 
                            premium_expires_at = {placeholder},
                            premium_granted_by = {placeholder}
                        WHERE id = {placeholder}
                    """, (expires_at, current_user['id'], existing_user['id']))
                else:
                    # Fallback: just update role
                    c.execute(f"""
                        UPDATE users 
                        SET role = 'premium'
                        WHERE id = {placeholder}
                    """, (existing_user['id'],))
                
                conn.commit()
                
                # Log the activity
                log_activity(current_user['id'], "premium_invite_existing", f"Upgraded existing user {request.email} to {request.months} months premium")
                
                # Send premium notification email
                try:
                    from email_config import email_service
                    email_sent = email_service.send_premium_notification_email(
                        to_email=request.email,
                        user_name=existing_user.get('full_name'),
                        expires_at=expires_at.isoformat(),
                        granted_by=current_user.get('email', 'Admin')
                    )
                    
                    if email_sent:
                        logger.info(f"‚úÖ Premium upgrade notification email sent to {request.email}")
                    else:
                        logger.error(f"‚ùå Failed to send premium upgrade notification email to {request.email}")
                except Exception as e:
                    logger.error(f"‚ùå Error sending premium upgrade notification email: {str(e)}")
                
                return {
                    "detail": f"Existing user upgraded to premium for {request.months} months",
                    "user_exists": True,
                    "user_id": existing_user['id'],
                    "previous_role": existing_user['role'],
                    "new_role": "premium",
                    "expires_at": expires_at.isoformat(),
                    "email": request.email,
                    "months": request.months
                }
            
            # Create invitation record (we'll send email here in production)
            # For now, we'll just log the invitation
            log_activity(current_user['id'], "premium_invite", f"Invited {request.email} for {request.months} months premium trial")
            
            # In a real implementation, you would send an email here
            # send_premium_invitation_email(request.email, request.months, request.message)
            
            return {
                "detail": f"Premium invitation sent to {request.email}",
                "email": request.email,
                "months": request.months,
                "message": "Invitation email would be sent in production"
            }
    except Exception as e:
        logger.error(f"Error sending premium invitation: {str(e)}")
        raise HTTPException(status_code=500, detail="Error sending invitation")

@app.post("/admin/enterprise-invite")
async def invite_enterprise_user(
    request: EnterpriseInviteRequest,
    current_user: dict = Depends(get_current_user)
):
    """Invite a new user via email with enterprise trial"""
    if current_user['role'] != UserRole.ADMIN:
        raise HTTPException(status_code=403, detail="Not authorized")
    
    placeholder = get_placeholder()
    try:
        with get_db() as conn:
            c = conn.cursor()
            
            # Check if user already exists
            c.execute(f"SELECT * FROM users WHERE email = {placeholder}", (request.email,))
            existing_user = c.fetchone()
            
            if existing_user:
                # For existing users, upgrade them to enterprise instead of returning error
                from datetime import datetime, timedelta
                expires_at = datetime.utcnow() + timedelta(days=30 * request.months)
                
                # Check if premium columns exist before updating
                try:
                    if IS_PRODUCTION and DB_URL:
                        # PostgreSQL - check if columns exist
                        c.execute("""
                            SELECT column_name 
                            FROM information_schema.columns 
                            WHERE table_name = 'users' 
                            AND column_name IN ('premium_expires_at', 'premium_granted_by')
                        """)
                        existing_columns = [row[0] for row in c.fetchall()]
                    else:
                        # SQLite - check if columns exist
                        c.execute('PRAGMA table_info(users)')
                        columns_info = c.fetchall()
                        existing_columns = [col[1] for col in columns_info if col[1] in ['premium_expires_at', 'premium_granted_by']]
                    
                    has_premium_columns = len(existing_columns) >= 2
                except Exception:
                    has_premium_columns = False
                
                # Update user to enterprise with appropriate columns
                if has_premium_columns:
                    c.execute(f"""
                        UPDATE users 
                        SET role = 'enterprise', 
                            premium_expires_at = {placeholder},
                            premium_granted_by = {placeholder}
                        WHERE id = {placeholder}
                    """, (expires_at, current_user['id'], existing_user['id']))
                else:
                    # Fallback: just update role
                    c.execute(f"""
                        UPDATE users 
                        SET role = 'enterprise'
                        WHERE id = {placeholder}
                    """, (existing_user['id'],))
                
                conn.commit()
                
                # Log the activity
                log_activity(current_user['id'], "enterprise_invite_existing", f"Upgraded existing user {request.email} to {request.months} months enterprise")
                
                # Send enterprise notification email
                try:
                    from email_config import email_service
                    email_sent = email_service.send_enterprise_notification_email(
                        to_email=request.email,
                        user_name=existing_user.get('full_name'),
                        expires_at=expires_at.isoformat(),
                        granted_by=current_user.get('email', 'Admin')
                    )
                    
                    if email_sent:
                        logger.info(f"‚úÖ Enterprise upgrade notification email sent to {request.email}")
                    else:
                        logger.error(f"‚ùå Failed to send enterprise upgrade notification email to {request.email}")
                except Exception as e:
                    logger.error(f"‚ùå Error sending enterprise upgrade notification email: {str(e)}")
                
                return {
                    "detail": f"Existing user upgraded to enterprise for {request.months} months",
                    "user_exists": True,
                    "user_id": existing_user['id'],
                    "previous_role": existing_user['role'],
                    "new_role": "enterprise",
                    "expires_at": expires_at.isoformat(),
                    "email": request.email,
                    "months": request.months
                }
            
            # Create invitation record (we'll send email here in production)
            # For now, we'll just log the invitation
            log_activity(current_user['id'], "enterprise_invite", f"Invited {request.email} for {request.months} months enterprise trial")
            
            # In a real implementation, you would send an email here
            # send_enterprise_invitation_email(request.email, request.months, request.message)
            
            return {
                "detail": f"Enterprise invitation sent to {request.email}",
                "email": request.email,
                "months": request.months,
                "message": "Invitation email would be sent in production"
            }
    except Exception as e:
        logger.error(f"Error sending enterprise invitation: {str(e)}")
        raise HTTPException(status_code=500, detail="Error sending invitation")

@app.get("/enterprise/stats")
async def get_enterprise_stats(
    current_user: dict = Depends(get_current_user)
):
    """Get enterprise dashboard stats"""
    if current_user['role'] not in [UserRole.ENTERPRISE, UserRole.ADMIN]:
        raise HTTPException(status_code=403, detail="Not authorized")
    
    placeholder = get_placeholder()
    try:
        with get_db() as conn:
            c = conn.cursor()
            
            # Get total events
            c.execute("SELECT COUNT(*) FROM events")
            total_events = c.fetchone()[0]
            
            # Get total users
            c.execute("SELECT COUNT(*) FROM users")
            total_users = c.fetchone()[0]
            
            # Get active users (users with events in last 30 days)
            c.execute("""
                SELECT COUNT(DISTINCT user_id) 
                FROM events 
                WHERE created_at >= NOW() - INTERVAL '30 days'
            """)
            active_users = c.fetchone()[0]
            
            # Calculate completion rate (events with descriptions vs without)
            c.execute("""
                SELECT 
                    COUNT(*) as total,
                    COUNT(CASE WHEN description IS NOT NULL AND description != '' THEN 1 END) as with_description
                FROM events
            """)
            result = c.fetchone()
            completion_rate = round((result[1] / result[0] * 100) if result[0] > 0 else 0, 1)
            
            return {
                "totalEvents": total_events,
                "totalUsers": total_users,
                "activeUsers": active_users,
                "completionRate": completion_rate
            }
            
    except Exception as e:
        logger.error(f"Error fetching enterprise stats: {str(e)}")
        raise HTTPException(status_code=500, detail="Error fetching stats")

@app.get("/enterprise/export")
async def export_enterprise_data(
    current_user: dict = Depends(get_current_user)
):
    """Export enterprise data as CSV"""
    if current_user['role'] not in [UserRole.ENTERPRISE, UserRole.ADMIN]:
        raise HTTPException(status_code=403, detail="Not authorized")
    
    placeholder = get_placeholder()
    try:
        with get_db() as conn:
            c = conn.cursor()
            
            # Get events data
            c.execute("""
                SELECT 
                    e.id,
                    e.title,
                    e.description,
                    e.date_time,
                    e.location,
                    e.category,
                    e.is_free,
                    u.email as creator_email,
                    e.created_at
                FROM events e
                JOIN users u ON e.user_id = u.id
                ORDER BY e.created_at DESC
            """)
            
            events = c.fetchall()
            
            # Create CSV content
            import io
            import csv
            output = io.StringIO()
            writer = csv.writer(output)
            
            # Write header
            writer.writerow([
                'Event ID', 'Title', 'Description', 'Date Time', 'Location', 
                'Category', 'Is Free', 'Creator Email', 'Created At'
            ])
            
            # Write data
            for event in events:
                writer.writerow(event)
            
            # Create response
            from fastapi.responses import Response
            csv_content = output.getvalue()
            output.close()
            
            return Response(
                content=csv_content,
                media_type="text/csv",
                headers={
                    "Content-Disposition": "attachment; filename=enterprise-data.csv"
                }
            )
            
    except Exception as e:
        logger.error(f"Error exporting enterprise data: {str(e)}")
        raise HTTPException(status_code=500, detail="Error exporting data")

@app.get("/enterprise/overview")
async def get_enterprise_overview(
    current_user: dict = Depends(get_current_user)
):
    """Get enterprise overview data"""
    if current_user['role'] not in [UserRole.ENTERPRISE, UserRole.ADMIN]:
        raise HTTPException(status_code=403, detail="Not authorized")
    
    placeholder = get_placeholder()
    try:
        with get_db() as conn:
            c = conn.cursor()
            
            logger.info(f"Enterprise overview request from user {current_user['id']} with role {current_user['role']}")
            
            # Get stats - for enterprise users, show only their data
            if current_user['role'] == UserRole.ENTERPRISE:
                # Only show this user's events
                query = f"SELECT COUNT(*) FROM events WHERE created_by = {placeholder}"
                logger.info(f"Executing query: {query} with params: {(current_user['id'],)}")
                c.execute(query, (current_user['id'],))
                result = c.fetchone()
                logger.info(f"Query result: {result}")
                
                total_events = get_count_from_result(result)
                
                # For enterprise users, total_users is just 1 (themselves)
                total_users = 1
                
                # Get monthly growth for this user's events  
                # Use proper PostgreSQL date math
                growth_query = f"""
                    SELECT 
                        COUNT(*) as this_month,
                        (SELECT COUNT(*) FROM events WHERE created_by = {placeholder} AND created_at >= NOW() - INTERVAL '60 days' AND created_at < NOW() - INTERVAL '30 days') as last_month
                    FROM events 
                    WHERE created_by = {placeholder} AND created_at >= NOW() - INTERVAL '30 days'
                """
                logger.info(f"Executing growth query: {growth_query}")
                try:
                    c.execute(growth_query, (current_user['id'], current_user['id']))
                except Exception as e:
                    logger.error(f"Growth query failed, using fallback: {e}")
                    # Fallback to simpler query
                    c.execute(f"SELECT COUNT(*) as this_month, 0 as last_month FROM events WHERE created_by = {placeholder}", (current_user['id'],))
            else:
                # Admin sees all data
                c.execute("SELECT COUNT(*) FROM events")
                total_events = get_count_from_result(c.fetchone())
                
                c.execute("SELECT COUNT(*) FROM users")
                total_users = get_count_from_result(c.fetchone())
                
                # Get monthly growth for all events
                try:
                    c.execute("""
                        SELECT 
                            COUNT(*) as this_month,
                            (SELECT COUNT(*) FROM events WHERE created_at >= NOW() - INTERVAL '60 days' AND created_at < NOW() - INTERVAL '30 days') as last_month
                        FROM events 
                        WHERE created_at >= NOW() - INTERVAL '30 days'
                    """)
                except Exception as e:
                    logger.error(f"Admin growth query failed, using fallback: {e}")
                    c.execute("SELECT COUNT(*) as this_month, 0 as last_month FROM events")
            growth_data = c.fetchone()
            
            # Handle PostgreSQL RealDictRow vs SQLite tuple
            if isinstance(growth_data, dict) or hasattr(growth_data, 'get'):
                this_month = growth_data.get('this_month', 0) or 0
                last_month = growth_data.get('last_month', 0) or 0
            else:
                this_month = growth_data[0] if growth_data and len(growth_data) > 0 else 0
                last_month = growth_data[1] if growth_data and len(growth_data) > 1 else 0
            
            event_growth = ((this_month - last_month) / max(last_month, 1)) * 100 if last_month > 0 else 0
            
            # Get client breakdown (users with events vs without)
            c.execute("""
                SELECT 
                    COUNT(DISTINCT u.id) as total_clients,
                    COUNT(DISTINCT CASE WHEN e.created_by IS NOT NULL THEN u.id END) as active_clients
                FROM users u
                LEFT JOIN events e ON u.id = e.created_by
                WHERE u.role IN ('user', 'premium', 'enterprise')
            """)
            client_data = c.fetchone()
            
            # Handle PostgreSQL RealDictRow vs SQLite tuple for client data
            if isinstance(client_data, dict) or hasattr(client_data, 'get'):
                total_clients = client_data.get('total_clients', 0) or 0
                active_clients = client_data.get('active_clients', 0) or 0
            else:
                total_clients = client_data[0] if client_data and len(client_data) > 0 else 0
                active_clients = client_data[1] if client_data and len(client_data) > 1 else 0
            
            return {
                "total_events": total_events,
                "total_users": total_users,
                "total_clients": total_clients,
                "active_clients": active_clients,
                "event_growth_rate": round(event_growth, 1),
                "client_engagement_rate": round((active_clients / max(total_clients, 1)) * 100, 1)
            }
            
    except Exception as e:
        logger.error(f"Error fetching enterprise overview: {str(e)}")
        raise HTTPException(status_code=500, detail="Error fetching overview")

@app.get("/enterprise/clients")
async def get_enterprise_clients(
    current_user: dict = Depends(get_current_user)
):
    """Get enterprise clients data"""
    if current_user['role'] not in [UserRole.ENTERPRISE, UserRole.ADMIN]:
        raise HTTPException(status_code=403, detail="Not authorized")
    
    placeholder = get_placeholder()
    try:
        with get_db() as conn:
            c = conn.cursor()
            
            # Get clients with their event counts and categories
            if current_user['role'] == UserRole.ENTERPRISE:
                # For enterprise users, show all platform users as potential clients
                # This gives them insights into the user base they could potentially serve
                c.execute("""
                    SELECT 
                        u.id,
                        u.email,
                        u.role,
                        u.created_at,
                        u.premium_expires_at,
                        COUNT(e.id) as event_count,
                        STRING_AGG(DISTINCT e.category, ', ') as categories,
                        MAX(e.created_at) as last_event_date
                    FROM users u
                    LEFT JOIN events e ON u.id = e.created_by
                    WHERE u.role IN ('user', 'premium')
                    GROUP BY u.id, u.email, u.role, u.created_at, u.premium_expires_at
                    ORDER BY event_count DESC, u.created_at DESC
                """)
            else:
                # Admin sees all users including enterprise
                c.execute("""
                    SELECT 
                        u.id,
                        u.email,
                        u.role,
                        u.created_at,
                        u.premium_expires_at,
                        COUNT(e.id) as event_count,
                        STRING_AGG(DISTINCT e.category, ', ') as categories,
                        MAX(e.created_at) as last_event_date
                    FROM users u
                    LEFT JOIN events e ON u.id = e.created_by
                    WHERE u.role IN ('user', 'premium', 'enterprise')
                    GROUP BY u.id, u.email, u.role, u.created_at, u.premium_expires_at
                    ORDER BY event_count DESC, u.created_at DESC
                """)
            
            clients = []
            for row in c.fetchall():
                client = {
                    "id": row[0],
                    "email": row[1],
                    "role": row[2],
                    "created_at": row[3].isoformat() if row[3] else None,
                    "premium_expires_at": row[4].isoformat() if row[4] else None,
                    "event_count": row[5],
                    "categories": row[6] if row[6] else "None",
                    "last_event_date": row[7].isoformat() if row[7] else None,
                    "status": "active" if row[5] > 0 else "inactive"
                }
                clients.append(client)
            
            return {"clients": clients}
            
    except Exception as e:
        logger.error(f"Error fetching enterprise clients: {str(e)}")
        raise HTTPException(status_code=500, detail="Error fetching clients")

@app.get("/enterprise/events")
async def get_enterprise_events(
    page: int = 1,
    client_filter: str = "",
    status_filter: str = "",
    search: str = "",
    user_id: int = None,
    current_user: dict = Depends(get_current_user)
):
    """Get enterprise events data with pagination and filtering"""
    if current_user['role'] not in [UserRole.ENTERPRISE, UserRole.ADMIN]:
        raise HTTPException(status_code=403, detail="Not authorized")
    
    placeholder = get_placeholder()
    try:
        with get_db() as conn:
            c = conn.cursor()
            
            # Build query with filters
            where_conditions = []
            params = []
            
            # For enterprise users, only show their own events
            if current_user['role'] == UserRole.ENTERPRISE:
                where_conditions.append(f"e.created_by = {placeholder}")
                params.append(current_user['id'])
            
            if client_filter:
                where_conditions.append(f"u.email ILIKE {placeholder}")
                params.append(f"%{client_filter}%")
            
            if search:
                where_conditions.append(f"(e.title ILIKE {placeholder} OR e.description ILIKE {placeholder})")
                params.extend([f"%{search}%", f"%{search}%"])
            
            where_clause = " AND ".join(where_conditions) if where_conditions else "1=1"
            
            # Get total count
            count_query = f"""
                SELECT COUNT(*)
                FROM events e
                JOIN users u ON e.created_by = u.id
                WHERE {where_clause}
            """
            c.execute(count_query, params)
            total_events = get_count_from_result(c.fetchone())
            
            # Get paginated events
            limit = 20
            offset = (page - 1) * limit
            
            events_query = f"""
                SELECT 
                    e.id,
                    e.title,
                    e.description,
                    e.date,
                    e.start_time,
                    e.end_time,
                    e.address,
                    e.category,
                    e.fee_required,
                    e.created_at,
                    u.email as client_email,
                    u.role as client_role,
                    e.interest_count,
                    e.view_count,
                    e.verified,
                    e.client_name
                FROM events e
                LEFT JOIN users u ON e.created_by = u.id
                WHERE {where_clause}
                ORDER BY e.created_at DESC
                LIMIT {limit} OFFSET {offset}
            """
            c.execute(events_query, params)
            
            events = []
            for row in c.fetchall():
                # Determine event status based on date
                event_date = row[3]  # date field
                from datetime import datetime, date
                try:
                    if isinstance(event_date, str):
                        event_date_obj = datetime.strptime(event_date, '%Y-%m-%d').date()
                    else:
                        event_date_obj = event_date
                    
                    today = date.today()
                    if event_date_obj < today:
                        status = "past"
                    elif event_date_obj == today:
                        status = "today"
                    else:
                        status = "upcoming"
                except:
                    status = "unknown"

                event = {
                    "id": row[0],
                    "title": row[1],
                    "description": row[2][:100] + "..." if row[2] and len(row[2]) > 100 else row[2],
                    "date": row[3],
                    "start_time": row[4],
                    "end_time": row[5],
                    "address": row[6],
                    "category": row[7] or "uncategorized",
                    "is_free": not bool(row[8] and row[8].strip()),  # fee_required empty means free
                    "created_at": row[9].isoformat() if row[9] else None,
                    "client_email": row[10] or "None",
                    "client_role": row[11] or "none", 
                    "interest_count": row[12] or 0,
                    "view_count": row[13] or 0,
                    "verified": bool(row[14]),
                    "client_name": row[15] or None,
                    "status": status
                }
                events.append(event)
            
            total_pages = (total_events + limit - 1) // limit
            
            return {
                "events": events,
                "pagination": {
                    "current_page": page,
                    "total_pages": total_pages,
                    "total_events": total_events,
                    "events_per_page": limit
                }
            }
            
    except Exception as e:
        logger.error(f"Error fetching enterprise events: {str(e)}")
        raise HTTPException(status_code=500, detail="Error fetching events")

@app.get("/enterprise/analytics/clients")
async def get_enterprise_client_analytics(
    current_user: dict = Depends(get_current_user)
):
    """Get enterprise client analytics data"""
    if current_user['role'] not in [UserRole.ENTERPRISE, UserRole.ADMIN]:
        raise HTTPException(status_code=403, detail="Not authorized")
    
    placeholder = get_placeholder()
    try:
        with get_db() as conn:
            c = conn.cursor()
            
            # Client performance data
            if current_user['role'] == UserRole.ENTERPRISE:
                # For enterprise users, only show their own events
                c.execute(f"""
                    SELECT 
                        COALESCE(u.email, 'None') as client,
                        COUNT(e.id) as event_count,
                        u.role as client_role
                    FROM events e
                    LEFT JOIN users u ON e.created_by = u.id
                    WHERE e.created_at >= NOW() - INTERVAL '90 days' 
                    AND e.created_by = {placeholder}
                    GROUP BY u.email, u.role
                    ORDER BY event_count DESC
                    LIMIT 20
                """, (current_user['id'],))
            else:
                # Admin sees all events
                c.execute("""
                    SELECT 
                        COALESCE(u.email, 'None') as client,
                        COUNT(e.id) as event_count,
                        u.role as client_role
                    FROM events e
                    LEFT JOIN users u ON e.created_by = u.id
                    WHERE e.created_at >= NOW() - INTERVAL '90 days'
                    GROUP BY u.email, u.role
                    ORDER BY event_count DESC
                    LIMIT 20
                """)
            
            client_performance = []
            for row in c.fetchall():
                client_performance.append({
                    "client": row[0],
                    "event_count": row[1],
                    "client_role": row[2] or "none"
                })
            
            # Category distribution
            if current_user['role'] == UserRole.ENTERPRISE:
                # For enterprise users, only show their own events
                c.execute(f"""
                    SELECT 
                        COALESCE(e.category, 'uncategorized') as category,
                        COUNT(*) as count
                    FROM events e
                    WHERE e.created_at >= NOW() - INTERVAL '90 days'
                    AND e.created_by = {placeholder}
                    GROUP BY e.category
                    ORDER BY count DESC
                """, (current_user['id'],))
            else:
                # Admin sees all events
                c.execute("""
                    SELECT 
                        COALESCE(e.category, 'uncategorized') as category,
                        COUNT(*) as count
                    FROM events e
                    WHERE e.created_at >= NOW() - INTERVAL '90 days'
                    GROUP BY e.category
                    ORDER BY count DESC
                """)
            
            category_distribution = []
            for row in c.fetchall():
                category_distribution.append({
                    "category": row[0],
                    "count": row[1]
                })
            
            return {
                "client_performance": client_performance,
                "category_distribution": category_distribution
            }
            
    except Exception as e:
        logger.error(f"Error fetching enterprise analytics: {str(e)}")
        raise HTTPException(status_code=500, detail="Error fetching analytics")

@app.post("/admin/users/{user_id}/notify-premium")
async def notify_premium_granted(
    user_id: int,
    current_user: dict = Depends(get_current_user)
):
    """Send notification to user about their premium access"""
    if current_user['role'] != UserRole.ADMIN:
        raise HTTPException(status_code=403, detail="Not authorized")
    
    placeholder = get_placeholder()
    try:
        with get_db() as conn:
            c = conn.cursor()
            
            # Check if user exists and has premium
            c.execute(f"SELECT * FROM users WHERE id = {placeholder}", (user_id,))
            user = c.fetchone()
            
            if not user:
                raise HTTPException(status_code=404, detail="User not found")
            
            if user['role'] not in ['premium', 'enterprise']:
                raise HTTPException(status_code=400, detail="User does not have premium or enterprise access")
            
            # Log the notification
            log_activity(current_user['id'], "premium_notification", f"Sent {user['role']} notification to {user['email']}")
            
            # Send appropriate notification email based on role
            try:
                from email_config import email_service
                
                # Get the admin who granted premium access
                granted_by_email = None
                if user.get('premium_granted_by'):
                    c.execute(f"SELECT email FROM users WHERE id = {placeholder}", (user['premium_granted_by'],))
                    granted_by_user = c.fetchone()
                    if granted_by_user:
                        granted_by_email = granted_by_user['email']
                
                if user['role'] == 'enterprise':
                    # Send enterprise notification email
                    email_sent = email_service.send_enterprise_notification_email(
                        to_email=user['email'],
                        user_name=user.get('full_name'),
                        expires_at=user.get('premium_expires_at'),
                        granted_by=granted_by_email
                    )
                    
                    if email_sent:
                        logger.info(f"‚úÖ Enterprise notification email sent to {user['email']}")
                    else:
                        logger.error(f"‚ùå Failed to send enterprise notification email to {user['email']}")
                else:
                    # Send premium notification email
                    email_sent = email_service.send_premium_notification_email(
                        to_email=user['email'],
                        user_name=user.get('full_name'),
                        expires_at=user.get('premium_expires_at'),
                        granted_by=granted_by_email
                    )
                
                if email_sent:
                    logger.info(f"‚úÖ Premium notification email sent to {user['email']}")
                else:
                    logger.error(f"‚ùå Failed to send premium notification email to {user['email']}")
            except Exception as e:
                logger.error(f"‚ùå Error sending {user['role']} notification email: {str(e)}")
            
            return {
                "detail": f"{user['role'].title()} notification sent to {user['email']}",
                "user_email": user['email'],
                "message": f"{user['role'].title()} notification email sent successfully"
            }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error sending premium notification: {str(e)}")
        raise HTTPException(status_code=500, detail="Error sending notification")

@app.post("/debug/fix-enterprise-role")
async def debug_fix_enterprise_role(current_user: dict = Depends(get_current_user)):
    """Debug endpoint to manually fix enterprise role based on Stripe subscription"""
    try:
        # Find the user's Stripe customer and subscription
        customers = stripe.Customer.list(email=current_user['email'], limit=1)
        
        if not customers.data:
            return {"error": "No Stripe customer found", "user_email": current_user['email']}
        
        customer = customers.data[0]
        
        # Get active subscriptions
        subscriptions = stripe.Subscription.list(
            customer=customer.id,
            status='active',
            limit=10
        )
        
        if not subscriptions.data:
            return {"error": "No active subscription found", "customer_id": customer.id}
        
        subscription = subscriptions.data[0]
        
        # Get subscription details
        subscription_details = {
            "subscription_id": subscription.id,
            "status": subscription.status,
            "metadata": subscription.metadata,
            "items": []
        }
        
        # Check subscription items and pricing
        for item in subscription.items.data:
            price = item.price
            subscription_details["items"].append({
                "price_id": price.id,
                "amount": price.unit_amount,
                "currency": price.currency,
                "interval": price.recurring.interval if price.recurring else None
            })
        
        # Determine if this should be enterprise based on amount
        is_enterprise = False
        for item in subscription_details["items"]:
            if item["amount"] >= 50000:  # $500 or more = enterprise
                is_enterprise = True
                break
        
        # Get current user role
        with get_db() as conn:
            cursor = conn.cursor()
            placeholder = get_placeholder()
            cursor.execute(f"SELECT role, premium_expires_at FROM users WHERE id = {placeholder}", (current_user['id'],))
            user_data = cursor.fetchone()
        
        current_role = user_data['role'] if user_data else 'unknown'
        expected_role = 'enterprise' if is_enterprise else 'premium'
        
        # Update role if needed
        role_updated = False
        if current_role != expected_role:
            with get_db_transaction() as conn:
                cursor = conn.cursor()
                
                # Set appropriate expiration (both enterprise and premium are monthly)
                if is_enterprise:
                    expires_at = datetime.utcnow() + timedelta(days=30)  # Enterprise is monthly subscription
                else:
                    expires_at = datetime.utcnow() + timedelta(days=30)
                
                cursor.execute(f"""
                    UPDATE users 
                    SET role = {placeholder}, premium_expires_at = {placeholder}
                    WHERE id = {placeholder}
                """, (expected_role, expires_at, current_user['id']))
                
                conn.commit()
                role_updated = True
                
                # Log the activity
                log_activity(current_user['id'], "debug_role_fix", f"Manually fixed role from {current_role} to {expected_role}")
                
                # Send appropriate email
                try:
                    from email_config import email_service
                    
                    if expected_role == 'enterprise':
                        email_sent = email_service.send_enterprise_notification_email(
                            to_email=current_user['email'],
                            user_name=current_user['email'].split('@')[0],
                            expires_at=expires_at.isoformat(),
                            granted_by="Manual Role Fix"
                        )
                    else:
                        email_sent = email_service.send_premium_notification_email(
                            to_email=current_user['email'],
                            user_name=current_user['email'].split('@')[0],
                            expires_at=expires_at.isoformat(),
                            granted_by="Manual Role Fix"
                        )
                    
                    logger.info(f"‚úÖ {expected_role.title()} notification email sent to {current_user['email']}")
                except Exception as e:
                    logger.error(f"‚ùå Error sending notification email: {str(e)}")
        
        return {
            "user_id": current_user['id'],
            "user_email": current_user['email'],
            "current_role": current_role,
            "expected_role": expected_role,
            "is_enterprise": is_enterprise,
            "role_updated": role_updated,
            "subscription_details": subscription_details
        }
        
    except Exception as e:
        logger.error(f"Error in debug fix enterprise role: {str(e)}")
        return {"error": str(e), "traceback": traceback.format_exc()}

# Main execution block
if __name__ == "__main__":
    # Ensure environment variables are loaded
    load_dotenv()
    
    # Get port from environment variable for Render.com
    port = int(os.getenv("PORT", 8000))
    
    # Run the server
    uvicorn.run(
        "backend:app", 
        host="0.0.0.0", 
        port=port, 
        reload=not IS_PRODUCTION  # Only use reload in development
    )

@app.get("/debug/test-422")
async def debug_test_422():
    """Test endpoint to check if 422 is from authentication or data processing"""
    return {"message": "If you see this, the 422 error is not from the endpoint itself", "status": "working"}
@app.get("/debug/events-simple")
async def debug_events_simple(current_user: dict = Depends(get_current_user)):
    """Simple debug endpoint to test event retrieval without complex processing"""
    try:
        with get_db() as conn:
            c = conn.cursor()
            placeholder = get_placeholder()
            
            # Simple count query first
            c.execute(f"SELECT COUNT(*) FROM events WHERE created_by = {placeholder}", (current_user["id"],))
            count = get_count_from_result(c.fetchone())
            
            # Get first few events raw
            c.execute(f"SELECT * FROM events WHERE created_by = {placeholder} LIMIT 3", (current_user["id"],))
            events = c.fetchall()
            
            raw_events = []
            for event in events:
                raw_events.append(dict(event))
            
            return {
                "user_id": current_user["id"],
                "user_email": current_user.get("email", "unknown"),
                "total_events": count,
                "sample_events": raw_events,
                "database_type": "PostgreSQL" if IS_PRODUCTION and DB_URL else "SQLite"
            }
    except Exception as e:
        logger.error(f"Debug events simple error: {str(e)}")
        return {"error": str(e), "traceback": traceback.format_exc()}

@app.get("/debug/schema")
async def debug_schema():
    """Debug endpoint to check database schema"""
    try:
        with get_db() as conn:
            c = conn.cursor()
            
            # Check events table columns with fallback
            if IS_PRODUCTION and DB_URL:
                try:
                    c.execute("""
                        SELECT column_name, data_type, is_nullable, column_default 
                        FROM information_schema.columns 
                        WHERE table_name = 'events' 
                        ORDER BY ordinal_position
                    """)
                    events_columns = c.fetchall()
                    
                    # Check if event_interests table exists
                    c.execute("""
                        SELECT table_name 
                        FROM information_schema.tables 
                        WHERE table_name = 'event_interests'
                    """)
                    interests_table = c.fetchone()
                    
                    # Check if event_views table exists
                    c.execute("""
                        SELECT table_name 
                        FROM information_schema.tables 
                        WHERE table_name = 'event_views'
                    """)
                    views_table = c.fetchone()
                    
                    # Get event_interests columns if table exists
                    interests_columns = []
                    if interests_table:
                        c.execute("""
                            SELECT column_name, data_type, is_nullable 
                            FROM information_schema.columns 
                            WHERE table_name = 'event_interests' 
                            ORDER BY ordinal_position
                        """)
                        interests_columns = c.fetchall()
                    
                    # Get event_views columns if table exists
                    views_columns = []
                    if views_table:
                        c.execute("""
                            SELECT column_name, data_type, is_nullable 
                            FROM information_schema.columns 
                            WHERE table_name = 'event_views' 
                            ORDER BY ordinal_position
                        """)
                        views_columns = c.fetchall()
                except Exception as e:
                    # Fallback for information_schema issues
                    logger.warning(f"information_schema query failed, using fallback: {e}")
                    events_columns = [("schema_query_failed", "text", "YES", None)]
                    interests_table = None
                    views_table = None
                    interests_columns = []
                    views_columns = []
            else:
                # SQLite
                c.execute('PRAGMA table_info(events)')
                events_columns = c.fetchall()
                
                c.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='event_interests'")
                interests_table = c.fetchone()
                
                c.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='event_views'")
                views_table = c.fetchone()
                
                interests_columns = []
                views_columns = []
                
                if interests_table:
                    c.execute('PRAGMA table_info(event_interests)')
                    interests_columns = c.fetchall()
                    
                if views_table:
                    c.execute('PRAGMA table_info(event_views)')
                    views_columns = c.fetchall()
            
            return {
                "database_type": "postgresql" if IS_PRODUCTION and DB_URL else "sqlite",
                "events_table": {
                    "columns": events_columns
                },
                "event_interests_table": {
                    "exists": bool(interests_table),
                    "columns": interests_columns
                },
                "event_views_table": {
                    "exists": bool(views_table),
                    "columns": views_columns
                }
            }
            
    except Exception as e:
        logger.error(f"Schema debug error: {str(e)}")
        return {"error": str(e)}
@app.post("/debug/create-missing-tables")
async def create_missing_tables():
    """Create missing tables and columns in production"""
    try:
        with get_db() as conn:
            cursor = conn.cursor()
            results = []
            
            # Check if privacy_requests table exists
            if IS_PRODUCTION and DB_URL:
                cursor.execute("""
                    SELECT EXISTS (
                        SELECT FROM information_schema.tables 
                        WHERE table_name = 'privacy_requests'
                    )
                """)
                privacy_table_exists = cursor.fetchone()[0]
            else:
                cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='privacy_requests'")
                privacy_table_exists = cursor.fetchone() is not None
            
            # Create privacy_requests table if it doesn't exist
            if not privacy_table_exists:
                if IS_PRODUCTION and DB_URL:
                    cursor.execute("""
                        CREATE TABLE privacy_requests (
                            id SERIAL PRIMARY KEY,
                            request_type VARCHAR(50) NOT NULL,
                            email VARCHAR(255) NOT NULL,
                            full_name VARCHAR(255),
                            details TEXT,
                            status VARCHAR(20) DEFAULT 'pending',
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            completed_at TIMESTAMP NULL,
                            admin_notes TEXT
                        )
                    """)
                else:
                    cursor.execute("""
                        CREATE TABLE privacy_requests (
                            id INTEGER PRIMARY KEY AUTOINCREMENT,
                            request_type TEXT NOT NULL,
                            email TEXT NOT NULL,
                            full_name TEXT,
                            details TEXT,
                            status TEXT DEFAULT 'pending',
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            completed_at TIMESTAMP NULL,
                            admin_notes TEXT
                        )
                    """)
                results.append("‚úÖ Created privacy_requests table")
            else:
                results.append("‚ÑπÔ∏è privacy_requests table already exists")
            
            # Add premium columns to users table
            premium_columns = [
                ("premium_expires_at", "TIMESTAMP"),
                ("premium_granted_by", "INTEGER"),
                ("premium_invited", "BOOLEAN DEFAULT FALSE")
            ]
            
            for column_name, column_type in premium_columns:
                try:
                    if IS_PRODUCTION and DB_URL:
                        cursor.execute(f"ALTER TABLE users ADD COLUMN IF NOT EXISTS {column_name} {column_type}")
                    else:
                        cursor.execute(f"ALTER TABLE users ADD COLUMN {column_name} {column_type}")
                    results.append(f"‚úÖ Added {column_name} to users table")
                except Exception as e:
                    if "already exists" in str(e).lower() or "duplicate" in str(e).lower():
                        results.append(f"‚ÑπÔ∏è {column_name} already exists in users table")
                    else:
                        results.append(f"‚ùå Failed to add {column_name}: {str(e)}")
            
            conn.commit()
            
            return {
                "success": True,
                "message": "Database creation completed",
                "results": results,
                "privacy_table_created": not privacy_table_exists
            }
            
    except Exception as e:
        logger.error(f"Error creating missing tables: {str(e)}")
        return {
            "success": False,
            "error": str(e),
            "message": "Failed to create missing tables"
        }

@app.post("/debug/create-tables")
async def create_tracking_tables():
    """Debug endpoint to manually create tracking tables and fix counter values"""
    try:
        with get_db() as conn:
            cursor = conn.cursor()
            placeholder = get_placeholder()
            
            # Create event_interests table if it doesn't exist
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS event_interests (
                    id SERIAL PRIMARY KEY,
                    event_id INTEGER NOT NULL,
                    user_id INTEGER,
                    browser_fingerprint TEXT NOT NULL DEFAULT 'legacy',
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(event_id, user_id, browser_fingerprint),
                    FOREIGN KEY(event_id) REFERENCES events(id) ON DELETE CASCADE
                )
            """)
            
            # Create event_views table if it doesn't exist  
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS event_views (
                    id SERIAL PRIMARY KEY,
                    event_id INTEGER NOT NULL,
                    user_id INTEGER,
                    browser_fingerprint TEXT NOT NULL DEFAULT 'legacy',
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    UNIQUE(event_id, user_id, browser_fingerprint),
                    FOREIGN KEY(event_id) REFERENCES events(id) ON DELETE CASCADE
                )
            """)
            
            # Create page_visits table for general page visit tracking
            cursor.execute("""
                CREATE TABLE IF NOT EXISTS page_visits (
                    id SERIAL PRIMARY KEY,
                    page_type TEXT NOT NULL,
                    page_path TEXT NOT NULL,
                    user_id INTEGER,
                    browser_fingerprint TEXT NOT NULL DEFAULT 'anonymous',
                    visited_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            
            # Create privacy_requests table for CCPA compliance with simpler constraints for compatibility
            cursor.execute("""
                    CREATE TABLE IF NOT EXISTS privacy_requests (
                        id SERIAL PRIMARY KEY,
                        request_type VARCHAR(50) NOT NULL,
                        email VARCHAR(255) NOT NULL,
                        full_name VARCHAR(255),
                        verification_info TEXT,
                        details TEXT,
                        status VARCHAR(50) DEFAULT 'pending',
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        completed_at TIMESTAMP,
                        admin_notes TEXT
                    )
                """)
            
            # Fix NULL counter values in existing events
            cursor.execute("UPDATE events SET interest_count = 0 WHERE interest_count IS NULL")
            cursor.execute("UPDATE events SET view_count = 0 WHERE view_count IS NULL")
            
            # Add premium columns to users table if they don't exist
            premium_columns_added = []
            try:
                cursor.execute("ALTER TABLE users ADD COLUMN IF NOT EXISTS premium_expires_at TIMESTAMP")
                premium_columns_added.append("premium_expires_at")
            except Exception as e:
                premium_columns_added.append(f"premium_expires_at: {str(e)}")
            
            try:
                cursor.execute("ALTER TABLE users ADD COLUMN IF NOT EXISTS premium_granted_by INTEGER")
                premium_columns_added.append("premium_granted_by")
            except Exception as e:
                premium_columns_added.append(f"premium_granted_by: {str(e)}")
            
            try:
                cursor.execute("ALTER TABLE users ADD COLUMN IF NOT EXISTS premium_invited BOOLEAN DEFAULT FALSE")
                premium_columns_added.append("premium_invited")
            except Exception as e:
                premium_columns_added.append(f"premium_invited: {str(e)}")
            
            conn.commit()
            
            # Test privacy request insertion
            try:
                created_at = datetime.utcnow().isoformat()
                cursor.execute(f"""
                    INSERT INTO privacy_requests 
                    (request_type, email, full_name, verification_info, details, status, created_at)
                    VALUES ({placeholder}, {placeholder}, {placeholder}, {placeholder}, {placeholder}, {placeholder}, {placeholder})
                    RETURNING id
                """, (
                    'access',
                    'test@example.com',
                    'Test User',
                    'Test verification',
                    'Test details',
                    'pending',
                    created_at
                ))
                test_result = cursor.fetchone()
                test_request_id = test_result[0] if test_result else None
                conn.commit()
            except Exception as test_error:
                test_request_id = f"Error: {test_error}"
            
            return {
                "success": True,
                "message": "All tables created successfully",
                "tables_created": ["event_interests", "event_views", "page_visits", "privacy_requests"],
                "events_updated": True,
                "privacy_test_insert": test_request_id,
                "premium_columns_added": premium_columns_added
            }
            
    except Exception as e:
        logger.error(f"Error creating tracking tables: {str(e)}")
        return {
            "success": False,
            "error": str(e),
            "message": "Failed to create tracking tables"
        }

@app.get("/debug/test-tracking/{event_id}")
async def debug_test_tracking(event_id: int):
    """Debug endpoint to test tracking functionality"""
    try:
        placeholder = get_placeholder()
        
        with get_db() as conn:
            cursor = conn.cursor()
            
            # Test 1: Check if event exists
            cursor.execute(f"SELECT id FROM events WHERE id = {placeholder}", (event_id,))
            event_exists = cursor.fetchone()
            
            # Test 2: Check tracking tables
            cursor.execute("SELECT COUNT(*) FROM event_interests")
            interest_count = get_count_from_result(cursor.fetchone())
            
            cursor.execute("SELECT COUNT(*) FROM event_views")
            view_count = get_count_from_result(cursor.fetchone())
            
            # Test 3: Try simple insert (without constraints)
            test_fingerprint = f"debug-test-{event_id}"
            
            # Clean up any existing test data first
            cursor.execute(f"DELETE FROM event_interests WHERE browser_fingerprint = {placeholder}", (test_fingerprint,))
            cursor.execute(f"DELETE FROM event_views WHERE browser_fingerprint = {placeholder}", (test_fingerprint,))
            
            # Test insert
            cursor.execute(
                f"INSERT INTO event_interests (event_id, user_id, browser_fingerprint) VALUES ({placeholder}, {placeholder}, {placeholder})",
                (event_id, None, test_fingerprint)
            )
            
            cursor.execute(
                f"INSERT INTO event_views (event_id, user_id, browser_fingerprint) VALUES ({placeholder}, {placeholder}, {placeholder})",
                (event_id, None, test_fingerprint)
            )
            
            conn.commit()
            
            return {
                "success": True,
                "event_exists": event_exists is not None,
                "event_id": event_id,
                "interest_records": interest_count,
                "view_records": view_count,
                "insert_test": "passed"
            }
            
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "error_type": type(e).__name__
        }

@app.post("/debug/test-ux-fields")
async def debug_test_ux_fields():
    """Debug endpoint to test UX fields insertion directly"""
    try:
        logger.info("=== DEBUG UX FIELDS TEST ENDPOINT ===")
        
        # Test data with various scenarios
        test_cases = [
            {"fee_required": "Free", "event_url": "https://example.com", "host_name": "Test Host"},
            {"fee_required": None, "event_url": None, "host_name": None},
            {"fee_required": "", "event_url": "", "host_name": ""},
            {"fee_required": "   ", "event_url": "  ", "host_name": "   "}  # whitespace test
        ]
        
        results = []
        placeholder = get_placeholder()
        
        with get_db() as conn:
            cursor = conn.cursor()
            
            for i, test_data in enumerate(test_cases):
                logger.info(f"--- Test Case {i+1} ---")
                logger.info(f"Input: {test_data}")
                
                # Sanitize the fields
                sanitized_fee = sanitize_ux_field(test_data["fee_required"])
                sanitized_url = sanitize_ux_field(test_data["event_url"])
                sanitized_host = sanitize_ux_field(test_data["host_name"])
                
                # Insert test event using centralized helpers
                from database_schema import generate_insert_query
                from event_data_builder import build_test_event_values
                
                insert_query = generate_insert_query(returning_id=True)
                values = build_test_event_values(
                    title=f"Debug UX Test {i+1}",
                    description="Debug test event for UX fields",
                    created_by=1,
                    fee_required=sanitized_fee,
                    event_url=sanitized_url,
                    host_name=sanitized_host
                )
                
                logger.info(f"Final insert values (UX part): {values[-5:-2]}")
                
                # Handle SQLite vs PostgreSQL
                if IS_PRODUCTION and DB_URL:
                    cursor.execute(insert_query, values)
                    result = cursor.fetchone()
                    event_id = result['id'] if result else None
                else:
                    cursor.execute(insert_query.replace(" RETURNING id", ""), values)
                    event_id = cursor.lastrowid
                
                if event_id:
                    # Fetch the inserted event
                    cursor.execute(f"SELECT * FROM events WHERE id = {placeholder}", (event_id,))
                    fetched_event = cursor.fetchone()
                    
                    if fetched_event:
                        event_dict = dict(fetched_event)
                        result_data = {
                            "test_case": i + 1,
                            "event_id": event_id,
                            "input": test_data,
                            "sanitized": {
                                "fee_required": sanitized_fee,
                                "event_url": sanitized_url,
                                "host_name": sanitized_host
                            },
                            "database_result": {
                                "fee_required": event_dict.get('fee_required'),
                                "event_url": event_dict.get('event_url'),
                                "host_name": event_dict.get('host_name')
                            },
                            "success": True
                        }
                        
                        logger.info(f"Database result: {result_data['database_result']}")
                        results.append(result_data)
                    else:
                        results.append({
                            "test_case": i + 1,
                            "error": "Failed to fetch inserted event",
                            "success": False
                        })
                else:
                    results.append({
                        "test_case": i + 1,
                        "error": "Failed to get event ID after insert",
                        "success": False
                    })
                
                conn.commit()
        
        logger.info("=== DEBUG UX FIELDS TEST COMPLETED ===")
        return {"test_results": results}
        
    except Exception as e:
        logger.error(f"Error in UX fields debug test: {str(e)}")
        return {"error": str(e)}
# Simple memory cache for frequently accessed data with improved memory management
import time
from typing import Dict, Any
import threading
class SimpleCache:
    def __init__(self, ttl_seconds: int = 300, max_size: int = 1000):  # 5 minute TTL, max 1000 items
        self.cache: Dict[str, Dict[str, Any]] = {}
        self.ttl = ttl_seconds
        self.max_size = max_size
        self._lock = threading.RLock()  # Thread-safe cache
        self._last_cleanup = time.time()
        self._cleanup_interval = 60  # Cleanup every minute
    
    def _cleanup_expired(self) -> None:
        """Remove expired entries and enforce size limits"""
        current_time = time.time()
        
        # Only cleanup if enough time has passed
        if current_time - self._last_cleanup < self._cleanup_interval:
            return
        
        with self._lock:
            # Remove expired items
            expired_keys = []
            for key, data in self.cache.items():
                if current_time - data['timestamp'] >= self.ttl:
                    expired_keys.append(key)
            
            for key in expired_keys:
                del self.cache[key]
            
            # Enforce size limit by removing oldest items
            if len(self.cache) > self.max_size:
                # Sort by timestamp and remove oldest items
                sorted_items = sorted(self.cache.items(), key=lambda x: x[1]['timestamp'])
                items_to_remove = len(self.cache) - self.max_size
                
                for i in range(items_to_remove):
                    key_to_remove = sorted_items[i][0]
                    del self.cache[key_to_remove]
            
            self._last_cleanup = current_time
            
            # Log cleanup if significant
            if expired_keys or len(self.cache) > self.max_size * 0.8:
                logger.info(f"Cache cleanup: removed {len(expired_keys)} expired items, current size: {len(self.cache)}")
    
    def get(self, key: str) -> Any:
        self._cleanup_expired()
        
        with self._lock:
            if key in self.cache:
                data = self.cache[key]
                if time.time() - data['timestamp'] < self.ttl:
                    # Update access time for LRU-like behavior
                    data['last_access'] = time.time()
                    return data['value']
                else:
                    del self.cache[key]
        return None
    
    def set(self, key: str, value: Any) -> None:
        self._cleanup_expired()
        
        with self._lock:
            current_time = time.time()
            self.cache[key] = {
                'value': value,
                'timestamp': current_time,
                'last_access': current_time
            }
    
    def delete(self, key: str) -> None:
        with self._lock:
            if key in self.cache:
                del self.cache[key]
    
    def clear(self) -> None:
        with self._lock:
            self.cache.clear()
            self._last_cleanup = time.time()
    
    def size(self) -> int:
        return len(self.cache)
    
    def stats(self) -> Dict[str, Any]:
        """Get cache statistics for monitoring"""
        self._cleanup_expired()
        with self._lock:
            return {
                'size': len(self.cache),
                'max_size': self.max_size,
                'ttl_seconds': self.ttl,
                'last_cleanup': self._last_cleanup
            }

# Global cache instance with improved settings
event_cache = SimpleCache(ttl_seconds=180, max_size=500)  # 3 minute cache, max 500 events

# Bulk Event Creation for Admin
class BulkEventCreate(BaseModel):
    events: List[EventCreate]

class BulkEventResponse(BaseModel):
    success_count: int
    error_count: int
    errors: List[dict]
    created_events: List[EventResponse]

@app.post("/admin/events/bulk-simple", response_model=BulkEventResponse)  
async def bulk_create_events_simple(
    bulk_events: BulkEventCreate, 
    current_user: dict = Depends(get_current_user)
):
    """
    SIMPLIFIED Bulk create events using individual event creation (admin-only) - Workaround
    """
    if current_user['role'] != UserRole.ADMIN:
        raise HTTPException(status_code=403, detail="Not authorized. Admin access required.")
    
    success_count = 0
    error_count = 0
    errors = []
    created_events = []
    
    logger.info(f"Admin {current_user['email']} initiating SIMPLE bulk event creation for {len(bulk_events.events)} events")
    
    # Use individual event creation to bypass bulk import issues
    for i, event_data in enumerate(bulk_events.events):
        try:
            # Create event using the proven single event creation function
            response_event = await create_event(event_data, current_user)
            created_events.append(response_event)
            success_count += 1
            logger.info(f"‚úÖ Successfully created event {i+1}: '{event_data.title}' (ID: {response_event.id})")
            
        except Exception as e:
            error_count += 1
            error_msg = str(e)
            logger.error(f"‚ùå Error creating event {i+1} ({event_data.title}): {error_msg}")
            errors.append({
                "event_index": i + 1,
                "event_title": event_data.title,
                "error": error_msg
            })
            continue
    
    logger.info(f"‚úÖ Simple bulk event creation completed. Success: {success_count}, Errors: {error_count}")
    
    return BulkEventResponse(
        success_count=success_count,
        error_count=error_count,
        errors=errors,
        created_events=created_events
    )

@app.post("/admin/events/bulk", response_model=BulkEventResponse)
async def bulk_create_events(
    bulk_events: BulkEventCreate, 
    current_user: dict = Depends(get_current_user)
):
    """
    ROBUST Bulk create events (admin-only endpoint) - Uses proven create_event logic
    """
    if current_user['role'] != UserRole.ADMIN:
        raise HTTPException(status_code=403, detail="Not authorized. Admin access required.")
    
    success_count = 0
    error_count = 0
    errors = []
    created_events = []
    
    logger.info(f"Admin {current_user['email']} initiating ROBUST bulk event creation for {len(bulk_events.events)} events using proven create_event logic")
    
    # Use the proven single event creation logic for each event
    for i, event_data in enumerate(bulk_events.events):
        try:
            # Create event using the proven single event creation function
            response_event = await create_event(event_data, current_user)
            created_events.append(response_event)
            success_count += 1
            logger.info(f"‚úÖ Successfully created event {i+1}: '{event_data.title}' (ID: {response_event.id})")
            
        except Exception as e:
            error_count += 1
            error_msg = str(e)
            logger.error(f"‚ùå Error creating event {i+1} ({event_data.title}): {error_msg}")
            errors.append({
                "event_index": i + 1,
                "event_title": event_data.title,
                "error": error_msg
            })
            continue
    
    logger.info(f"‚úÖ Robust bulk event creation completed. Success: {success_count}, Errors: {error_count}")
    
    # Clear event cache after bulk import
    event_cache.clear()
    logger.info(f"üßπ Cleared event cache to ensure new events appear in API")
    
    return BulkEventResponse(
        success_count=success_count,
        error_count=error_count,
        errors=errors,
        created_events=created_events
    )

def ensure_unique_slug_failsafe(cursor, base_slug: str, placeholder: str) -> str:
    """Enhanced failsafe slug uniqueness check with production-ready error handling"""
    import time
    
    if not base_slug:
        fallback_slug = f"event-{int(time.time())}"
        logger.warning(f"üîÑ Empty base_slug provided, using fallback: {fallback_slug}")
        return fallback_slug
    
    try:
        # Enhanced approach: check if slug exists with better error handling
        logger.debug(f"üîç Checking slug uniqueness for: '{base_slug}'")
        
        if placeholder == "?":
            cursor.execute("SELECT COUNT(*) FROM events WHERE slug = ?", (base_slug,))
        else:
            cursor.execute("SELECT COUNT(*) FROM events WHERE slug = %s", (base_slug,))
        
        result = cursor.fetchone()
        logger.debug(f"üîç Slug query result: {result} (type: {type(result)})")
        
        # Enhanced result handling with better validation
        count = 0
        if result is None:
            logger.debug("üîç Query returned None, assuming count = 0")
            count = 0
        elif isinstance(result, (list, tuple)) and len(result) > 0:
            raw_value = result[0]
            count = int(raw_value) if raw_value is not None else 0
            logger.debug(f"üîç Extracted count from tuple/list: {count}")
        elif hasattr(result, '__getitem__'):
            try:
                raw_value = result[0]
                count = int(raw_value) if raw_value is not None else 0
                logger.debug(f"üîç Extracted count from indexable: {count}")
            except (IndexError, KeyError, TypeError):
                logger.warning(f"‚ö†Ô∏è Could not extract count from indexable result: {result}")
                count = 0
        else:
            try:
                count = int(result) if result is not None else 0
                logger.debug(f"üîç Used result directly as count: {count}")
            except (ValueError, TypeError):
                logger.warning(f"‚ö†Ô∏è Could not convert result to int: {result}")
                count = 0
        
        if count > 0:
            # Slug exists, create unique variant
            suffix = str(int(time.time()))[-6:]  # Last 6 digits for brevity
            unique_slug = f"{base_slug}-{suffix}"
            logger.info(f"‚úÖ Slug '{base_slug}' exists ({count} times), generated unique: '{unique_slug}'")
            return unique_slug
        else:
            logger.info(f"‚úÖ Slug '{base_slug}' is unique, using as-is")
            return base_slug
            
    except Exception as e:
        logger.error(f"‚ùå Slug uniqueness check failed for '{base_slug}': {type(e).__name__}: {e}")
        # Enhanced emergency fallback with more detail
        emergency_slug = f"{base_slug}-{int(time.time())}"
        logger.warning(f"üö® Using emergency fallback slug: '{emergency_slug}'")
        return emergency_slug
@app.get("/debug/database-info")
async def debug_database_info():
    """Debug endpoint to check database configuration and status"""
    try:
        db_info = {
            "is_production": IS_PRODUCTION,
            "database_url_set": bool(DB_URL),
            "database_url_prefix": DB_URL[:20] + "..." if DB_URL else None,
            "database_type": "postgresql" if IS_PRODUCTION and DB_URL else "sqlite",
            "render_env": bool(os.getenv("RENDER", False)),
        }
        
        # Test database connection and get table info
        with get_db() as conn:
            cursor = conn.cursor()
            
            if IS_PRODUCTION and DB_URL:
                # PostgreSQL queries
                cursor.execute("""
                    SELECT table_name 
                    FROM information_schema.tables 
                    WHERE table_schema = 'public'
                """)
                tables = [row[0] for row in cursor.fetchall()]
                
                # Check events table structure
                cursor.execute("""
                    SELECT column_name, data_type 
                    FROM information_schema.columns 
                    WHERE table_name = 'events'
                    ORDER BY ordinal_position
                """)
                event_columns = [{"name": row[0], "type": row[1]} for row in cursor.fetchall()]
                
                # Count events
                cursor.execute("SELECT COUNT(*) FROM events")
                event_count = get_count_from_result(cursor.fetchone())
                
                db_info.update({
                    "tables": tables,
                    "event_columns": event_columns,
                    "event_count": event_count,
                    "ux_fields_present": all(
                        col["name"] in ["fee_required", "event_url", "host_name"] 
                        for col in event_columns 
                        if col["name"] in ["fee_required", "event_url", "host_name"]
                    )
                })
                
            else:
                # SQLite queries
                cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
                tables = [row[0] for row in cursor.fetchall()]
                
                cursor.execute("PRAGMA table_info(events)")
                event_columns = [{"name": row[1], "type": row[2]} for row in cursor.fetchall()]
                
                cursor.execute("SELECT COUNT(*) FROM events")
                event_count = get_count_from_result(cursor.fetchone())
                
                db_info.update({
                    "tables": tables,
                    "event_columns": event_columns,
                    "event_count": event_count,
                    "ux_fields_present": all(
                        col["name"] in [c["name"] for c in event_columns]
                        for col in event_columns 
                        if col["name"] in ["fee_required", "event_url", "host_name"]
                    )
                })
        
        return db_info
        
    except Exception as e:
        return {
            "error": str(e),
            "is_production": IS_PRODUCTION,
            "database_url_set": bool(DB_URL),
            "database_type": "postgresql" if IS_PRODUCTION and DB_URL else "sqlite",
        }

@app.post("/admin/migrate-database")
async def migrate_production_database():
    """Trigger PostgreSQL database migration (production only)"""
    try:
        if not IS_PRODUCTION or not DB_URL:
            return {
                "status": "skipped",
                "message": "Migration only available in production with PostgreSQL",
                "is_production": IS_PRODUCTION,
                "has_db_url": bool(DB_URL)
            }
        
        # Import and run the migration
        from migrate_production_postgres import main as migrate_db
        migrate_db()
        
        return {
            "status": "success", 
            "message": "PostgreSQL database migration completed successfully"
        }
        
    except Exception as e:
        logger.error(f"Migration failed: {e}")
        return {
            "status": "error", 
            "message": f"Migration failed: {str(e)}"
        }
@app.post("/admin/fix-production-database")
async def fix_production_database():
    """Fix critical production database schema issues causing bulk import failures"""
    try:
        if not IS_PRODUCTION or not DB_URL:
            return {
                "status": "skipped",
                "message": "Database fix only available in production with PostgreSQL",
                "is_production": IS_PRODUCTION,
                "has_db_url": bool(DB_URL)
            }
        
        logger.info("üöÄ Starting production database schema fix via API")
        
        # Run the database fix directly
        with get_db() as conn:
            cursor = conn.cursor()
            
            # Check current schema
            try:
                cursor.execute("""
                    SELECT column_name 
                    FROM information_schema.columns 
                    WHERE table_name = 'events' AND table_schema = 'public'
                """)
                current_columns = [row[0] for row in cursor.fetchall()]
                logger.info(f"üìä Current table has {len(current_columns)} columns")
            except Exception as e:
                logger.error(f"‚ùå Could not check current schema: {e}")
                current_columns = []
            
            # Define missing columns to add
            required_columns = {
                'fee_required': 'TEXT',
                'event_url': 'TEXT', 
                'host_name': 'TEXT',
                'organizer_url': 'TEXT',
                'price': 'DECIMAL(10,2) DEFAULT 0.0',
                'currency': 'VARCHAR(3) DEFAULT \'USD\'',
                'slug': 'TEXT',
                'short_description': 'TEXT',
                'city': 'VARCHAR(100)',
                'state': 'VARCHAR(50)',
                'country': 'VARCHAR(50) DEFAULT \'USA\'',
                'geo_hash': 'VARCHAR(20)',
                'is_published': 'BOOLEAN DEFAULT TRUE',
                'start_datetime': 'TIMESTAMP',
                'end_datetime': 'TIMESTAMP',
                'updated_at': 'TIMESTAMP',
                'interest_count': 'INTEGER DEFAULT 0',
                'view_count': 'INTEGER DEFAULT 0'
            }
            
            added_columns = []
            failed_columns = []
            
            # Add missing columns
            for column_name, column_spec in required_columns.items():
                if column_name not in current_columns:
                    try:
                        sql = f"ALTER TABLE events ADD COLUMN {column_name} {column_spec}"
                        logger.info(f"üîß Adding column: {column_name}")
                        cursor.execute(sql)
                        added_columns.append(column_name)
                    except Exception as e:
                        logger.error(f"‚ùå Failed to add column {column_name}: {e}")
                        failed_columns.append(column_name)
            
            # Update NULL values
            null_updates = [
                "UPDATE events SET fee_required = '' WHERE fee_required IS NULL",
                "UPDATE events SET event_url = '' WHERE event_url IS NULL",
                "UPDATE events SET host_name = '' WHERE host_name IS NULL",
                "UPDATE events SET short_description = '' WHERE short_description IS NULL",
                "UPDATE events SET interest_count = 0 WHERE interest_count IS NULL",
                "UPDATE events SET view_count = 0 WHERE view_count IS NULL",
                "UPDATE events SET is_published = TRUE WHERE is_published IS NULL"
            ]
            
            updated_fields = []
            for sql in null_updates:
                try:
                    cursor.execute(sql)
                    if cursor.rowcount > 0:
                        field = sql.split("SET ")[1].split(" =")[0]
                        updated_fields.append(f"{field}({cursor.rowcount})")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è NULL update failed: {e}")
            
            # Commit changes
            conn.commit()
            logger.info("‚úÖ Database schema fix completed and committed")
            
        return {
            "status": "success",
            "message": "Production database schema fixed successfully",
            "details": {
                "columns_added": added_columns,
                "columns_failed": failed_columns,
                "null_updates": updated_fields,
                "total_columns_after": len(current_columns) + len(added_columns)
            }
        }
        
    except Exception as e:
        logger.error(f"‚ùå Database fix failed: {e}")
        logger.error(f"üìù Traceback: {traceback.format_exc()}")
        return {
            "status": "error", 
            "message": f"Database fix failed: {str(e)}"
        }

@app.post("/admin/test-bulk-import-fix")
async def test_bulk_import_fix():
    """Test the PostgreSQL RETURNING clause fix for bulk import (admin only)"""
    try:
        if not IS_PRODUCTION or not DB_URL:
            return {
                "status": "skipped",
                "message": "Test only available in production with PostgreSQL",
                "is_production": IS_PRODUCTION,
                "has_db_url": bool(DB_URL)
            }
        
        with get_db() as conn:
            cursor = conn.cursor()
            
            # Test RETURNING clause with a simple insert
            test_query = """
                INSERT INTO events (title, description, date, start_time, category, address, lat, lng, created_by)
                VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
                RETURNING id
            """
            
            test_values = [
                "BULK IMPORT TEST EVENT",
                "Test event to verify RETURNING clause works",
                (datetime.utcnow() + timedelta(days=30)).strftime('%Y-%m-%d'),
                "12:00",
                "Educational & Business",
                "Test Address, Test City, TS 12345",
                40.7128,
                -74.0060,
                1  # Assuming admin user ID 1 exists
            ]
            
            cursor.execute(test_query, test_values)
            result = cursor.fetchone()
            
            if result is not None:
                try:
                    if hasattr(result, 'get') and 'id' in result:
                        event_id = int(result['id'])
                        method = "RealDictRow['id']"
                    elif hasattr(result, '__getitem__'):
                        event_id = int(result[0])
                        method = "result[0]"
                    else:
                        event_id = int(result)
                        method = "direct conversion"
                    
                    # Clean up test event
                    cursor.execute("DELETE FROM events WHERE id = %s", (event_id,))
                    conn.commit()
                    
                    return {
                        "status": "success",
                        "message": f"RETURNING clause fix working correctly! Got ID: {event_id} via {method}",
                        "event_id": event_id,
                        "extraction_method": method,
                        "result_type": str(type(result))
                    }
                    
                except Exception as extract_error:
                    return {
                        "status": "error",
                        "message": f"Failed to extract ID from result: {extract_error}",
                        "result": str(result),
                        "result_type": str(type(result))
                    }
            else:
                return {
                    "status": "error", 
                    "message": "RETURNING clause returned no result"
                }
                
    except Exception as e:
        logger.error(f"Bulk import test failed: {e}")
        return {
            "status": "error",
            "message": f"Test failed: {str(e)}"
        }

def sanitize_ux_field(value):
    """Convert None to empty string for UX fields to avoid NULLs in DB."""
    logger.info(f"sanitize_ux_field called with: {value!r} (type: {type(value)})")
    result = value if value is not None else ""
    logger.info(f"sanitize_ux_field returning: {result!r} (type: {type(result)})")
    return result

def convert_event_datetime_fields(event_dict):
    """Convert datetime objects to ISO format strings for API response"""
    from datetime import datetime, timezone
    
    # Handle NULL end_time values specifically to prevent validation errors
    if 'end_time' in event_dict and event_dict['end_time'] is None:
        event_dict['end_time'] = ""  # Convert None to empty string
    
    # Handle other potential NULL string fields
    string_fields = ['end_date', 'short_description', 'fee_required', 'event_url', 'host_name', 'organizer_url', 'slug']
    for field in string_fields:
        if field in event_dict and event_dict[field] is None:
            event_dict[field] = ""  # Convert None to empty string for optional string fields
    
    datetime_fields = ['created_at', 'updated_at', 'start_datetime', 'end_datetime']
    for field in datetime_fields:
        if field in event_dict and isinstance(event_dict[field], datetime):
            event_dict[field] = event_dict[field].isoformat()
        elif field in event_dict and event_dict[field] is not None and not isinstance(event_dict[field], str):
            # Convert any non-string datetime-like values to strings
            try:
                event_dict[field] = str(event_dict[field])
            except:
                event_dict[field] = None
        elif field in event_dict and event_dict[field] is None:
            # Handle NULL values - set a default for required fields
            if field == 'created_at':
                # For created_at, use current timestamp if None
                event_dict[field] = datetime.now(timezone.utc).isoformat()
            else:
                # For optional fields, keep as None
                event_dict[field] = None
    
    # Ensure counters are integers
    event_dict['interest_count'] = event_dict.get('interest_count', 0) or 0
    event_dict['view_count'] = event_dict.get('view_count', 0) or 0
    
    return event_dict

# ===== SEO ENDPOINTS =====

@app.get("/api/seo/events/{event_id}")
async def get_event_seo_data(event_id: int):
    """Get complete SEO data package for event"""
    
    if not SEOEventProcessor:
        raise HTTPException(status_code=501, detail="SEO utilities not available")
    
    with get_db() as conn:
        cursor = conn.cursor()
        
        # Get event with all SEO fields
        cursor.execute('''
            SELECT 
                id, title, slug, description, short_description,
                date, start_time, end_time, end_date,
                start_datetime, end_datetime,
                category, address, city, state, country,
                lat, lng, price, currency, fee_required,
                event_url, host_name, organizer_url,
                created_by, created_at, updated_at,
                interest_count, view_count, is_published
            FROM events 
            WHERE id = ? AND is_published = 1
        ''', (event_id,))
        
        event_row = cursor.fetchone()
        if not event_row:
            raise HTTPException(status_code=404, detail="Event not found")
        
        # Convert to dict
        event_dict = dict(event_row)
        
        # Generate SEO data
        processor = SEOEventProcessor(base_url="https://todo-events.com")
        seo_data = processor.generate_full_seo_data(event_dict)
        
        return seo_data

@app.get("/api/seo/events/by-slug/{slug}")
async def get_event_by_slug(slug: str):
    """Get event data by SEO slug"""
    
    with get_db() as conn:
        cursor = conn.cursor()
        
        # Use database-specific syntax for is_published
        if IS_PRODUCTION and DB_URL:
            # PostgreSQL - use boolean true
            cursor.execute('''
                SELECT 
                    id, title, slug, description, short_description,
                    date, start_time, end_time, end_date,
                    start_datetime, end_datetime,
                    category, address, city, state, country,
                    lat, lng, price, currency, fee_required,
                    event_url, host_name, organizer_url,
                    created_by, created_at, updated_at,
                    interest_count, view_count, is_published
                FROM events 
                WHERE slug = %s AND (is_published = true OR is_published IS NULL)
            ''', (slug,))
        else:
            # SQLite - use integer 1
            cursor.execute('''
                SELECT 
                    id, title, slug, description, short_description,
                    date, start_time, end_time, end_date,
                    start_datetime, end_datetime,
                    category, address, city, state, country,
                    lat, lng, price, currency, fee_required,
                    event_url, host_name, organizer_url,
                    created_by, created_at, updated_at,
                    interest_count, view_count, is_published
                FROM events 
                WHERE slug = ? AND (is_published = 1 OR is_published IS NULL)
            ''', (slug,))
        
        event_row = cursor.fetchone()
        if not event_row:
            raise HTTPException(status_code=404, detail="Event not found")
        
        event_dict = dict(event_row)
        return event_dict

@app.get("/api/seo/events/location/{state}/{city}")
async def get_events_by_location(
    state: str, 
    city: str,
    limit: int = 100,
    offset: int = 0
):
    """Get events by state and city for geographic SEO"""
    
    with get_db() as conn:
        cursor = conn.cursor()
        
        # Use database-specific syntax for location queries
        if IS_PRODUCTION and DB_URL:
            # PostgreSQL
            cursor.execute('''
                SELECT 
                    id, title, slug, description, short_description,
                    date, start_time, end_time, start_datetime, end_datetime,
                    category, address, city, state, country,
                    lat, lng, price, currency,
                    host_name, interest_count, view_count
                FROM events 
                WHERE LOWER(state) = %s 
                AND LOWER(city) = %s 
                AND (is_published = true OR is_published IS NULL)
                AND CAST(date AS DATE) >= CURRENT_DATE
                ORDER BY date ASC, start_time ASC
                LIMIT %s OFFSET %s
            ''', (state.lower(), city.lower(), limit, offset))
        else:
            # SQLite
            cursor.execute('''
                SELECT 
                    id, title, slug, description, short_description,
                    date, start_time, end_time, start_datetime, end_datetime,
                    category, address, city, state, country,
                    lat, lng, price, currency,
                    host_name, interest_count, view_count
                FROM events 
                WHERE LOWER(state) = ? 
                AND LOWER(city) = ? 
                AND (is_published = 1 OR is_published IS NULL)
                AND date::date >= CURRENT_DATE
                ORDER BY date ASC, start_time ASC
                LIMIT ? OFFSET ?
            ''', (state.lower(), city.lower(), limit, offset))
        
        events = [dict(row) for row in cursor.fetchall()]
        
        # Get total count with database-specific syntax
        if IS_PRODUCTION and DB_URL:
            # PostgreSQL
            cursor.execute('''
                SELECT COUNT(*) FROM events 
                WHERE LOWER(state) = %s 
                AND LOWER(city) = %s 
                AND (is_published = true OR is_published IS NULL)
                AND CAST(date AS DATE) >= CURRENT_DATE
            ''', (state.lower(), city.lower()))
        else:
            # SQLite
            cursor.execute('''
                SELECT COUNT(*) FROM events 
                WHERE LOWER(state) = ? 
                AND LOWER(city) = ? 
                AND (is_published = 1 OR is_published IS NULL)
                AND date::date >= CURRENT_DATE
            ''', (state.lower(), city.lower()))
        
        total = get_count_from_result(cursor.fetchone())
        
        return {
            "events": events,
            "total": total,
            "limit": limit,
            "offset": offset,
            "location": {
                "city": city.title(),
                "state": state.upper(),
                "slug": f"{state.lower()}/{city.lower()}"
            }
        }
@app.get("/api/events/{event_id}/share-card")
async def get_event_share_card(event_id: int):
    """Generate and serve pre-generated share card data for SEO and social media"""
    
    try:
        with get_db() as conn:
            cursor = conn.cursor()
            
            # Use database-specific syntax for share card
            if IS_PRODUCTION and DB_URL:
                # PostgreSQL
                cursor.execute('''
                    SELECT id, title, description, date, start_time, end_time, address, 
                           city, state, category, secondary_category, verified, 
                           view_count, interest_count, host_name, fee_required
                    FROM events 
                    WHERE id = %s AND (is_published = true OR is_published IS NULL)
                ''', (event_id,))
            else:
                # SQLite
                cursor.execute('''
                    SELECT id, title, description, date, start_time, end_time, address, 
                           city, state, category, secondary_category, verified, 
                           view_count, interest_count, host_name, fee_required
                    FROM events 
                    WHERE id = ? AND (is_published = 1 OR is_published IS NULL)
                ''', (event_id,))
            
            event_row = cursor.fetchone()
            if not event_row:
                raise HTTPException(status_code=404, detail="Event not found")
            
            event_dict = dict(event_row)
            
            # Return enhanced JSON data with proper image URL for SEO
            share_card_data = {
                "type": "share_card_image",
                "event_id": event_id,
                "title": event_dict["title"],
                "date": event_dict["date"],
                "time": f"{event_dict['start_time']}" + (f" - {event_dict['end_time']}" if event_dict.get('end_time') else ""),
                "location": f"{event_dict.get('city', '')}, {event_dict.get('state', '')}".strip(', '),
                "address": event_dict.get("address", ""),
                "category": event_dict["category"],
                "secondary_category": event_dict.get("secondary_category"),
                "description": event_dict["description"][:120] + "..." if len(event_dict["description"]) > 120 else event_dict["description"],
                "verified": bool(event_dict.get("verified", False)),
                "view_count": event_dict.get("view_count", 0),
                "interest_count": event_dict.get("interest_count", 0),
                "host_name": event_dict.get("host_name"),
                "is_free": not bool(event_dict.get("fee_required")),
                "image_url": f"https://todoevents-backend.onrender.com/api/events/{event_id}/share-card.png",
                "meta": {
                    "width": 800,
                    "height": 600,
                    "format": "PNG",
                    "seo_optimized": True,
                    "social_ready": True
                }
            }
            
            return share_card_data
            
    except Exception as e:
        logger.error(f"Error generating share card for event {event_id}: {str(e)}")
        raise HTTPException(status_code=500, detail="Error generating share card")

@app.get("/api/events/{event_id}/share-card.png")
async def get_event_share_card_png(event_id: int):
    """Serve actual PNG image for share cards (placeholder for future implementation)"""
    
    try:
        # For now, redirect to a placeholder image service
        # In production, this would generate and cache actual PNG images using PIL, Playwright, etc.
        
        with get_db() as conn:
            cursor = conn.cursor()
            
            if IS_PRODUCTION and DB_URL:
                cursor.execute('''
                    SELECT title, category, verified FROM events 
                    WHERE id = %s AND (is_published = true OR is_published IS NULL)
                ''', (event_id,))
            else:
                cursor.execute('''
                    SELECT title, category, verified FROM events 
                    WHERE id = ? AND (is_published = 1 OR is_published IS NULL)
                ''', (event_id,))
            
            event_row = cursor.fetchone()
            if not event_row:
                raise HTTPException(status_code=404, detail="Event not found")
            
            # Placeholder URL with event info
            event_title = event_row['title'][:30] if len(event_row['title']) > 30 else event_row['title']
            placeholder_url = f"https://via.placeholder.com/800x600/3B82F6/FFFFFF?text=Event+{event_id}%0A{event_title.replace(' ', '+')}"
            
            return RedirectResponse(url=placeholder_url, status_code=302)
            
    except Exception as e:
        logger.error(f"Error serving PNG share card for event {event_id}: {str(e)}")
        # Fallback to generic placeholder
        return RedirectResponse(url="https://via.placeholder.com/800x600/3B82F6/FFFFFF?text=TodoEvents", status_code=302)

@app.post("/api/seo/migrate-events")
async def migrate_events_for_seo(background_tasks: BackgroundTasks):
    """Trigger production SEO migration for existing events"""
    
    def run_migration():
        try:
            # Note: production_seo_migration.py was removed - using populate_production_seo_fields instead
            from populate_production_seo_fields import populate_seo_data
            result = populate_seo_data()
            logger.info(f"Production SEO migration completed: {result}")
        except Exception as e:
            logger.error(f"Production SEO migration failed: {e}")
    
    background_tasks.add_task(run_migration)
    
    return {
        "message": "Production SEO migration started in background",
        "status": "processing",
        "info": "Processing all existing events to populate missing SEO fields including slugs"
    }

@app.post("/api/seo/migrate-events-sync")
async def migrate_events_for_seo_sync():
    """Execute production SEO migration synchronously with real-time results"""
    try:
        # Note: production_seo_migration.py was removed - using populate_production_seo_fields instead
        from populate_production_seo_fields import populate_seo_data
        result = populate_seo_data()
        
        return {
            "success": True,
            "message": "Production SEO migration completed successfully",
            "result": result
        }
        
    except Exception as e:
        logger.error(f"Production SEO migration failed: {e}")
        raise HTTPException(
            status_code=500, 
            detail=f"Migration failed: {str(e)}"
        )

@app.post("/api/seo/populate-production-fields")
async def populate_production_seo_fields():
    """Populate SEO fields for all events in production PostgreSQL database"""
    try:
        from populate_production_seo_fields import populate_seo_data
        
        # Call the population function directly 
        populate_seo_data()
        
        return {
            "success": True,
            "message": "Production SEO field population completed successfully",
            "info": "All events have been updated with slugs, city/state, and other SEO fields"
        }
        
    except Exception as e:
        logger.error(f"Production SEO population failed: {e}")
        raise HTTPException(
            status_code=500, 
            detail=f"SEO population failed: {str(e)}"
        )

@app.get("/api/seo/sitemap/events")
async def get_events_sitemap():
    """Get sitemap XML for events with proper SEO URLs"""
    
    with get_db() as conn:
        cursor = conn.cursor()
        
        # Use proper database-specific syntax
        if IS_PRODUCTION and DB_URL:
            # PostgreSQL - cast date text to date for comparison
            cursor.execute('''
                SELECT slug, city, state, updated_at, date
                FROM events 
                WHERE (is_published = true OR is_published IS NULL)
                AND slug IS NOT NULL
                AND CAST(date AS DATE) >= (CURRENT_DATE - INTERVAL '30 days')::DATE
                ORDER BY updated_at DESC
            ''')
        else:
            # SQLite
            cursor.execute('''
                SELECT slug, city, state, updated_at, date
                FROM events 
                WHERE (is_published = 1 OR is_published IS NULL)
                AND slug IS NOT NULL
                AND date::date >= CURRENT_DATE - INTERVAL '30 days'
                ORDER BY updated_at DESC
            ''')
        
        events = cursor.fetchall()
        
        # Generate sitemap XML
        sitemap_entries = []
        for event in events:
            slug, city, state, updated_at, event_date = event
            
            # Generate URL based on location
            if city and state:
                url = f"https://todo-events.com/us/{state.lower()}/{slugify(city)}/events/{slug}"
            else:
                url = f"https://todo-events.com/events/{slug}"
            
            # Use updated_at or event date for lastmod
            lastmod = updated_at or event_date
            
            sitemap_entries.append({
                "url": url,
                "lastmod": lastmod,
                "changefreq": "weekly",
                "priority": "0.8"
            })
        
        return {
            "entries": sitemap_entries,
            "count": len(sitemap_entries),
            "generated_at": datetime.now().isoformat()
        }

@app.get("/api/seo/validate/{event_id}")
async def validate_event_seo(event_id: int):
    """Validate event SEO completeness"""
    
    if not SEOEventProcessor:
        return {"error": "SEO utilities not available"}
    
    with get_db() as conn:
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT 
                id, title, slug, description, short_description,
                date, start_time, end_time, category, address, 
                city, state, lat, lng, host_name, is_published
            FROM events 
            WHERE id = ?
        ''', (event_id,))
        
        event_row = cursor.fetchone()
        if not event_row:
            raise HTTPException(status_code=404, detail="Event not found")
        
        event_dict = dict(event_row)
        
        # Import validation function
        from seo_utils import validate_event_data
        issues = validate_event_data(event_dict)
        
        return {
            "event_id": event_id,
            "is_seo_ready": len(issues) == 0,
            "issues": issues,
            "suggestions": [
                "Add missing required fields",
                "Generate SEO slug if missing", 
                "Add geographic information",
                "Include host/organizer details"
            ] if issues else ["Event is SEO-ready!"]
        }

@app.post("/api/fix/null-end-times")
async def fix_null_end_times():
    """Fix NULL end_time values in the database"""
    try:
        with get_db() as conn:
            c = conn.cursor()
            
            # Check how many events have NULL end_time
            c.execute("SELECT COUNT(*) FROM events WHERE end_time IS NULL")
            result = c.fetchone()
            null_count = result[0] if isinstance(result, (tuple, list)) else result.get("COUNT(*)", 0) if result else 0
            
            if null_count == 0:
                return {
                    "status": "success",
                    "message": "No NULL end_time values found",
                    "updated_count": 0
                }
            
            # Update NULL end_time values to empty string
            c.execute("UPDATE events SET end_time = '' WHERE end_time IS NULL")
            updated_count = c.rowcount
            conn.commit()
            
            # Verify the fix
            c.execute("SELECT COUNT(*) FROM events WHERE end_time IS NULL")
            result = c.fetchone()
            remaining_nulls = result[0] if isinstance(result, (tuple, list)) else result.get("COUNT(*)", 0) if result else 0
            
            return {
                "status": "success",
                "message": f"Fixed {updated_count} NULL end_time values",
                "updated_count": updated_count,
                "remaining_nulls": remaining_nulls
            }
            
    except Exception as e:
        logger.error(f"Error fixing NULL end_time values: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error fixing NULL end_time values: {str(e)}")
@app.get("/debug/events-raw")
async def debug_events_raw():
    """Debug endpoint to test events query without caching"""
    try:
        with get_db() as conn:
            cursor = conn.cursor()
            
            # Database-specific date comparison logic
            if IS_PRODUCTION and DB_URL:
                date_comparison = "date::date >= CURRENT_DATE"
            else:
                date_comparison = "date >= date('now')"
            
            # Test the exact same query as the events endpoint
            query = f"""
                SELECT id, title, description, short_description, date, start_time, end_time, end_date, 
                       category, address, city, state, country, lat, lng, recurring, frequency, created_by, created_at,
                       COALESCE(interest_count, 0) as interest_count,
                       COALESCE(view_count, 0) as view_count,
                       fee_required, price, currency, event_url, host_name, organizer_url, slug, is_published,
                       start_datetime, end_datetime, updated_at
                FROM events 
                ORDER BY 
                    CASE 
                        WHEN {date_comparison} THEN 0  -- Future/today events first
                        ELSE 1                           -- Past events later  
                    END,
                    date ASC, start_time ASC
                LIMIT 500
            """
            
            cursor.execute(query)
            events = cursor.fetchall()
            
            # Process results
            result = []
            errors = []
            for i, event in enumerate(events):
                try:
                    column_names = [
                        'id', 'title', 'description', 'short_description', 'date', 'start_time', 'end_time',
                        'end_date', 'category', 'address', 'city', 'state', 'country', 'lat', 'lng', 'recurring',
                        'frequency', 'created_by', 'created_at', 'interest_count', 'view_count',
                        'fee_required', 'price', 'currency', 'event_url', 'host_name', 'organizer_url', 'slug', 'is_published',
                        'start_datetime', 'end_datetime', 'updated_at'
                    ]
                    
                    event_dict = dict(zip(column_names, event))
                    event_dict = convert_event_datetime_fields(event_dict)
                    
                    # Ensure counters are integers
                    event_dict['interest_count'] = int(event_dict.get('interest_count', 0) or 0)
                    event_dict['view_count'] = int(event_dict.get('view_count', 0) or 0)
                    
                    result.append(event_dict)
                    
                except Exception as event_error:
                    errors.append(f"Error processing event {i}: {event_error}")
                    continue
            
            return {
                "query_used": query,
                "total_events_retrieved": len(events),
                "processed_events": len(result),
                "processing_errors": errors,
                "database_type": "PostgreSQL" if IS_PRODUCTION and DB_URL else "SQLite",
                "sample_events": result[:3] if result else [],
                "cache_key_that_would_be_used": f"events:all:all:500:0:None:None:25.0"
            }
            
    except Exception as e:
        logger.error(f"Debug events query failed: {str(e)}")
        return {
            "error": str(e),
            "database_type": "PostgreSQL" if IS_PRODUCTION and DB_URL else "SQLite"
        }

@app.post("/debug/clear-cache")
async def debug_clear_cache():
    """Debug endpoint to clear the events cache"""
    try:
        cache_stats_before = event_cache.stats()
        event_cache.clear()
        cache_stats_after = event_cache.stats()
        
        return {
            "message": "Cache cleared successfully",
            "cache_stats_before": cache_stats_before,
            "cache_stats_after": cache_stats_after
        }
    except Exception as e:
        logger.error(f"Failed to clear cache: {str(e)}")
        return {"error": str(e)}
@app.get("/debug/test-event-urls")
async def debug_test_event_urls():
    """Test URL generation for specific events to ensure Google indexing format"""
    try:
        with get_db() as conn:
            cursor = conn.cursor()
            
            # Get the specific events mentioned by the user
            cursor.execute('''
                SELECT id, title, slug, city, state, date, created_at, updated_at
                FROM events 
                WHERE slug IN ('wolly-worm-festival-camargo', 'camargo-woolly-worm-festival-camargo')
                OR title LIKE '%Wolly Worm%' OR title LIKE '%Woolly Worm%'
                ORDER BY id
            ''')
            
            events = cursor.fetchall()
            
            if not events:
                return {
                    "message": "No matching events found",
                    "searched_slugs": ['wolly-worm-festival-camargo', 'camargo-woolly-worm-festival-camargo']
                }
            
            results = []
            base_domain = "https://todo-events.com"
            
            for event in events:
                event_id, title, slug, city, state, date, created_at, updated_at = event
                
                # Generate all URL formats for this event
                urls = {}
                
                # 1. Canonical URL (location-based if city/state available)
                if city and state:
                    city_slug = slugify(city)
                    state_lower = state.lower()
                    urls['canonical'] = f"{base_domain}/us/{state_lower}/{city_slug}/events/{slug}"
                    urls['canonical_priority'] = "0.9"
                else:
                    urls['canonical'] = f"{base_domain}/events/{slug}"
                    urls['canonical_priority'] = "0.85"
                
                # 2. Short URL (compatibility)
                urls['short'] = f"{base_domain}/e/{slug}"
                urls['short_priority'] = "0.7"
                
                # 3. Date-indexed URL
                try:
                    if date:
                        date_obj = datetime.fromisoformat(str(date))
                        year = date_obj.strftime('%Y')
                        month = date_obj.strftime('%m')
                        day = date_obj.strftime('%d')
                        urls['date_indexed'] = f"{base_domain}/events/{year}/{month}/{day}/{slug}"
                        urls['date_indexed_priority'] = "0.75"
                except:
                    urls['date_indexed'] = "Error parsing date"
                
                # 4. SEO metadata
                seo_data = {
                    "title": title,
                    "slug": slug,
                    "city": city,
                    "state": state,
                    "date": date,
                    "google_indexable": bool(slug and title),
                    "location_specific": bool(city and state)
                }
                
                results.append({
                    "event_id": event_id,
                    "title": title,
                    "slug": slug,
                    "urls": urls,
                    "seo_data": seo_data,
                    "created_at": created_at,
                    "updated_at": updated_at
                })
            
            return {
                "status": "success",
                "events_found": len(results),
                "events": results,
                "url_format_explanation": {
                    "canonical": "Primary URL for Google indexing - location-based if city/state available",
                    "short": "Compatibility URL for sharing and redirects",
                    "date_indexed": "Time-based URL for event discovery",
                    "priority_explanation": "Higher priority = more important for search engines"
                }
            }
            
    except Exception as e:
        logger.error(f"Error testing event URLs: {str(e)}")
        return {
            "status": "error",
            "error": str(e),
            "traceback": traceback.format_exc()
        }

@app.get("/admin/analytics/metrics")
async def get_analytics_metrics(
    current_user: dict = Depends(get_current_user),
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    exclude_users: Optional[str] = None,  # Comma-separated user IDs
    category: Optional[str] = None
):
    """Get comprehensive analytics metrics with filtering"""
    # Check admin permission
    if current_user.get('role') != 'admin':
        raise HTTPException(status_code=403, detail="Admin access required")
    
    try:
        with get_db() as conn:
            cursor = conn.cursor()
            placeholder = get_placeholder()
            
            # Parse filters
            excluded_user_ids = []
            if exclude_users:
                try:
                    excluded_user_ids = [int(uid.strip()) for uid in exclude_users.split(',') if uid.strip()]
                except ValueError:
                    excluded_user_ids = []
            
            # Build WHERE conditions
            conditions = ["1=1"]
            params = []
            
            if start_date:
                conditions.append(f"date >= {placeholder}")
                params.append(start_date)
            if end_date:
                conditions.append(f"date <= {placeholder}")
                params.append(end_date)
            if excluded_user_ids:
                placeholders = ','.join([placeholder for _ in excluded_user_ids])
                conditions.append(f"created_by NOT IN ({placeholders})")
                params.extend(excluded_user_ids)
            if category:
                conditions.append(f"category = {placeholder}")
                params.append(category)
            
            where_clause = " AND ".join(conditions)
            
            # Total events
            cursor.execute(f"""
                SELECT COUNT(*) FROM events 
                WHERE {where_clause}
            """, params)
            total_events = get_count_from_result(cursor.fetchone())
            
            # Total users
            cursor.execute("SELECT COUNT(*) FROM users")
            total_users = get_count_from_result(cursor.fetchone())
            
            # Active hosts (users with at least one event in the last month)
            one_month_ago = (datetime.utcnow() - timedelta(days=30)).strftime('%Y-%m-%d')
            host_conditions = [f"date >= {placeholder}"]
            host_params = [one_month_ago]
            if excluded_user_ids:
                host_placeholders = ','.join([placeholder for _ in excluded_user_ids])
                host_conditions.append(f"created_by NOT IN ({host_placeholders})")
                host_params.extend(excluded_user_ids)
            if category:
                host_conditions.append(f"category = {placeholder}")
                host_params.append(category)
            host_where = " AND ".join(host_conditions)
            
            cursor.execute(f"""
                SELECT COUNT(DISTINCT created_by) FROM events 
                WHERE {host_where}
            """, host_params)
            active_hosts = get_count_from_result(cursor.fetchone())
            
            # Events by category
            cursor.execute(f"""
                SELECT category, COUNT(*) 
                FROM events 
                WHERE {where_clause}
                GROUP BY category
                ORDER BY COUNT(*) DESC
            """, params)
            events_by_category = dict(cursor.fetchall())
            
            # User role distribution
            cursor.execute("SELECT role, COUNT(*) FROM users GROUP BY role")
            user_roles = dict(cursor.fetchall())
            
            # Recent events trend (last 30 days)
            cursor.execute(f"""
                SELECT date, COUNT(*) 
                FROM events 
                WHERE {host_where}
                GROUP BY date
                ORDER BY date
            """, host_params)
            events_trend = dict(cursor.fetchall())
            
            return {
                "total_events": total_events,
                "total_users": total_users,
                "active_hosts": active_hosts,
                "events_by_category": events_by_category,
                "user_roles": user_roles,
                "events_trend": events_trend,
                "filters_applied": {
                    "start_date": start_date,
                    "end_date": end_date,
                    "excluded_users": excluded_user_ids,
                    "category": category
                }
            }
    except Exception as e:
        print(f"Analytics metrics error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to fetch analytics: {str(e)}")

@app.get("/admin/analytics/time-series")
async def get_time_series_data(
    current_user: dict = Depends(get_current_user),
    metric: str = "events",  # events, users, active_hosts
    period: str = "daily",   # daily, weekly, monthly
    cumulative: Optional[bool] = None,  # Auto-default based on metric if None
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    exclude_users: Optional[str] = None,
    category: Optional[str] = None
):
    """Get time-series data for various metrics"""
    # Check admin permission
    if current_user.get('role') != 'admin':
        raise HTTPException(status_code=403, detail="Admin access required")
    
    try:
        with get_db() as conn:
            cursor = conn.cursor()
            
            # Set cumulative defaults based on metric if not specified
            if cumulative is None:
                if metric == "events":
                    cumulative = True  # Events should show cumulative growth by default
                elif metric == "users":
                    cumulative = True  # Users should show cumulative growth by default
                elif metric == "active_hosts":
                    cumulative = False  # Active hosts is more meaningful as period-based by default
                else:
                    cumulative = False
            
            # Parse filters
            excluded_user_ids = []
            if exclude_users:
                try:
                    excluded_user_ids = [int(uid.strip()) for uid in exclude_users.split(',') if uid.strip()]
                except ValueError:
                    excluded_user_ids = []
            
            # Default date range (last 90 days)
            if not start_date:
                start_date = (datetime.utcnow() - timedelta(days=90)).strftime('%Y-%m-%d')
            if not end_date:
                end_date = datetime.utcnow().strftime('%Y-%m-%d')
            
            # Build WHERE conditions for events queries
            placeholder = get_placeholder()
            conditions = [f"date >= {placeholder}", f"date <= {placeholder}"]
            params = [start_date, end_date]
            
            if excluded_user_ids:
                placeholders = ','.join([placeholder for _ in excluded_user_ids])
                conditions.append(f"created_by NOT IN ({placeholders})")
                params.extend(excluded_user_ids)
            
            if category:
                conditions.append(f"category = {placeholder}")
                params.append(category)
            
            where_clause = " AND ".join(conditions)
            
            # Date grouping based on period and database type
            if IS_PRODUCTION and DB_URL:
                # PostgreSQL date functions
                if period == "weekly":
                    date_group = "TO_CHAR(DATE_TRUNC('week', date::date), 'YYYY-\"W\"WW')"
                    date_label = "Week"
                elif period == "monthly":
                    date_group = "TO_CHAR(date::date, 'YYYY-MM')"
                    date_label = "Month"
                else:  # daily
                    date_group = "date"
                    date_label = "Date"
            else:
                # SQLite date functions
                if period == "weekly":
                    date_group = "strftime('%Y-W%W', date)"
                    date_label = "Week"
                elif period == "monthly":
                    date_group = "strftime('%Y-%m', date)"
                    date_label = "Month"
                else:  # daily
                    date_group = "date"
                    date_label = "Date"
            
            if metric == "events":
                cursor.execute(f"""
                    SELECT {date_group} as period, COUNT(*) as count
                    FROM events 
                    WHERE {where_clause}
                    GROUP BY {date_group}
                    ORDER BY period
                """, params)
                
            elif metric == "users":
                # User registrations by period
                if IS_PRODUCTION and DB_URL:
                    # PostgreSQL - replace date references with created_at::date
                    user_date_group = date_group.replace('date', 'created_at::date')
                    cursor.execute(f"""
                        SELECT {user_date_group} as period, COUNT(*) as count
                        FROM users 
                        WHERE created_at::date >= {placeholder}::date AND created_at::date <= {placeholder}::date
                        GROUP BY {user_date_group}
                        ORDER BY period
                    """, [start_date, end_date])
                else:
                    # SQLite
                    cursor.execute(f"""
                        SELECT {date_group.replace('date', 'DATE(created_at)')} as period, COUNT(*) as count
                        FROM users 
                        WHERE DATE(created_at) >= {placeholder} AND DATE(created_at) <= {placeholder}
                        GROUP BY {date_group.replace('date', 'DATE(created_at)')}
                        ORDER BY period
                    """, [start_date, end_date])
                
            elif metric == "active_hosts":
                # Active hosts by period (users who created events in that period)
                cursor.execute(f"""
                    SELECT {date_group} as period, COUNT(DISTINCT created_by) as count
                    FROM events 
                    WHERE {where_clause}
                    GROUP BY {date_group}
                    ORDER BY period
                """, params)
            
            data = cursor.fetchall()
            
            # Handle both SQLite tuple results and PostgreSQL dict-like results
            formatted_data = []
            for row in data:
                if isinstance(row, dict) or hasattr(row, 'get'):
                    # PostgreSQL RealDictCursor returns dict-like objects
                    period_val = row.get('period') 
                    count_val = row.get('count')
                    
                    # Fallback: if standard keys don't exist, use first two values
                    if period_val is None or count_val is None:
                        keys = list(row.keys()) if hasattr(row, 'keys') else []
                        if len(keys) >= 2:
                            period_val = row.get(keys[0]) if period_val is None else period_val
                            count_val = row.get(keys[1]) if count_val is None else count_val
                else:
                    # SQLite returns tuples
                    period_val = row[0] if len(row) > 0 else None
                    count_val = row[1] if len(row) > 1 else None
                
                # Only add valid data
                if period_val is not None and count_val is not None:
                    formatted_data.append({"period": period_val, "count": count_val})
            
            # Convert to cumulative if requested
            if cumulative:
                cumulative_count = 0
                for item in formatted_data:
                    cumulative_count += item["count"]
                    item["count"] = cumulative_count
            
            return {
                "metric": metric,
                "period": period,
                "cumulative": cumulative,
                "date_label": date_label,
                "data": formatted_data,
                "filters_applied": {
                    "start_date": start_date,
                    "end_date": end_date,
                    "excluded_users": excluded_user_ids,
                    "category": category
                }
            }
    except Exception as e:
        import traceback
        error_details = f"Time series data error: {str(e)}"
        print(error_details)
        print(f"Traceback: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"Failed to fetch time series data: {str(e)}")

@app.get("/admin/analytics/top-hosts")
async def get_top_hosts(
    current_user: dict = Depends(get_current_user),
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    limit: int = 10,
    exclude_users: Optional[str] = None
):
    """Get top event hosts/creators by event count"""
    # Check admin permission
    if current_user.get('role') != 'admin':
        raise HTTPException(status_code=403, detail="Admin access required")
    
    try:
        with get_db() as conn:
            cursor = conn.cursor()
            placeholder = get_placeholder()
            
            # Parse filters
            excluded_user_ids = []
            if exclude_users:
                try:
                    excluded_user_ids = [int(uid.strip()) for uid in exclude_users.split(',') if uid.strip()]
                except ValueError:
                    excluded_user_ids = []
            
            # Build WHERE conditions
            conditions = ["1=1"]
            params = []
            
            if start_date:
                conditions.append(f"e.date >= {placeholder}")
                params.append(start_date)
            if end_date:
                conditions.append(f"e.date <= {placeholder}")
                params.append(end_date)
            if excluded_user_ids:
                placeholders = ','.join([placeholder for _ in excluded_user_ids])
                conditions.append(f"e.created_by NOT IN ({placeholders})")
                params.extend(excluded_user_ids)
            
            where_clause = " AND ".join(conditions)
            params.append(limit)
            
            cursor.execute(f"""
                SELECT 
                    u.email,
                    u.id,
                    COUNT(e.id) as event_count,
                    MIN(e.date) as first_event_date,
                    MAX(e.date) as last_event_date
                FROM events e
                JOIN users u ON e.created_by = u.id
                WHERE {where_clause}
                GROUP BY u.id, u.email
                ORDER BY event_count DESC
                LIMIT {placeholder}
            """, params)
            
            hosts = cursor.fetchall()
            
            # Handle both SQLite tuple results and PostgreSQL dict-like results
            formatted_hosts = []
            for host in hosts:
                if isinstance(host, dict):
                    # PostgreSQL RealDictCursor returns dict-like objects
                    host_data = {
                        "email": host.get('email'),
                        "user_id": host.get('id'),
                        "event_count": host.get('event_count'),
                        "first_event_date": host.get('first_event_date'),
                        "last_event_date": host.get('last_event_date')
                    }
                else:
                    # SQLite returns tuples
                    host_data = {
                        "email": host[0],
                        "user_id": host[1],
                        "event_count": host[2],
                        "first_event_date": host[3],
                        "last_event_date": host[4]
                    }
                formatted_hosts.append(host_data)
            
            return {
                "hosts": formatted_hosts,
                "filters_applied": {
                    "start_date": start_date,
                    "end_date": end_date,
                    "excluded_users": excluded_user_ids,
                    "limit": limit
                }
            }
    except Exception as e:
        print(f"Top hosts error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to fetch top hosts: {str(e)}")

@app.get("/admin/analytics/geographic")
async def get_geographic_analytics(
    current_user: dict = Depends(get_current_user),
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    exclude_users: Optional[str] = None
):
    """Get geographic distribution of events"""
    # Check admin permission
    if current_user.get('role') != 'admin':
        raise HTTPException(status_code=403, detail="Admin access required")
    
    try:
        with get_db() as conn:
            cursor = conn.cursor()
            placeholder = get_placeholder()
            
            # Parse filters
            excluded_user_ids = []
            if exclude_users:
                try:
                    excluded_user_ids = [int(uid.strip()) for uid in exclude_users.split(',') if uid.strip()]
                except ValueError:
                    excluded_user_ids = []
            
            # Build WHERE conditions for state query
            state_conditions = ["state IS NOT NULL"]
            state_params = []
            
            if start_date:
                state_conditions.append(f"date >= {placeholder}")
                state_params.append(start_date)
            if end_date:
                state_conditions.append(f"date <= {placeholder}")
                state_params.append(end_date)
            if excluded_user_ids:
                state_placeholders = ','.join([placeholder for _ in excluded_user_ids])
                state_conditions.append(f"created_by NOT IN ({state_placeholders})")
                state_params.extend(excluded_user_ids)
            
            state_where = " AND ".join(state_conditions)
            
            # Build WHERE conditions for city query
            city_conditions = ["city IS NOT NULL"]
            city_params = []
            
            if start_date:
                city_conditions.append(f"date >= {placeholder}")
                city_params.append(start_date)
            if end_date:
                city_conditions.append(f"date <= {placeholder}")
                city_params.append(end_date)
            if excluded_user_ids:
                city_placeholders = ','.join([placeholder for _ in excluded_user_ids])
                city_conditions.append(f"created_by NOT IN ({city_placeholders})")
                city_params.extend(excluded_user_ids)
            
            city_where = " AND ".join(city_conditions)
            
            # Events by state
            cursor.execute(f"""
                SELECT state, COUNT(*) as count
                FROM events 
                WHERE {state_where}
                GROUP BY state
                ORDER BY count DESC
            """, state_params)
            state_results = cursor.fetchall()
            
            # Handle both SQLite tuple results and PostgreSQL dict-like results
            by_state = {}
            for row in state_results:
                if isinstance(row, dict) or hasattr(row, 'get'):
                    # PostgreSQL RealDictCursor returns dict-like objects
                    state_name = row.get('state')
                    count_val = row.get('count')
                else:
                    # SQLite returns tuples
                    state_name = row[0] if len(row) > 0 else None
                    count_val = row[1] if len(row) > 1 else None
                
                if state_name is not None and count_val is not None:
                    by_state[state_name] = count_val
            
            # Events by city
            cursor.execute(f"""
                SELECT city, state, COUNT(*) as count
                FROM events 
                WHERE {city_where}
                GROUP BY city, state
                ORDER BY count DESC
                LIMIT 20
            """, city_params)
            city_results = cursor.fetchall()
            
            # Handle both SQLite tuple results and PostgreSQL dict-like results
            by_city = []
            for row in city_results:
                if isinstance(row, dict):
                    # PostgreSQL RealDictCursor returns dict-like objects
                    city_data = {
                        "city": row.get('city'),
                        "state": row.get('state'),
                        "count": row.get('count')
                    }
                else:
                    # SQLite returns tuples
                    city_data = {
                        "city": row[0],
                        "state": row[1],
                        "count": row[2]
                    }
                by_city.append(city_data)
            
            return {
                "by_state": by_state,
                "by_city": by_city,
                "filters_applied": {
                    "start_date": start_date,
                    "end_date": end_date,
                    "excluded_users": excluded_user_ids
                }
            }
    except Exception as e:
        print(f"Geographic analytics error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to fetch geographic analytics: {str(e)}")

@app.get("/debug/analytics-sql")
async def debug_analytics_sql():
    """Debug the analytics SQL query construction"""
    try:
        with get_db() as conn:
            cursor = conn.cursor()
            placeholder = get_placeholder()
            
            # Test basic count first
            cursor.execute("SELECT COUNT(*) FROM events")
            total_events = get_count_from_result(cursor.fetchone())
            
            # Test with date filter
            start_date = "2025-05-09"
            end_date = "2025-06-08"
            
            # Build WHERE conditions exactly like the main analytics function
            conditions = ["1=1"]
            params = []
            
            if start_date:
                conditions.append(f"date >= {placeholder}")
                params.append(start_date)
            if end_date:
                conditions.append(f"date <= {placeholder}")
                params.append(end_date)
            
            where_clause = " AND ".join(conditions)
            
            # Build the final query
            query = f"SELECT COUNT(*) FROM events WHERE {where_clause}"
            
            # Test the query
            cursor.execute(query, params)
            result = cursor.fetchone()
            filtered_count = get_count_from_result(result)
            
            return {
                "status": "success",
                "placeholder_used": placeholder,
                "database_type": "PostgreSQL" if placeholder == "%s" else "SQLite",
                "total_events": total_events,
                "conditions": conditions,
                "where_clause": where_clause,
                "params": params,
                "final_query": query,
                "filtered_count": filtered_count
            }
    except Exception as e:
        import traceback
        return {
            "status": "error",
            "error": str(e),
            "traceback": traceback.format_exc(),
            "placeholder_used": get_placeholder()
        }

@app.get("/admin/analytics/category-cloud")
async def get_category_cloud_data(
    current_user: dict = Depends(get_current_user),
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    exclude_users: Optional[str] = None
):
    """Get category data for word cloud/bubble visualization"""
    # Check admin permission
    if current_user.get('role') != 'admin':
        raise HTTPException(status_code=403, detail="Admin access required")
    
    try:
        with get_db() as conn:
            cursor = conn.cursor()
            placeholder = get_placeholder()
            
            # Parse filters
            excluded_user_ids = []
            if exclude_users:
                try:
                    excluded_user_ids = [int(uid.strip()) for uid in exclude_users.split(',') if uid.strip()]
                except ValueError:
                    excluded_user_ids = []
            
            # Build WHERE conditions
            conditions = ["1=1"]
            params = []
            
            if start_date:
                conditions.append(f"date >= {placeholder}")
                params.append(start_date)
            if end_date:
                conditions.append(f"date <= {placeholder}")
                params.append(end_date)
            if excluded_user_ids:
                placeholders = ','.join([placeholder for _ in excluded_user_ids])
                conditions.append(f"created_by NOT IN ({placeholders})")
                params.extend(excluded_user_ids)
            
            where_clause = " AND ".join(conditions)
            
            # Get primary categories with counts and additional metrics
            cursor.execute(f"""
                SELECT 
                    category,
                    COUNT(*) as count,
                    COUNT(DISTINCT created_by) as unique_hosts,
                    AVG(COALESCE(interest_count, 0)) as avg_interest,
                    AVG(COALESCE(view_count, 0)) as avg_views
                FROM events 
                WHERE {where_clause}
                GROUP BY category
                ORDER BY count DESC
            """, params)
            
            primary_results = cursor.fetchall()
            
            # Get secondary categories (if they exist)
            cursor.execute(f"""
                SELECT 
                    secondary_category,
                    COUNT(*) as count
                FROM events 
                WHERE {where_clause} AND secondary_category IS NOT NULL AND secondary_category != ''
                GROUP BY secondary_category
                ORDER BY count DESC
            """, params)
            
            secondary_results = cursor.fetchall()
            
            # Process primary categories
            categories = []
            for row in primary_results:
                if isinstance(row, dict) or hasattr(row, 'get'):
                    category_data = {
                        "name": row.get('category', 'Unknown'),
                        "count": row.get('count', 0),
                        "unique_hosts": row.get('unique_hosts', 0),
                        "avg_interest": float(row.get('avg_interest', 0) or 0),
                        "avg_views": float(row.get('avg_views', 0) or 0),
                        "type": "primary"
                    }
                else:
                    category_data = {
                        "name": row[0] if row[0] else 'Unknown',
                        "count": row[1],
                        "unique_hosts": row[2],
                        "avg_interest": float(row[3] or 0),
                        "avg_views": float(row[4] or 0),
                        "type": "primary"
                    }
                categories.append(category_data)
            
            # Process secondary categories
            for row in secondary_results:
                if isinstance(row, dict) or hasattr(row, 'get'):
                    category_data = {
                        "name": row.get('secondary_category', 'Unknown'),
                        "count": row.get('count', 0),
                        "unique_hosts": 0,  # Not calculated for secondary
                        "avg_interest": 0,   # Not calculated for secondary
                        "avg_views": 0,      # Not calculated for secondary
                        "type": "secondary"
                    }
                else:
                    category_data = {
                        "name": row[0] if row[0] else 'Unknown',
                        "count": row[1],
                        "unique_hosts": 0,
                        "avg_interest": 0,
                        "avg_views": 0,
                        "type": "secondary"
                    }
                categories.append(category_data)
            
            return {
                "categories": categories,
                "total_categories": len(categories),
                "filters_applied": {
                    "start_date": start_date,
                    "end_date": end_date,
                    "excluded_users": excluded_user_ids
                }
            }
    except Exception as e:
        print(f"Category cloud error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to fetch category cloud data: {str(e)}")

@app.get("/admin/analytics/event-locations")
async def get_event_locations(
    current_user: dict = Depends(get_current_user),
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    exclude_users: Optional[str] = None,
    category: Optional[str] = None,
    limit: Optional[int] = 1000
):
    """Get event locations with coordinates for density mapping"""
    # Check admin permission
    if current_user.get('role') != 'admin':
        raise HTTPException(status_code=403, detail="Admin access required")
    
    try:
        with get_db() as conn:
            cursor = conn.cursor()
            placeholder = get_placeholder()
            
            # Parse filters
            excluded_user_ids = []
            if exclude_users:
                try:
                    excluded_user_ids = [int(uid.strip()) for uid in exclude_users.split(',') if uid.strip()]
                except ValueError:
                    excluded_user_ids = []
            
            # Build WHERE conditions
            conditions = ["lat IS NOT NULL", "lng IS NOT NULL"]
            params = []
            
            if start_date:
                conditions.append(f"date >= {placeholder}")
                params.append(start_date)
            if end_date:
                conditions.append(f"date <= {placeholder}")
                params.append(end_date)
            if category:
                conditions.append(f"category = {placeholder}")
                params.append(category)
            if excluded_user_ids:
                placeholders = ','.join([placeholder for _ in excluded_user_ids])
                conditions.append(f"created_by NOT IN ({placeholders})")
                params.extend(excluded_user_ids)
            
            where_clause = " AND ".join(conditions)
            
            # Get event locations with metadata
            cursor.execute(f"""
                SELECT 
                    lat, lng, category, city, state,
                    COALESCE(interest_count, 0) as interest_count,
                    COALESCE(view_count, 0) as view_count,
                    title
                FROM events 
                WHERE {where_clause}
                ORDER BY COALESCE(interest_count, 0) + COALESCE(view_count, 0) DESC
                LIMIT {placeholder}
            """, params + [limit])
            
            location_results = cursor.fetchall()
            
            # Process locations
            locations = []
            for row in location_results:
                if isinstance(row, dict) or hasattr(row, 'get'):
                    location_data = {
                        "lat": float(row.get('lat', 0)),
                        "lng": float(row.get('lng', 0)),
                        "category": row.get('category', 'Unknown'),
                        "city": row.get('city', ''),
                        "state": row.get('state', ''),
                        "interest_count": row.get('interest_count', 0),
                        "view_count": row.get('view_count', 0),
                        "title": row.get('title', '')[:50] + ("..." if len(row.get('title', '')) > 50 else "")
                    }
                else:
                    location_data = {
                        "lat": float(row[0] or 0),
                        "lng": float(row[1] or 0),
                        "category": row[2] or 'Unknown',
                        "city": row[3] or '',
                        "state": row[4] or '',
                        "interest_count": row[5] or 0,
                        "view_count": row[6] or 0,
                        "title": (row[7] or '')[:50] + ("..." if len(row[7] or '') > 50 else "")
                    }
                locations.append(location_data)
            
            # Calculate some summary stats
            total_locations = len(locations)
            avg_lat = sum(loc['lat'] for loc in locations) / total_locations if total_locations > 0 else 0
            avg_lng = sum(loc['lng'] for loc in locations) / total_locations if total_locations > 0 else 0
            
            return {
                "locations": locations,
                "total_locations": total_locations,
                "center": {"lat": avg_lat, "lng": avg_lng},
                "categories": list(set(loc['category'] for loc in locations)),
                "filters_applied": {
                    "start_date": start_date,
                    "end_date": end_date,
                    "category": category,
                    "excluded_users": excluded_user_ids,
                    "limit": limit
                }
            }
    except Exception as e:
        print(f"Event locations error: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to fetch event locations: {str(e)}")

# Page Visit Tracking - Privacy-friendly without personal data storage
async def track_page_visit(page_type: str, page_path: str, user_id: int = None, browser_fingerprint: str = None):
    """
    Track page visits in a privacy-friendly way
    Only stores aggregated data and basic page info
    """
    try:
        print(f"DEBUG: Tracking page visit - type: {page_type}, path: {page_path}, user: {user_id}, fingerprint: {browser_fingerprint}")
        
        with get_db() as conn:
            cursor = conn.cursor()
            placeholder = get_placeholder()
            
            # Temporarily disable deduplication to test tracking
            # TODO: Re-enable this later
            
            # Insert new page visit (always for testing)
            insert_query = f"""
                INSERT INTO page_visits (page_type, page_path, user_id, browser_fingerprint, visited_at)
                VALUES ({placeholder}, {placeholder}, {placeholder}, {placeholder}, CURRENT_TIMESTAMP)
            """
            params = (page_type, page_path, user_id, browser_fingerprint or 'anonymous')
            
            print(f"DEBUG: Executing insert query: {insert_query}")
            print(f"DEBUG: With params: {params}")
            
            cursor.execute(insert_query, params)
            
            print(f"DEBUG: Insert successful, committing...")
            conn.commit()
            print(f"DEBUG: Commit successful")
            
            return True
            
    except Exception as e:
        print(f"ERROR: Error tracking page visit: {e}")
        print(f"ERROR: Exception type: {type(e)}")
        import traceback
        print(f"ERROR: Traceback: {traceback.format_exc()}")
        return False

@app.post("/api/track-visit")
async def track_page_visit_endpoint(
    request: Request,
    page_type: str,  # 'homepage', 'events_list', 'event_detail', 'admin', etc.
    page_path: str = "/",
    current_user: dict = Depends(get_current_user_optional_no_exception)
):
    """
    Privacy-friendly page visit tracking endpoint
    Only tracks basic page types and paths, no personal data
    """
    try:
        # Get user ID if authenticated (for better analytics, but not required)
        user_id = current_user.get('id') if current_user else None
        
        # Generate browser fingerprint for deduplication only
        browser_fingerprint = generate_browser_fingerprint(request)
        
        # Track the page visit
        visit_tracked = await track_page_visit(page_type, page_path, user_id, browser_fingerprint)
        
        return {
            "success": True,
            "visit_tracked": visit_tracked,
            "page_type": page_type
        }
        
    except Exception as e:
        logger.error(f"Error in page visit tracking endpoint: {e}")
        return {"success": False, "error": "Failed to track visit"}

@app.post("/admin/verify-premium-events")
async def verify_premium_events(current_user: dict = Depends(get_current_user)):
    """
    Verify all existing events created by premium users
    """
    if current_user['role'] != 'admin':
        raise HTTPException(status_code=403, detail="Admin access required")
    
    placeholder = get_placeholder()
    try:
        with get_db() as conn:
            cursor = conn.cursor()
            
            # Find all events created by premium users that aren't verified
            cursor.execute(f"""
                UPDATE events 
                SET verified = TRUE 
                WHERE created_by IN (
                    SELECT id FROM users WHERE role IN ('premium', 'admin')
                ) AND verified = FALSE
            """)
            
            affected_rows = cursor.rowcount
            
            return {
                "success": True,
                "verified_events": affected_rows,
                "message": f"Verified {affected_rows} events from premium users"
            }
            
    except Exception as e:
        logger.error(f"Error verifying premium events: {str(e)}")
        raise HTTPException(status_code=500, detail="Error verifying premium events")

@app.get("/admin/analytics/page-visits")
async def get_page_visits_analytics(
    current_user: dict = Depends(get_current_user),
    metric: str = "page_visits",  # page_visits
    period: str = "daily",   # daily, weekly, monthly
    cumulative: Optional[bool] = None,  # Auto-default to True for page visits
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    exclude_users: Optional[str] = None,
    page_type: Optional[str] = None  # Filter by page type
):
    """Get page visit analytics with time series data"""
    # Check admin permission
    if current_user.get('role') != 'admin':
        raise HTTPException(status_code=403, detail="Admin access required")
    
    try:
        with get_db() as conn:
            cursor = conn.cursor()
            placeholder = get_placeholder()
            
            # Set cumulative default for page visits (usually we want to see growth)
            if cumulative is None:
                cumulative = True
            
            # Parse filters
            excluded_user_ids = []
            if exclude_users:
                try:
                    excluded_user_ids = [int(uid.strip()) for uid in exclude_users.split(',') if uid.strip()]
                except ValueError:
                    excluded_user_ids = []
            
            # Default date range (last 90 days)
            if not start_date:
                start_date = (datetime.utcnow() - timedelta(days=90)).strftime('%Y-%m-%d')
            if not end_date:
                end_date = datetime.utcnow().strftime('%Y-%m-%d')
            
            # Build WHERE clause
            where_conditions = [f"DATE(visited_at) >= {placeholder}", f"DATE(visited_at) <= {placeholder}"]
            params = [start_date, end_date]
            
            if excluded_user_ids:
                user_placeholders = ','.join([placeholder] * len(excluded_user_ids))
                where_conditions.append(f"(user_id IS NULL OR user_id NOT IN ({user_placeholders}))")
                params.extend(excluded_user_ids)
            
            if page_type:
                where_conditions.append(f"page_type = {placeholder}")
                params.append(page_type)
            
            where_clause = " AND ".join(where_conditions)
            
            # Generate period-specific date grouping
            if IS_PRODUCTION and DB_URL:
                # PostgreSQL
                if period == "daily":
                    date_group = "TO_CHAR(visited_at, 'YYYY-MM-DD')"
                    date_label = "Daily"
                elif period == "weekly":
                    date_group = "TO_CHAR(DATE_TRUNC('week', visited_at), 'YYYY-\"W\"WW')"
                    date_label = "Weekly"
                elif period == "monthly":
                    date_group = "TO_CHAR(DATE_TRUNC('month', visited_at), 'YYYY-MM')"
                    date_label = "Monthly"
                else:
                    date_group = "TO_CHAR(visited_at, 'YYYY-MM-DD')"
                    date_label = "Daily"
            else:
                # SQLite
                if period == "daily":
                    date_group = "strftime('%Y-%m-%d', visited_at)"
                    date_label = "Daily"
                elif period == "weekly":
                    date_group = "strftime('%Y-W%W', visited_at)"
                    date_label = "Weekly"
                elif period == "monthly":
                    date_group = "strftime('%Y-%m', visited_at)"
                    date_label = "Monthly"
                else:
                    date_group = "strftime('%Y-%m-%d', visited_at)"
                    date_label = "Daily"
            
            # Query for time series data
            query = f"""
                SELECT {date_group} as period, COUNT(*) as count
                FROM page_visits
                WHERE {where_clause}
                GROUP BY {date_group}
                ORDER BY period
            """
            
            cursor.execute(query, params)
            data = cursor.fetchall()
            
            # Handle both SQLite tuple results and PostgreSQL dict-like results
            formatted_data = []
            for row in data:
                if isinstance(row, dict) or hasattr(row, 'get'):
                    # PostgreSQL RealDictCursor returns dict-like objects
                    period_val = row.get('period') 
                    count_val = row.get('count')
                    
                    # Fallback: if standard keys don't exist, use first two values
                    if period_val is None or count_val is None:
                        keys = list(row.keys()) if hasattr(row, 'keys') else []
                        if len(keys) >= 2:
                            period_val = row.get(keys[0]) if period_val is None else period_val
                            count_val = row.get(keys[1]) if count_val is None else count_val
                else:
                    # SQLite returns tuples
                    period_val = row[0]
                    count_val = row[1]
                
                formatted_data.append({
                    "period": period_val,
                    "count": int(count_val) if count_val is not None else 0
                })
            
            # Apply cumulative calculation if requested
            if cumulative and formatted_data:
                running_total = 0
                for item in formatted_data:
                    running_total += item["count"]
                    item["count"] = running_total
            
            return {
                "metric": metric,
                "period": period,
                "cumulative": cumulative,
                "date_label": date_label,
                "data": formatted_data,
                "filters_applied": {
                    "start_date": start_date,
                    "end_date": end_date,
                    "excluded_users": excluded_user_ids,
                    "page_type": page_type
                }
            }
            
    except Exception as e:
        logger.error(f"Error fetching page visits analytics: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to fetch page visits analytics: {str(e)}")


@app.get("/api/report-event")
async def report_event_test():
    """Test if the report endpoint is reachable"""
    logger.info("üß™ GET /api/report-event endpoint reached")
    return {"message": "Report endpoint is working", "method": "GET"}
@app.post("/api/report-event")
async def report_event(report_data: dict):
    """Report an event for inappropriate content, incorrect information, etc."""
    try:
        logger.info(f"üö® Report event endpoint called with data: {report_data}")
        # Validate required fields
        required_fields = ['eventId', 'eventTitle', 'category', 'description', 'reporterEmail']
        for field in required_fields:
            if not report_data.get(field):
                raise HTTPException(status_code=400, detail=f"Missing required field: {field}")
        
        # Validate email format
        email = report_data.get('reporterEmail', '').strip()
        if '@' not in email or '.' not in email:
            raise HTTPException(status_code=400, detail="Invalid email address")
        
        # Validate description length
        description = report_data.get('description', '').strip()
        if len(description) < 10:
            raise HTTPException(status_code=400, detail="Description must be at least 10 characters")
        
        # Send email notification
        try:
            logger.info("üìß Attempting to send report email...")
            from email_config import EmailService
            email_service = EmailService()
            
            report_reason = report_data.get('reason', report_data.get('category', 'Unknown'))
            subject = f"Event Report: {report_reason} - Event ID {report_data['eventId']}"
            
            html_content = f"""
            <h2>Event Report Received</h2>
            <p><strong>Event ID:</strong> {report_data['eventId']}</p>
            <p><strong>Event Title:</strong> {report_data['eventTitle']}</p>
            <p><strong>Report Type:</strong> {report_reason}</p>
            <p><strong>Reporter:</strong> {report_data.get('reporterName', 'Anonymous')} ({email})</p>
            <p><strong>Description:</strong> {description}</p>
            <p><strong>Time:</strong> {report_data.get('reportedAt', 'N/A')}</p>
            """
            
            email_sent = email_service.send_email(
                to_email="support@todo-events.com",
                subject=subject,
                html_content=html_content
            )
            
            logger.info(f"üìß Email send result: {email_sent}")
            
        except Exception as e:
            logger.error(f"Error sending report email: {e}")
        
        # Log the report
        logger.info(f"Event report received - Event ID: {report_data['eventId']}, Type: {report_reason}, Reporter: {email}")
        
        logger.info("‚úÖ Report processing completed successfully")
        
        return {
            "success": True,
            "message": "Report submitted successfully. Our team will review it shortly."
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error processing event report: {e}")
        raise HTTPException(status_code=500, detail="Failed to submit report. Please contact support@todo-events.com directly.")

# CCPA/Privacy Compliance Endpoints

class PrivacyRequestType(str, Enum):
    ACCESS = "access"
    DELETE = "delete"
    OPT_OUT = "opt_out"

class PrivacyRequest(BaseModel):
    request_type: PrivacyRequestType
    email: EmailStr
    full_name: Optional[str] = None
    verification_info: Optional[str] = None
    details: Optional[str] = None

@app.post("/api/privacy/request")
async def submit_privacy_request(request: PrivacyRequest):
    """
    Handle CCPA privacy requests (access, deletion, opt-out)
    """
    try:
        logger.debug(f"=== PRIVACY REQUEST SUBMISSION DEBUG ===")
        logger.debug(f"Request data: request_type={request.request_type}, email={request.email}")
        logger.debug(f"IS_PRODUCTION: {IS_PRODUCTION}, DB_URL exists: {bool(DB_URL)}")
        
        with get_db() as conn:
            cursor = conn.cursor()
            
            # Log the privacy request with proper database compatibility
            placeholder = get_placeholder()
            created_at = datetime.utcnow().isoformat()
            
            logger.debug(f"Using placeholder: {placeholder}, created_at: {created_at}")
            
            # Use different approach for PostgreSQL vs SQLite
            if IS_PRODUCTION and DB_URL:  # PostgreSQL
                logger.debug(f"Using PostgreSQL, inserting privacy request for {request.email}")
                
                # Test if table exists first, create if it doesn't
                try:
                    cursor.execute("SELECT COUNT(*) FROM privacy_requests LIMIT 1")
                    logger.debug("privacy_requests table exists and is accessible")
                except Exception as table_error:
                    logger.warning(f"privacy_requests table doesn't exist, creating it: {table_error}")
                    
                    # Create the table
                    cursor.execute("""
                        CREATE TABLE IF NOT EXISTS privacy_requests (
                            id SERIAL PRIMARY KEY,
                            request_type VARCHAR(50) NOT NULL,
                            email VARCHAR(255) NOT NULL,
                            full_name VARCHAR(255),
                            verification_info TEXT,
                            details TEXT,
                            status VARCHAR(50) DEFAULT 'pending',
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            completed_at TIMESTAMP,
                            admin_notes TEXT
                        )
                    """)
                    conn.commit()
                    logger.info("privacy_requests table created successfully")
                
                insert_sql = f"""
                    INSERT INTO privacy_requests 
                    (request_type, email, full_name, verification_info, details, status, created_at)
                    VALUES ({placeholder}, {placeholder}, {placeholder}, {placeholder}, {placeholder}, {placeholder}, {placeholder})
                    RETURNING id
                """
                insert_values = (
                    request.request_type,
                    request.email,
                    request.full_name,
                    request.verification_info,
                    request.details,
                    'pending',
                    created_at
                )
                
                logger.debug(f"PostgreSQL insert SQL: {insert_sql}")
                logger.debug(f"PostgreSQL insert values: {insert_values}")
                
                cursor.execute(insert_sql, insert_values)
                result = cursor.fetchone()
                logger.debug(f"PostgreSQL insert result: {result}, type: {type(result)}")
                
                # Handle different result formats from PostgreSQL drivers
                if result:
                    if hasattr(result, 'keys') and hasattr(result, 'values'):
                        # psycopg2 RealDictRow or similar
                        request_id = result.get('id') or result.get(0)
                    elif isinstance(result, (list, tuple)) and len(result) > 0:
                        # Tuple or list result
                        request_id = result[0]
                    elif hasattr(result, '__getitem__'):
                        # Try direct indexing
                        try:
                            request_id = result[0]
                        except (KeyError, IndexError):
                            request_id = result.get('id') if hasattr(result, 'get') else None
                    else:
                        request_id = None
                    
                    logger.debug(f"Extracted request_id: {request_id}")
                    
                    if not request_id:
                        logger.error(f"Could not extract ID from result: {result}")
                        raise ValueError("Failed to get ID from PostgreSQL RETURNING clause")
                else:
                    logger.error(f"PostgreSQL RETURNING failed, result: {result}")
                    raise ValueError("Failed to get ID from PostgreSQL RETURNING clause")
            else:  # SQLite
                logger.debug(f"Using SQLite, inserting privacy request for {request.email}")
                cursor.execute(f"""
                    INSERT INTO privacy_requests 
                    (request_type, email, full_name, verification_info, details, status, created_at)
                    VALUES ({placeholder}, {placeholder}, {placeholder}, {placeholder}, {placeholder}, {placeholder}, {placeholder})
                """, (
                    request.request_type,
                    request.email,
                    request.full_name,
                    request.verification_info,
                    request.details,
                    'pending',
                    created_at
                ))
                request_id = cursor.lastrowid
                logger.debug(f"SQLite lastrowid: {request_id}")
            
            if not request_id or request_id == 0:
                logger.error(f"Invalid request_id after insert: {request_id}")
                raise ValueError(f"Failed to get valid ID for created privacy request, got: {request_id}")
                
            conn.commit()
            
            logger.info(f"Privacy request {request_id} created successfully for {request.email} - type: {request.request_type}")
            
            # Send confirmation email with user details
            try:
                from email_config import EmailService
                email_service = EmailService()
                
                # Prepare user details for the email
                user_details = {
                    'full_name': request.full_name,
                    'verification_info': request.verification_info,
                    'details': request.details,
                    'created_at': created_at
                }
                
                email_service.send_privacy_request_email(
                    request.email, 
                    request.request_type, 
                    request_id, 
                    user_details
                )
                logger.info(f"Privacy request confirmation email sent for request {request_id}")
            except Exception as e:
                logger.error(f"Failed to send privacy request confirmation email for request {request_id}: {e}")
                # Don't fail the entire request if email fails
            
            return {
                "success": True,
                "message": f"Privacy request submitted successfully. Request ID: {request_id}",
                "request_id": request_id,
                "estimated_response_time": "45 days"
            }
            
    except Exception as e:
        logger.error(f"Privacy request submission error: {e}")
        logger.error(f"Error type: {type(e).__name__}")
        logger.error(f"Error details: {str(e)}")
        import traceback
        logger.error(f"Traceback: {traceback.format_exc()}")
        # Return more detailed error for debugging
        error_detail = f"Failed to submit privacy request: {type(e).__name__}: {str(e)}"
        raise HTTPException(status_code=500, detail=error_detail)
async def get_user_data_export(email: str, verification_code: str = None):
    """
    Export all data associated with a user email (for CCPA access requests)
    This endpoint should be protected and only used by admins or with proper verification
    """
    try:
        with get_db() as conn:
            cursor = conn.cursor()
            
            user_data = {}
            
            # Get user account data
            placeholder = get_placeholder()
            cursor.execute(f"SELECT id, email, role, created_at FROM users WHERE email = {placeholder}", (email,))
            user_row = cursor.fetchone()
            
            if user_row:
                user_data["account"] = {
                    "id": user_row[0],
                    "email": user_row[1], 
                    "role": user_row[2],
                    "created_at": user_row[3]
                }
                user_id = user_row[0]
                
                # Get events created by user
                cursor.execute(f"""
                    SELECT id, title, description, date, start_time, end_time, address, 
                           category, created_at, updated_at
                    FROM events WHERE created_by = {placeholder}
                """, (user_id,))
                events = cursor.fetchall()
                
                user_data["events_created"] = [
                    {
                        "id": row[0], "title": row[1], "description": row[2],
                        "date": row[3], "start_time": row[4], "end_time": row[5],
                        "address": row[6], "category": row[7],
                        "created_at": row[8], "updated_at": row[9]
                    } for row in events
                ]
                
                # Get interest data
                cursor.execute(f"""
                    SELECT event_id, interested, created_at, updated_at
                    FROM event_interests WHERE user_id = {placeholder}
                """, (user_id,))
                interests = cursor.fetchall()
                
                user_data["event_interests"] = [
                    {
                        "event_id": row[0], "interested": bool(row[1]),
                        "created_at": row[2], "updated_at": row[3]
                    } for row in interests
                ]
                
                # Get page visit data
                cursor.execute(f"""
                    SELECT page_type, page_path, visited_at
                    FROM page_visits WHERE user_id = {placeholder}
                    ORDER BY visited_at DESC LIMIT 100
                """, (user_id,))
                visits = cursor.fetchall()
                
                user_data["page_visits"] = [
                    {
                        "page_type": row[0], "page_path": row[1], 
                        "visited_at": row[2]
                    } for row in visits
                ]
            
            # Also check for anonymous data by email (from reports, etc.)
            cursor.execute(f"""
                SELECT id, event_id, reason, details, created_at
                FROM event_reports WHERE reporter_email = {placeholder}
            """, (email,))
            reports = cursor.fetchall()
            
            if reports:
                user_data["reports_submitted"] = [
                    {
                        "id": row[0], "event_id": row[1], "reason": row[2],
                        "details": row[3], "created_at": row[4]
                    } for row in reports
                ]
            
            return {
                "email": email,
                "data_export": user_data,
                "export_date": datetime.utcnow().isoformat(),
                "retention_policy": "Data is retained as needed for service operation. Contact support@todo-events.com for specific retention periods."
            }
            
    except Exception as e:
        logger.error(f"Data export error for {email}: {e}")
        raise HTTPException(status_code=500, detail="Failed to export user data")

@app.delete("/api/privacy/delete/{email}")
async def delete_user_data(email: str, verification_code: str = None, current_user: dict = Depends(get_current_user)):
    """
    Delete all data associated with a user email (for CCPA deletion requests)
    Only accessible by admins
    """
    if current_user.get("role") != "admin":
        raise HTTPException(status_code=403, detail="Admin access required")
    
    try:
        with get_db_transaction() as conn:
            cursor = conn.cursor()
            placeholder = get_placeholder()
            
            # Get user ID first
            cursor.execute(f"SELECT id FROM users WHERE email = {placeholder}", (email,))
            user_row = cursor.fetchone()
            
            deleted_items = {}
            
            if user_row:
                user_id = user_row[0]
                
                # Delete user's events (and cascade related data)
                cursor.execute(f"DELETE FROM event_interests WHERE event_id IN (SELECT id FROM events WHERE created_by = {placeholder})", (user_id,))
                cursor.execute(f"DELETE FROM event_views WHERE event_id IN (SELECT id FROM events WHERE created_by = {placeholder})", (user_id,))
                cursor.execute(f"DELETE FROM events WHERE created_by = {placeholder}", (user_id,))
                deleted_items["events"] = cursor.rowcount
                
                # Delete user's interests
                cursor.execute(f"DELETE FROM event_interests WHERE user_id = {placeholder}", (user_id,))
                deleted_items["interests"] = cursor.rowcount
                
                # Delete user's page visits
                cursor.execute(f"DELETE FROM page_visits WHERE user_id = {placeholder}", (user_id,))
                deleted_items["page_visits"] = cursor.rowcount
                
                # Delete user account
                cursor.execute(f"DELETE FROM users WHERE id = {placeholder}", (user_id,))
                deleted_items["user_account"] = cursor.rowcount
            
            # Delete reports by email (even if no user account)
            cursor.execute(f"DELETE FROM event_reports WHERE reporter_email = {placeholder}", (email,))
            deleted_items["reports"] = cursor.rowcount
            
            # Update privacy request status
            cursor.execute(f"""
                UPDATE privacy_requests 
                SET status = 'completed', completed_at = {placeholder}
                WHERE email = {placeholder} AND request_type = 'delete'
            """, (datetime.utcnow().isoformat(), email))
            
            conn.commit()
            
            return {
                "success": True,
                "message": f"All data for {email} has been deleted",
                "deleted_items": deleted_items,
                "deletion_date": datetime.utcnow().isoformat()
            }
            
    except Exception as e:
        logger.error(f"Data deletion error for {email}: {e}")
        raise HTTPException(status_code=500, detail="Failed to delete user data")

# Add alias for privacy requests endpoint (frontend expects /admin/privacy-requests)
@app.get("/admin/privacy-requests")
async def list_privacy_requests_alias(current_user: dict = Depends(get_current_user)):
    """
    Alias for privacy requests endpoint (frontend compatibility)
    """
    return await list_privacy_requests(current_user)

@app.get("/admin/privacy/requests")
async def list_privacy_requests(current_user: dict = Depends(get_current_user)):
    """
    List all privacy requests for admin review
    """
    if current_user.get("role") != "admin":
        raise HTTPException(status_code=403, detail="Admin access required")
    
    try:
        with get_db() as conn:
            cursor = conn.cursor()
            
            # Simplified approach: directly query without complex column detection
            try:
                cursor.execute("""
                    SELECT id, request_type, email, 
                           COALESCE(full_name, '') as full_name, 
                           COALESCE(details, '') as details, 
                           status, 
                           created_at, 
                           completed_at
                    FROM privacy_requests 
                    ORDER BY created_at DESC
                """)
                
                requests = cursor.fetchall()
                
                result = []
                for row in requests:
                    # Handle both tuple and dict-like results
                    if isinstance(row, (tuple, list)):
                        result.append({
                            "id": row[0],
                            "request_type": row[1],
                            "email": row[2],
                            "full_name": row[3] if row[3] else "",
                            "details": row[4] if row[4] else "",
                            "status": row[5],
                            "created_at": row[6],
                            "completed_at": row[7] if row[7] else None
                        })
                    else:
                        # If it's a dict-like object
                        result.append({
                            "id": row.get('id'),
                            "request_type": row.get('request_type'),
                            "email": row.get('email'),
                            "full_name": row.get('full_name', ''),
                            "details": row.get('details', ''),
                            "status": row.get('status'),
                            "created_at": row.get('created_at'),
                            "completed_at": row.get('completed_at')
                        })
                
                return result
                
            except Exception as query_error:
                # If the main query fails, try alternative column names
                try:
                    cursor.execute("""
                        SELECT id, request_type, email, 
                               COALESCE(full_name, '') as full_name, 
                               COALESCE(verification_info, details, '') as details, 
                               status, 
                               created_at, 
                               COALESCE(admin_notes, completed_at) as completed_at
                        FROM privacy_requests 
                        ORDER BY created_at DESC
                    """)
                    
                    requests = cursor.fetchall()
                    
                    return [
                        {
                            "id": row[0] if isinstance(row, (tuple, list)) else row.get('id'),
                            "request_type": row[1] if isinstance(row, (tuple, list)) else row.get('request_type'),
                            "email": row[2] if isinstance(row, (tuple, list)) else row.get('email'),
                            "full_name": (row[3] if isinstance(row, (tuple, list)) else row.get('full_name', '')) or "",
                            "details": (row[4] if isinstance(row, (tuple, list)) else row.get('details', '')) or "",
                            "status": row[5] if isinstance(row, (tuple, list)) else row.get('status'),
                            "created_at": row[6] if isinstance(row, (tuple, list)) else row.get('created_at'),
                            "completed_at": (row[7] if isinstance(row, (tuple, list)) else row.get('completed_at')) or None
                        } for row in requests
                    ]
                    
                except Exception as alt_error:
                    logger.error(f"Both privacy request queries failed: {query_error}, {alt_error}")
                    return []
            
    except Exception as e:
        logger.error(f"Error listing privacy requests: {e}")
        # Return empty list instead of throwing error
        return []

@app.put("/admin/privacy/requests/{request_id}/status")
async def update_privacy_request_status(
    request_id: int, 
    status: str, 
    notes: Optional[str] = None,
    current_user: dict = Depends(get_current_user)
):
    """
    Update privacy request status
    """
    if current_user.get("role") != "admin":
        raise HTTPException(status_code=403, detail="Admin access required")
    
    try:
        with get_db() as conn:
            cursor = conn.cursor()
            
            completed_at = datetime.utcnow().isoformat() if status == "completed" else None
            
            cursor.execute("""
                UPDATE privacy_requests 
                SET status = ?, completed_at = ?, admin_notes = ?
                WHERE id = ?
            """, (status, completed_at, notes, request_id))
            
            conn.commit()
            
            return {"success": True, "message": "Privacy request status updated"}
            
    except Exception as e:
        logger.error(f"Error updating privacy request status: {e}")
        raise HTTPException(status_code=500, detail="Failed to update privacy request status")

@app.get("/debug/test-email")
async def debug_test_email():
    """Test email configuration"""
    try:
        from email_config import EmailService
        email_service = EmailService()
        
        # Test email send
        result = email_service.send_email(
            to_email="support@todo-events.com",
            subject="Test Email from Render - Report System",
            html_content="<h2>Test Email</h2><p>This is a test email to verify the report system email configuration is working.</p>"
        )
        
        return {
            "email_test_result": result,
            "config": {
                "smtp_server": email_service.config['smtp_server'],
                "smtp_port": email_service.config['smtp_port'],
                "smtp_username": email_service.config['smtp_username'],
                "from_email": email_service.config['from_email'],
                "password_configured": bool(email_service.config['smtp_password'])
            }
        }
    except Exception as e:
        return {"error": str(e), "traceback": str(e.__traceback__)}

@app.get("/debug/page-visits")
async def debug_page_visits():
    """Debug endpoint to check page visits table"""
    try:
        with get_db() as conn:
            cursor = conn.cursor()
            
            # Check if table exists and get structure
            if IS_PRODUCTION and DB_URL:
                cursor.execute("SELECT column_name, data_type FROM information_schema.columns WHERE table_name = 'page_visits' ORDER BY ordinal_position")
                columns = cursor.fetchall()
                
                cursor.execute("SELECT COUNT(*) FROM page_visits")
                count = get_count_from_result(cursor.fetchone())
                
                cursor.execute("SELECT * FROM page_visits ORDER BY visited_at DESC LIMIT 5")
                recent_visits = cursor.fetchall()
            else:
                cursor.execute("PRAGMA table_info(page_visits)")
                columns = cursor.fetchall()
                
                cursor.execute("SELECT COUNT(*) FROM page_visits")
                count = get_count_from_result(cursor.fetchone())
                
                cursor.execute("SELECT * FROM page_visits ORDER BY visited_at DESC LIMIT 5")
                recent_visits = cursor.fetchall()
            
            # Test direct insert
            try:
                placeholder = get_placeholder()
                cursor.execute(f"""
                    INSERT INTO page_visits (page_type, page_path, user_id, browser_fingerprint, visited_at)
                    VALUES ({placeholder}, {placeholder}, {placeholder}, {placeholder}, CURRENT_TIMESTAMP)
                """, ('debug_direct', '/debug-direct', None, 'debug-fingerprint'))
                conn.commit()
                direct_insert_success = True
                direct_insert_error = None
            except Exception as e:
                direct_insert_success = False
                direct_insert_error = str(e)
                
            return {
                "table_exists": len(columns) > 0,
                "columns": columns,
                "visit_count": count,
                "recent_visits": recent_visits,
                "direct_insert_success": direct_insert_success,
                "direct_insert_error": direct_insert_error,
                "is_production": IS_PRODUCTION,
                "db_url": DB_URL is not None
            }
            
    except Exception as e:
        return {"error": str(e), "traceback": str(e.__traceback__)}


@app.get("/debug/users-schema")
async def debug_users_schema():
    """Debug endpoint to check users table schema"""
    try:
        with get_db() as conn:
            c = conn.cursor()
            
            if IS_PRODUCTION and DB_URL:
                # PostgreSQL - get users table columns
                c.execute("""
                    SELECT column_name, data_type, is_nullable, column_default 
                    FROM information_schema.columns 
                    WHERE table_name = 'users' 
                    ORDER BY ordinal_position
                """)
                users_columns = c.fetchall()
                
                # Check specifically for premium columns
                c.execute("""
                    SELECT column_name 
                    FROM information_schema.columns 
                    WHERE table_name = 'users' 
                    AND column_name IN ('premium_expires_at', 'premium_granted_by', 'premium_invited')
                """)
                premium_columns = [row['column_name'] if hasattr(row, 'keys') else row[0] for row in c.fetchall()]
                
            else:
                # SQLite - get users table columns
                c.execute('PRAGMA table_info(users)')
                users_columns = c.fetchall()
                premium_columns = [col[1] if hasattr(col, '__getitem__') else str(col) for col in users_columns if (col[1] if hasattr(col, '__getitem__') else str(col)) in ['premium_expires_at', 'premium_granted_by', 'premium_invited']]
            
            return {
                "database_type": "postgresql" if IS_PRODUCTION and DB_URL else "sqlite",
                "users_table": {
                    "columns": users_columns,
                    "total_columns": len(users_columns)
                },
                "premium_columns": {
                    "found": premium_columns,
                    "missing": [col for col in ['premium_expires_at', 'premium_granted_by', 'premium_invited'] if col not in premium_columns],
                    "has_all_premium_columns": len(premium_columns) == 3
                }
            }
            
    except Exception as e:
        logger.error(f"Users schema debug error: {str(e)}")
        return {"error": str(e), "traceback": traceback.format_exc()}

@app.post("/debug/create-premium-columns")
async def create_premium_columns():
    """Debug endpoint to create premium columns in production"""
    try:
        logger.info("üöÄ Starting premium columns creation...")
        
        with get_db() as conn:
            cursor = conn.cursor()
            
            # Add premium and deletion columns to users table
            columns_to_add = [
                ("premium_expires_at", "TIMESTAMP"),
                ("premium_granted_by", "INTEGER"),
                ("premium_invited", "BOOLEAN DEFAULT FALSE"),
                ("deleted_at", "TIMESTAMP"),
                ("deletion_scheduled_at", "TIMESTAMP")
            ]
            
            results = []
            for column_name, column_type in columns_to_add:
                try:
                    if IS_PRODUCTION and DB_URL:
                        # PostgreSQL
                        cursor.execute(f"ALTER TABLE users ADD COLUMN IF NOT EXISTS {column_name} {column_type}")
                    else:
                        # SQLite
                        cursor.execute(f"ALTER TABLE users ADD COLUMN {column_name} {column_type}")
                    
                    results.append(f"‚úÖ Added {column_name}")
                    logger.info(f"‚úÖ Added column: {column_name}")
                    
                except Exception as e:
                    if "already exists" in str(e).lower() or "duplicate column" in str(e).lower():
                        results.append(f"‚ÑπÔ∏è {column_name} already exists")
                        logger.info(f"‚ÑπÔ∏è Column {column_name} already exists")
                    else:
                        results.append(f"‚ùå Failed to add {column_name}: {str(e)}")
                        logger.error(f"‚ùå Failed to add {column_name}: {str(e)}")
            
            conn.commit()
            
            # Verify columns were created
            if IS_PRODUCTION and DB_URL:
                cursor.execute("""
                    SELECT column_name 
                    FROM information_schema.columns 
                    WHERE table_name = 'users' 
                    AND column_name IN ('premium_expires_at', 'premium_granted_by', 'premium_invited', 'deleted_at', 'deletion_scheduled_at')
                """)
                created_columns = [row['column_name'] if hasattr(row, 'keys') else row[0] for row in cursor.fetchall()]
            else:
                cursor.execute('PRAGMA table_info(users)')
                columns_info = cursor.fetchall()
                created_columns = [col[1] for col in columns_info if col[1] in ['premium_expires_at', 'premium_granted_by', 'premium_invited', 'deleted_at', 'deletion_scheduled_at']]
            
            return {
                "success": True,
                "message": "Premium and deletion columns creation completed",
                "results": results,
                "created_columns": created_columns,
                "all_columns_present": len(created_columns) == 5
            }
            
    except Exception as e:
        logger.error(f"Error creating premium columns: {str(e)}")
        return {
            "success": False,
            "error": str(e),
            "message": "Failed to create premium columns"
        }

@app.post("/debug/add-deletion-columns")
async def add_deletion_columns():
    """Add deletion tracking columns to users table"""
    try:
        with get_db() as conn:
            cursor = conn.cursor()
            
            # Add deleted_at column if it doesn't exist
            try:
                if IS_PRODUCTION and DB_URL:
                    # PostgreSQL
                    cursor.execute("""
                        ALTER TABLE users 
                        ADD COLUMN IF NOT EXISTS deleted_at TIMESTAMP
                    """)
                else:
                    # SQLite
                    cursor.execute("""
                        ALTER TABLE users 
                        ADD COLUMN deleted_at TEXT
                    """)
                logger.info("‚úÖ Added deleted_at column to users table")
            except Exception as e:
                if "duplicate column" in str(e).lower() or "already exists" in str(e).lower():
                    logger.info("‚ÑπÔ∏è deleted_at column already exists")
                else:
                    logger.error(f"Error adding deleted_at column: {e}")
            
            # Add deletion_scheduled_at column if it doesn't exist
            try:
                if IS_PRODUCTION and DB_URL:
                    # PostgreSQL
                    cursor.execute("""
                        ALTER TABLE users 
                        ADD COLUMN IF NOT EXISTS deletion_scheduled_at TIMESTAMP
                    """)
                else:
                    # SQLite
                    cursor.execute("""
                        ALTER TABLE users 
                        ADD COLUMN deletion_scheduled_at TEXT
                    """)
                logger.info("‚úÖ Added deletion_scheduled_at column to users table")
            except Exception as e:
                if "duplicate column" in str(e).lower() or "already exists" in str(e).lower():
                    logger.info("‚ÑπÔ∏è deletion_scheduled_at column already exists")
                else:
                    logger.error(f"Error adding deletion_scheduled_at column: {e}")
            
            conn.commit()
            
            return {
                "success": True,
                "message": "Deletion columns added/verified successfully"
            }
            
    except Exception as e:
        logger.error(f"Error adding deletion columns: {str(e)}")
        return {"error": str(e)}

@app.get("/debug/privacy-requests-table")
async def debug_privacy_requests_table():
    """Debug endpoint to check privacy_requests table structure and test insert"""
    try:
        with get_db() as conn:
            cursor = conn.cursor()
            
            # Check if table exists and get structure
            if IS_PRODUCTION and DB_URL:
                cursor.execute("SELECT column_name, data_type, is_nullable FROM information_schema.columns WHERE table_name = 'privacy_requests' ORDER BY ordinal_position")
                columns = cursor.fetchall()
                
                # Check for sequence/auto-increment
                cursor.execute("SELECT column_default FROM information_schema.columns WHERE table_name = 'privacy_requests' AND column_name = 'id'")
                id_default = cursor.fetchone()
                
                cursor.execute("SELECT COUNT(*) FROM privacy_requests")
                count = get_count_from_result(cursor.fetchone())
                
                cursor.execute("SELECT id, request_type, email, status, created_at FROM privacy_requests ORDER BY created_at DESC LIMIT 5")
                recent_requests = cursor.fetchall()
                
                # Test the RETURNING clause
                placeholder = get_placeholder()
                cursor.execute(f"""
                    INSERT INTO privacy_requests 
                    (request_type, email, full_name, verification_info, details, status, created_at)
                    VALUES ({placeholder}, {placeholder}, {placeholder}, {placeholder}, {placeholder}, {placeholder}, {placeholder})
                    RETURNING id
                """, (
                    'access',
                    'debug@test.com',
                    'Debug Test',
                    'Test verification',
                    'Debug test request',
                    'pending',
                    datetime.utcnow().isoformat()
                ))
                test_result = cursor.fetchone()
                test_id = test_result[0] if test_result else None
                
                # Clean up test record
                if test_id:
                    cursor.execute(f"DELETE FROM privacy_requests WHERE id = {placeholder}", (test_id,))
                
                conn.commit()
                
            else:
                cursor.execute("PRAGMA table_info(privacy_requests)")
                columns = cursor.fetchall()
                
                cursor.execute("SELECT COUNT(*) FROM privacy_requests")
                count = cursor.fetchone()[0]
                
                cursor.execute("SELECT id, request_type, email, status, created_at FROM privacy_requests ORDER BY created_at DESC LIMIT 5")
                recent_requests = cursor.fetchall()
                
                # Test insert for SQLite
                placeholder = get_placeholder()
                cursor.execute(f"""
                    INSERT INTO privacy_requests 
                    (request_type, email, full_name, verification_info, details, status, created_at)
                    VALUES ({placeholder}, {placeholder}, {placeholder}, {placeholder}, {placeholder}, {placeholder}, {placeholder})
                """, (
                    'access',
                    'debug@test.com',
                    'Debug Test',
                    'Test verification',
                    'Debug test request',
                    'pending',
                    datetime.utcnow().isoformat()
                ))
                test_id = cursor.lastrowid
                
                # Clean up test record
                if test_id:
                    cursor.execute(f"DELETE FROM privacy_requests WHERE id = {placeholder}", (test_id,))
                
                conn.commit()
                id_default = "AUTO_INCREMENT"
            
            return {
                "table_exists": True,
                "columns": [str(col) for col in columns],
                "id_column_default": str(id_default) if 'id_default' in locals() else "Unknown",
                "total_requests": count,
                "recent_requests": [str(req) for req in recent_requests],
                "test_insert_id": test_id,
                "test_insert_success": test_id is not None and test_id > 0,
                "database_type": "PostgreSQL" if (IS_PRODUCTION and DB_URL) else "SQLite"
            }
            
    except Exception as e:
        return {
            "error": str(e),
            "table_exists": False,
            "database_type": "PostgreSQL" if (IS_PRODUCTION and DB_URL) else "SQLite"
        }

@app.post("/debug/create-privacy-table")
async def create_privacy_table():
    """Create the missing privacy_requests table"""
    try:
        with get_db() as conn:
            cursor = conn.cursor()
            
            if IS_PRODUCTION and DB_URL:  # PostgreSQL
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS privacy_requests (
                        id SERIAL PRIMARY KEY,
                        request_type VARCHAR(50) NOT NULL,
                        email VARCHAR(255) NOT NULL,
                        full_name VARCHAR(255),
                        verification_info TEXT,
                        details TEXT,
                        status VARCHAR(50) DEFAULT 'pending',
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        completed_at TIMESTAMP,
                        admin_notes TEXT
                    )
                """)
                conn.commit()
                
                # Verify table was created
                cursor.execute("SELECT column_name FROM information_schema.columns WHERE table_name = 'privacy_requests'")
                columns = cursor.fetchall()
                
                return {
                    "success": True,
                    "message": "privacy_requests table created successfully",
                    "columns": [col[0] for col in columns],
                    "database_type": "PostgreSQL"
                }
            else:  # SQLite
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS privacy_requests (
                        id INTEGER PRIMARY KEY AUTOINCREMENT,
                        request_type TEXT NOT NULL,
                        email TEXT NOT NULL,
                        full_name TEXT,
                        verification_info TEXT,
                        details TEXT,
                        status TEXT DEFAULT 'pending',
                        created_at TEXT DEFAULT CURRENT_TIMESTAMP,
                        completed_at TEXT,
                        admin_notes TEXT
                    )
                """)
                conn.commit()
                
                # Verify table was created
                cursor.execute("PRAGMA table_info(privacy_requests)")
                columns = cursor.fetchall()
                
                return {
                    "success": True,
                    "message": "privacy_requests table created successfully",
                    "columns": columns,
                    "database_type": "SQLite"
                }
                
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "database_type": "PostgreSQL" if (IS_PRODUCTION and DB_URL) else "SQLite"
        }
@app.post("/debug/test-privacy-insert")
async def debug_test_privacy_insert():
    """Debug endpoint to test privacy request insertion with detailed error info"""
    try:
        debug_info = {
            "IS_PRODUCTION": IS_PRODUCTION,
            "DB_URL_exists": bool(DB_URL),
            "timestamp": datetime.utcnow().isoformat()
        }
        
        test_data = {
            "request_type": "access",
            "email": "debug-test@example.com",
            "full_name": "Debug Test User",
            "verification_info": "Debug verification",
            "details": "Debug test request"
        }
        
        with get_db() as conn:
            cursor = conn.cursor()
            placeholder = get_placeholder()
            
            # Test 1: Basic table check
            try:
                cursor.execute("SELECT table_name FROM information_schema.tables WHERE table_name = 'privacy_requests'")
                table_check = cursor.fetchone()
                debug_info["table_exists_direct"] = table_check is not None
                debug_info["table_check_result"] = table_check
            except Exception as e:
                debug_info["table_check_error"] = str(e)
            
            # Test 2: Try to count records
            try:
                cursor.execute("SELECT COUNT(*) FROM privacy_requests")
                count_result = cursor.fetchone()
                debug_info["current_record_count"] = count_result[0] if count_result else "No result"
            except Exception as e:
                debug_info["count_error"] = str(e)
                debug_info["count_error_type"] = type(e).__name__
            
            # Test 3: Try the actual insert from the main endpoint
            try:
                created_at = datetime.utcnow().isoformat()
                
                insert_sql = f"""
                    INSERT INTO privacy_requests 
                    (request_type, email, full_name, verification_info, details, status, created_at)
                    VALUES ({placeholder}, {placeholder}, {placeholder}, {placeholder}, {placeholder}, {placeholder}, {placeholder})
                    RETURNING id
                """
                insert_values = (
                    test_data["request_type"],
                    test_data["email"],
                    test_data["full_name"],
                    test_data["verification_info"],
                    test_data["details"],
                    'pending',
                    created_at
                )
                
                debug_info["insert_sql"] = insert_sql
                debug_info["insert_values"] = insert_values
                
                cursor.execute(insert_sql, insert_values)
                result = cursor.fetchone()
                
                if result and len(result) > 0:
                    request_id = result[0]
                    debug_info["insert_success"] = True
                    debug_info["request_id"] = request_id
                    
                    conn.commit()
                    
                    # Test retrieval
                    cursor.execute(f"SELECT * FROM privacy_requests WHERE id = {placeholder}", (request_id,))
                    retrieved = cursor.fetchone()
                    debug_info["retrieval_test"] = dict(retrieved) if hasattr(retrieved, 'keys') else list(retrieved)
                    
                else:
                    debug_info["insert_success"] = False
                    debug_info["insert_result"] = result
                    
            except Exception as e:
                debug_info["insert_error"] = str(e)
                debug_info["insert_error_type"] = type(e).__name__
                import traceback
                debug_info["insert_traceback"] = traceback.format_exc()
            
            return {
                "success": True,
                "message": "Privacy request debug test completed",
                "debug_info": debug_info
            }
            
    except Exception as e:
        import traceback
        return {
            "success": False,
            "error": str(e),
            "error_type": type(e).__name__,
            "traceback": traceback.format_exc(),
            "debug_info": debug_info if 'debug_info' in locals() else {}
        }
async def create_missing_tables():
    """Create missing tables and columns in production"""
    try:
        with get_db() as conn:
            cursor = conn.cursor()
            results = []
            
            # Check if privacy_requests table exists
            if IS_PRODUCTION and DB_URL:
                cursor.execute("""
                    SELECT EXISTS (
                        SELECT FROM information_schema.tables 
                        WHERE table_name = 'privacy_requests'
                    )
                """)
                privacy_table_exists = cursor.fetchone()[0]
            else:
                cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='privacy_requests'")
                privacy_table_exists = cursor.fetchone() is not None
            
            # Create privacy_requests table if it doesn't exist
            if not privacy_table_exists:
                if IS_PRODUCTION and DB_URL:
                    cursor.execute("""
                        CREATE TABLE privacy_requests (
                            id SERIAL PRIMARY KEY,
                            request_type VARCHAR(50) NOT NULL,
                            email VARCHAR(255) NOT NULL,
                            full_name VARCHAR(255),
                            details TEXT,
                            status VARCHAR(20) DEFAULT 'pending',
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            completed_at TIMESTAMP NULL,
                            admin_notes TEXT
                        )
                    """)
                results.append("‚úÖ Created privacy_requests table")
            else:
                results.append("‚ÑπÔ∏è privacy_requests table already exists")
            
            # Add premium columns to users table
            premium_columns = [
                ("premium_expires_at", "TIMESTAMP"),
                ("premium_granted_by", "INTEGER"),
                ("premium_invited", "BOOLEAN DEFAULT FALSE")
            ]
            
            for column_name, column_type in premium_columns:
                try:
                    if IS_PRODUCTION and DB_URL:
                        cursor.execute(f"ALTER TABLE users ADD COLUMN IF NOT EXISTS {column_name} {column_type}")
                    else:
                        cursor.execute(f"ALTER TABLE users ADD COLUMN {column_name} {column_type}")
                    results.append(f"‚úÖ Added {column_name} to users table")
                except Exception as e:
                    if "already exists" in str(e).lower() or "duplicate" in str(e).lower():
                        results.append(f"‚ÑπÔ∏è {column_name} already exists in users table")
                    else:
                        results.append(f"‚ùå Failed to add {column_name}: {str(e)}")
            
            conn.commit()
            
            return {
                "success": True,
                "message": "Database creation completed",
                "results": results,
                "privacy_table_created": not privacy_table_exists
            }
            
    except Exception as e:
        logger.error(f"Error creating missing tables: {str(e)}")
        return {
            "success": False,
            "error": str(e),
            "message": "Failed to create missing tables"
        } @app.post("/debug/verify-premium-events-simple")
async def verify_premium_events_simple():
    """Simple debug endpoint to verify premium events without authentication"""
    try:
        with get_db() as conn:
            cursor = conn.cursor()
            
            # Find all events created by premium users that aren't verified
            cursor.execute("""
                UPDATE events 
                SET verified = TRUE 
                WHERE created_by IN (
                    SELECT id FROM users WHERE role IN ('premium', 'admin')
                ) AND (verified = FALSE OR verified IS NULL)
            """)
            
            affected_rows = cursor.rowcount
            conn.commit()
            
            return {
                "success": True,
                "verified_events": affected_rows,
                "message": f"Verified {affected_rows} events from premium users"
            }
            
    except Exception as e:
                    return {
                "success": False,
                "error": str(e),
                "message": "Error verifying premium events"
            }

@app.get("/users/me", response_model=UserResponse)
async def read_users_me(current_user: dict = Depends(get_current_user)):
    """Get current user information"""
    return current_user

@app.post("/debug/fix-privacy-table")
async def fix_privacy_table():
    """Fix privacy requests table creation with better error handling"""
    try:
        with get_db() as conn:
            cursor = conn.cursor()
            results = []
            
            # Create privacy_requests table with IF NOT EXISTS
            try:
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS privacy_requests (
                        id SERIAL PRIMARY KEY,
                        request_type VARCHAR(50) NOT NULL,
                        email VARCHAR(255) NOT NULL,
                        full_name VARCHAR(255),
                        details TEXT,
                        status VARCHAR(20) DEFAULT 'pending',
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        completed_at TIMESTAMP NULL,
                        admin_notes TEXT
                    )
                """)
                results.append("‚úÖ Privacy requests table created/verified")
                
                # Test if we can insert a sample record
                cursor.execute("""
                    INSERT INTO privacy_requests (request_type, email, full_name, details, status)
                    VALUES ('access', 'test@example.com', 'Test User', 'Test privacy request', 'pending')
                    ON CONFLICT DO NOTHING
                """)
                results.append("‚úÖ Sample record inserted successfully")
                
                # Count existing records
                cursor.execute("SELECT COUNT(*) FROM privacy_requests")
                count = cursor.fetchone()
                record_count = count[0] if isinstance(count, (tuple, list)) else count
                results.append(f"‚ÑπÔ∏è Current record count: {record_count}")
                
                conn.commit()
                
                return {
                    "success": True,
                    "message": "Privacy requests table fixed successfully",
                    "results": results
                }
                
            except Exception as e:
                conn.rollback()
                return {
                    "success": False,
                    "error": str(e),
                    "message": "Error creating privacy requests table",
                    "results": results
                }
            
    except Exception as e:
                 return {
             "success": False,
             "error": str(e),
             "message": "Database connection error"
         }

@app.get("/debug/admin-privacy-requests-test")
async def test_admin_privacy_requests():
    """Test the admin privacy requests endpoint logic without authentication"""
    try:
        with get_db() as conn:
            cursor = conn.cursor()
            
            results = {
                "column_check": None,
                "direct_query": None
            }
            
            # Test 1: Check column detection
            try:
                if IS_PRODUCTION and DB_URL:
                    cursor.execute("SELECT column_name FROM information_schema.columns WHERE table_name = 'privacy_requests'")
                    columns_raw = cursor.fetchall()
                    columns = [row[0] if isinstance(row, (tuple, list)) else row for row in columns_raw]
                else:
                    cursor.execute("PRAGMA table_info(privacy_requests)")
                    columns_raw = cursor.fetchall()
                    columns = [row[1] if isinstance(row, (tuple, list)) else row for row in columns_raw]
                
                results["column_check"] = {
                    "columns_raw": columns_raw,
                    "columns_processed": columns,
                    "has_details": "details" in columns,
                    "has_completed_at": "completed_at" in columns
                }
            except Exception as e:
                results["column_check"] = {"error": str(e)}
            
            # Test 2: Direct query with simple SELECT
            try:
                cursor.execute("SELECT id, request_type, email, full_name, details, status, created_at FROM privacy_requests ORDER BY created_at DESC")
                requests_raw = cursor.fetchall()
                
                requests = []
                for row in requests_raw:
                    if isinstance(row, (tuple, list)):
                        requests.append({
                            "id": row[0],
                            "request_type": row[1],
                            "email": row[2],
                            "full_name": row[3],
                            "details": row[4],
                            "status": row[5],
                            "created_at": row[6]
                        })
                    else:
                        requests.append(dict(row))
                
                results["direct_query"] = {
                    "count": len(requests),
                    "requests": requests
                }
            except Exception as e:
                results["direct_query"] = {"error": str(e)}
            
            return {
                "success": True,
                "is_production": IS_PRODUCTION,
                "db_url": DB_URL is not None,
                "results": results
            }
            
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "message": "Database connection error"
        }

@app.get("/debug/test-privacy-requests-direct")
async def test_privacy_requests_direct():
    """Test privacy requests table directly without authentication"""
    try:
        with get_db() as conn:
            cursor = conn.cursor()
            
            results = {
                "table_check": None,
                "count_check": None,
                "sample_insert": None,
                "list_requests": None
            }
            
            # Test 1: Check if table exists using direct SQL
            try:
                cursor.execute("SELECT to_regclass('privacy_requests')")
                table_check = cursor.fetchone()
                results["table_check"] = {
                    "exists": table_check[0] is not None if isinstance(table_check, (tuple, list)) else table_check is not None,
                    "result": table_check[0] if isinstance(table_check, (tuple, list)) else table_check
                }
            except Exception as e:
                results["table_check"] = {"error": str(e)}
            
            # Test 2: Try to count records
            try:
                cursor.execute("SELECT COUNT(*) FROM privacy_requests")
                count = cursor.fetchone()
                results["count_check"] = {
                    "count": count[0] if isinstance(count, (tuple, list)) else count
                }
            except Exception as e:
                results["count_check"] = {"error": str(e)}
            
            # Test 3: Try to insert a sample record
            try:
                cursor.execute("""
                    INSERT INTO privacy_requests (request_type, email, full_name, details, status)
                    VALUES ('access', 'debug@example.com', 'Debug User', 'Debug request', 'pending')
                    RETURNING id
                """)
                insert_result = cursor.fetchone()
                conn.commit()
                results["sample_insert"] = {
                    "success": True,
                    "id": insert_result[0] if isinstance(insert_result, (tuple, list)) else insert_result
                }
            except Exception as e:
                results["sample_insert"] = {"error": str(e)}
            
            # Test 4: Try to list all requests
            try:
                cursor.execute("SELECT * FROM privacy_requests ORDER BY created_at DESC")
                requests = cursor.fetchall()
                results["list_requests"] = {
                    "count": len(requests),
                    "requests": [dict(zip([desc[0] for desc in cursor.description], row)) for row in requests] if requests else []
                }
            except Exception as e:
                results["list_requests"] = {"error": str(e)}
            
            return {
                "success": True,
                "message": "Privacy requests test completed",
                "results": results
            }
            
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "message": "Database connection error"
        } 

@app.get("/debug/create-missionops-tables")
async def create_missionops_tables():
    """Debug endpoint to create MissionOps database tables"""
    try:
        # Import and initialize MissionOps database
        from missionops_models import init_missionops_db
        init_missionops_db()
        return {"status": "success", "message": "MissionOps tables created successfully"}
    except Exception as e:
        return {"status": "error", "message": f"Failed to create MissionOps tables: {str(e)}"}

class RouteEventRequest(BaseModel):
    coordinates: List[Dict[str, float]]  # List of {lat: float, lng: float}
    radius: Optional[float] = 10.0  # Search radius in miles
    dateRange: Optional[Dict[str, str]] = None  # {startDate: "YYYY-MM-DD", endDate: "YYYY-MM-DD"}

@app.post("/events/route-batch")
async def get_route_events_batch(request: RouteEventRequest):
    """
    Efficiently retrieve events along a route using batch processing.
    Takes multiple coordinate points and returns deduplicated events within radius.
    Optimized for route planning with minimal API calls.
    """
    placeholder = get_placeholder()
    
    if not request.coordinates or len(request.coordinates) == 0:
        return []
    
    # Limit coordinates to prevent abuse
    max_coords = 50
    if len(request.coordinates) > max_coords:
        raise HTTPException(
            status_code=400, 
            detail=f"Too many coordinates. Maximum {max_coords} allowed."
        )
    
    try:
        with get_db() as conn:
            cursor = conn.cursor()
            
            # Build a single query with multiple distance calculations
            # This is much more efficient than multiple separate queries
            distance_conditions = []
            params = []
            
            for i, coord in enumerate(request.coordinates):
                lat = coord.get('lat')
                lng = coord.get('lng')
                
                if lat is None or lng is None:
                    continue
                    
                # Add distance condition for this coordinate
                distance_condition = f"""
                    (6371 * acos(cos(radians({lat})) * cos(radians(lat)) * 
                     cos(radians(lng) - radians({lng})) + sin(radians({lat})) * 
                     sin(radians(lat)))) * 0.621371 <= {request.radius}
                """
                distance_conditions.append(distance_condition)
            
            if not distance_conditions:
                return []
            
            # Build date filter conditions
            date_conditions = []
            if request.dateRange and request.dateRange.get('startDate') and request.dateRange.get('endDate'):
                # Use custom date range
                start_date = request.dateRange['startDate']
                end_date = request.dateRange['endDate']
                
                if IS_PRODUCTION and DB_URL:
                    date_conditions.append(f"date::date BETWEEN '{start_date}' AND '{end_date}'")
                else:
                    date_conditions.append(f"date BETWEEN '{start_date}' AND '{end_date}'")
                    
                params.extend([])  # No additional params needed for string literals
            else:
                # Default: future events only
                if IS_PRODUCTION and DB_URL:
                    date_conditions.append("date::date >= CURRENT_DATE")
                else:
                    date_conditions.append("date >= date('now')")
            
            # Combine all conditions
            all_conditions = [f"({' OR '.join(distance_conditions)})"]
            if date_conditions:
                all_conditions.extend(date_conditions)
            
            where_clause = " AND ".join(all_conditions)
            
            # Single optimized query that finds events near ANY of the coordinates
            query = f"""
                SELECT DISTINCT id, title, description, short_description, date, start_time, end_time, end_date, 
                       category, address, city, state, country, lat, lng, recurring, frequency, created_by, created_at,
                       COALESCE(interest_count, 0) as interest_count,
                       COALESCE(view_count, 0) as view_count,
                       fee_required, price, currency, event_url, host_name, organizer_url, slug, is_published,
                       start_datetime, end_datetime, updated_at, verified
                FROM events 
                WHERE {where_clause}
                ORDER BY 
                    interest_count DESC, date ASC
                LIMIT 100
            """
            
            cursor.execute(query, params)
            events = cursor.fetchall()
            
            # Process results
            result = []
            seen_event_ids = set()
            
            for event in events:
                try:
                    # Convert to dict
                    if hasattr(event, '_asdict'):
                        event_dict = event._asdict()
                    elif isinstance(event, dict):
                        event_dict = dict(event)
                    else:
                        column_names = [
                            'id', 'title', 'description', 'short_description', 'date', 'start_time', 'end_time',
                            'end_date', 'category', 'address', 'city', 'state', 'country', 'lat', 'lng', 'recurring',
                            'frequency', 'created_by', 'created_at', 'interest_count', 'view_count',
                            'fee_required', 'price', 'currency', 'event_url', 'host_name', 'organizer_url', 'slug', 'is_published',
                            'start_datetime', 'end_datetime', 'updated_at', 'verified'
                        ]
                        event_dict = dict(zip(column_names, event))
                    
                    # Deduplicate events
                    event_id = event_dict.get('id')
                    if event_id and event_id not in seen_event_ids:
                        seen_event_ids.add(event_id)
                        
                        # Convert datetime fields
                        event_dict = convert_event_datetime_fields(event_dict)
                        
                        # Ensure counters are integers
                        event_dict['interest_count'] = int(event_dict.get('interest_count', 0) or 0)
                        event_dict['view_count'] = int(event_dict.get('view_count', 0) or 0)
                        
                        result.append(event_dict)
                        
                except Exception as event_error:
                    logger.warning(f"Error processing route event {event}: {event_error}")
                    continue
            
            logger.info(f"Route batch query returned {len(result)} unique events for {len(request.coordinates)} coordinates")
            return result
            
    except Exception as e:
        error_msg = str(e)
        logger.error(f"Error retrieving route events: {error_msg}")
        
        if "timeout" in error_msg.lower():
            raise HTTPException(status_code=504, detail="Route events query timed out")
        elif "connection" in error_msg.lower():
            raise HTTPException(status_code=503, detail="Database connection issues")
        else:
            raise HTTPException(status_code=500, detail="Error retrieving route events")
